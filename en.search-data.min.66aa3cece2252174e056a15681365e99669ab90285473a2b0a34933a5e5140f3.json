[{"id":0,"href":"/doc/ns-labs/table-of-contents/raid/","title":"Lab01 RAID 阵列","section":"Table of Contents","content":" Lab01 RAID阵列 # 本指导书是实验过程的一个示例，供不熟悉实验环境的同学参考，无需严格按照指导书中的步骤来进行实验 :)\n实验内容 # 本实验通过软件 RAID 管理磁盘：\n通过工具查看磁盘列表 [仅Windows] C 盘扩展卷，检查其它磁盘是否联机，联机并初始化磁盘 [仅Ubuntu] 使用LVM扩展根目录/的容量 创建RAID阵列，测试读写文件并模拟磁盘损坏，观察有什么情况发生，数据是否损坏或者丢失？ 尝试RAID 0 尝试RAID 1 尝试RAID 5 [选做] 控制变量，测试RAID阵列的读写速率，并结合理论分析实验结果 [选做] 自行设计与扩展，体会 RAID 0 1 5 10 等方案下，在遇到磁盘损坏故障、冗余备份的效果等 [选做] 思考题： 为什么及在什么条件下选择用 RAID RAID \u0026amp; 分布式存储 \u0026amp; 集中存储的区别 其中，选做的题目不计入本实验的总得分，仅额外加分\n实验报告模板分别是lab01_win.md和lab01_ubuntu.md，供参考使用，最后请将实验报告按 lab01-学号-姓名.pdf 的命名格式提交\n实验准备 # Windows下连接Windows 10虚拟机 # 使用系统自带的“远程桌面连接”即可\nMacOS下连接Windows 10虚拟机 # 使用Microsoft Remote Desktop连接，云平台课程资源页提供该软件的下载\nWindows下连接Ubuntu虚拟机 # 一般来讲，Windows 10（及以上）自带的cmd.exe都自带ssh client，打开cmd后直接ssh foo@x.x.x.x即可登录\n为了更好的使用体验，推荐下载使用 Windows Terminal\n当然，你也可以使用 termius 或者其他工具（如 Xshell等）进行多个ssh连接的管理\nMacOS下连接Ubuntu虚拟机 # 使用系统自带的Terminal.app登录即可\n为了更好的使用体验，推荐使用 iterm2 登录\n当然，你也可以使用 termius 进行多个ssh连接的管理\n校外连接Ubuntu虚拟机 # 在云平台的虚拟机列表中点击“连接”即可 Ubuntu虚拟机联网 # 虚拟机已内置联网登录工具\nbuaalogin config buaalogin login 重要提醒 # 请登录实验虚拟机后，及时更改虚拟机密码，以避免以下情况发生：\n未意识到登录了他人的虚拟机， 帮助别人完成了实验工作 自己的虚拟机被别人登录，已经做完的实验被破坏 Windows环境 # 点击桌面上的“计算机管理”，在左侧菜单栏打开“计算机管理-存储-磁盘管理” 右键点击C盘，选择“扩展卷”，即可把C盘所在磁盘上未分区的80GB空间合并至C盘内 右键点击未分配的磁盘，看到可以选择创建跨区卷、带区卷、镜像卷、RAID-5卷，它们分别对应不做条带化的RAID 0、条带化的RAID 0、RAID 1、RAID 5，这里以RAID 5为例 选择阵列中包括的磁盘，下一步选中“快速格式化”，最后确认转化为动态磁盘，即可创建阵列 点击桌面上的“ATTO 磁盘基准测试”，选择RAID 5阵列所在的盘符，测试读写速度 在RAID 5阵列上保存文件，将其中的一块磁盘脱机，模拟磁盘损坏，文件仍然可以正常查看 Ubuntu环境 # 实验全程在root权限下进行\nsudo -i 使用LVM为根目录扩容 # 查看虚拟机上的磁盘列表，其中有sdb~sdf共五块磁盘可以供本次实验使用，sda是系统所在的磁盘，容量为50GB\n$ lsblk -d sda 8:0 0 50G 0 disk sdb 8:16 0 10G 0 disk sdc 8:32 0 10G 0 disk sdd 8:48 0 10G 0 disk sde 8:64 0 10G 0 disk sdf 8:80 0 10G 0 disk 查看根目录/的容量，为15GB，说明sda磁盘上有空闲空间没有利用上\n$ df -lh / Filesystem Size Used Avail Use% Mounted on /dev/mapper/ubuntu--vg-ubuntu--lv 15G 4.9G 9.1G 35% / 使用fdisk命令给空闲的磁盘空间分区\n在Command (m for help):后面输入n，一路回车，默认按磁盘最大容量分区\n在Command (m for help):后面输入t，输入4选择新创建的分区，输入31选择Linux LVM分区类型\n最后在Command (m for help):后面输入w保存\n$ fdisk /dev/sda Command (m for help): n Partition number (4-128, default 4): First sector (33552384-104857566, default 33552384): Last sector, +/-sectors or +/-size{K,M,G,T,P} (33552384-104857566, default 104857566): Created a new partition 4 of type \u0026#39;Linux filesystem\u0026#39; and of size 34 GiB. Command (m for help): t Partition number (1-4, default 4): 4 Partition type (type L to list all types): 31 Changed type of partition \u0026#39;Linux filesystem\u0026#39; to \u0026#39;Linux LVM\u0026#39;. Command (m for help): w The partition table has been altered. Syncing disks. 创建物理卷（PV），命名格式一般为磁盘名+分区号\npvcreate /dev/sda4 查看卷组（VG）列表，Ubuntu在系统安装时已创建好了一个默认的卷组，如下所示\n$ vgdisplay --- Volume group --- VG Name ubuntu-vg ... 将刚创建的物理卷加入卷组\nvgextend ubuntu-vg /dev/sda4 查看逻辑卷（LV）列表，Ubuntu系统安装在默认逻辑卷上\n$ lvdisplay --- Logical volume --- LV Path /dev/ubuntu-vg/ubuntu-lv ... 将逻辑卷扩容，然后为文件系统（ext4类型）扩容\nlvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv resize2fs /dev/ubuntu-vg/ubuntu-lv 查看根目录容量，已扩容至49GB\n$ df -lh / Filesystem Size Used Avail Use% Mounted on /dev/mapper/ubuntu--vg-ubuntu--lv 49G 4.9G 42G 11% / RAID阵列搭建 # 安装mdadm工具\napt-get install mdadm mdadm命令的常用参数及作用如下\n参数 作用 -a 检测设备名称 -n 指定设备数量 -l 指定RAID级别 -C 创建 -v 显示过程 -f 模拟设备损坏 -r 移除设备 -Q 查看摘要信息 -D 查看详细信息 -S 停止RAID磁盘阵列 用命令创建 RAID 1，看到如下输出即为成功\n$ mdadm -Cv /dev/md1 -a yes -n 2 -l 1 /dev/sd{b,c} mdadm: chunk size defaults to 512K mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md1 started. 将创建好的RAID 1阵列格式化为ext4文件系统\nmkfs.ext4 /dev/md1 查看RAID 1阵列的摘要信息，可以看到两块10GB的磁盘组成的RAID 1阵列容量约为10GB\n$ mdadm -Q /dev/md1 /dev/md1: 9.99GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail. 将RAID 1阵列临时挂载到/mnt/raid1目录下，测试文件读写\nmkdir -p /mnt/raid1/ mount /dev/md1 /mnt/raid1 echo \u0026#39;Hello, world!\u0026#39; \u0026gt; /mnt/raid1/hello cat /mnt/raid1/hello 测试写文件速率，由于dd命令在计算时单位换算有误，因此我们自己计算出实际值为$8 \\times 300000 \\div 1024 \\div 9.462=247.70MB/s$\n$ time dd if=/dev/zero of=/mnt/raid1/test.bdf bs=8k count=300000 300000+0 records in 300000+0 records out 2457600000 bytes (2.5 GB, 2.3 GiB) copied, 9.46088 s, 260 MB/s real 0m9.462s user 0m0.159s sys 0m2.525s 测试读文件速率，实际值为$8 \\times 1309568 \\div 1024 \\div 37.837=270.40MB/s$\n$ time dd if=/dev/md1 of=/dev/null bs=8k 1309568+0 records in 1309568+0 records out 10727981056 bytes (11 GB, 10 GiB) copied, 37.836 s, 284 MB/s real 0m37.837s user 0m0.456s sys 0m7.154s 模拟阵列中的一块磁盘损坏（注意：此方法仅适用于RAID 1、RAID 5、RAID 10，模拟RAID 0损坏的方法见下一节）\n$ mdadm /dev/md1 -f /dev/sdb mdadm: set /dev/sdb faulty in /dev/md1 再次读取文件，发现文件完好无损\n$ cat /mnt/raid1/hello Hello, world! 删除该阵列，释放磁盘空间\numount /mnt/raid1 mdadm -S /dev/md1 mdadm --zero-superblock /dev/sd{b,c} 同理可创建RAID 0、RAID 5、RAID 10等阵列\nmdadm -Cv /dev/md0 -a yes -n 2 -l 0 /dev/sd{b,c} mdadm -Cv /dev/md5 -a yes -n 5 -l 5 /dev/sd{b,c,d,e,f} mdadm -Cv /dev/md10 -a yes -n 4 -l 10 /dev/sd{b,c,d,e} 模拟RAID 0阵列中磁盘损坏 # 在创建阵列之前，需要先给待使用的磁盘分区，以/dev/sdb为例\n在Command (m for help):后面输入n，一路回车，默认按磁盘最大容量分区\n最后在Command (m for help):后面输入w保存\n$ fdisk /dev/sdb Command (m for help): 新分区名称默认为/dev/sdb1\n同理，为/dev/sdc也创建一个分区/dev/sdc1\n在这两个分区上创建RAID 0阵列\nmdadm -Cv /dev/md0 -a yes -n 2 -l 0 /dev/sd{b1,c1} 再使用fdisk删除分区：fdisk /dev/sdb\n在Command (m for help):后面输入d，然后在Command (m for help):后面输入w保存\n更新分区表并重启\npartprobe reboot 重启后发现RAID 0阵列已损坏\n$ mdadm -Q /dev/md0 mdadm: cannot open /dev/md0: No such file or directory "},{"id":1,"href":"/doc/ns-labs/table-of-contents/virtualization/","title":"Lab02 虚拟化实验","section":"Table of Contents","content":" Lab02 虚拟化实验 # 实验目标 # 实施计算虚拟化，安装配置环境，熟悉计算虚拟化的概念，理解基本操作，掌握基础知识。 理解集中管理对于虚拟化的作用，通过部署集中vCenter体验集群的设置，分布式交换机的设置，了解主机从不同网络进行迁移的实际需求。 实验内容 # 按照实验指南的指导，完成实验。 按照实验报告模板，撰写实验报告，将重要的实验步骤截图，填入实验报告中，并回答相应问题。 本次实验以小组形式进行，虚拟机已分发至每组第一位同学的账户中，每组一台虚拟机，实验报告每个人都需要提交，同组的实验报告内容可以一致\n请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab02-学号-姓名.pdf的格式。\nHypervisor # 我们知道，一台计算机一般有以下的结构：\n操作系统负责管理硬件资源（CPU，内存，硬盘等），并向上提供相应的系统调用，供具体的应用程序使用。\n而我们平常提到的操作系统的虚拟化，本质上就是要模拟出一套硬件（包括虚拟CPU，虚拟内存，虚拟硬盘等），然后在这一套虚拟的硬件的基础上部署客户操作系统。客户操作系统完全不需要做任何修改，即可在这个“虚拟的机器”中顺利执行。但客户操作系统的运行结果（比如接收键盘输入，输出图像和声音等），最终都是要靠原始的“实实在在”的硬件（物理机）来完成的。也就是说，需要有那么一个结构，能够将这个“虚拟的机器”的行为翻译到物理机的行为（比如将虚拟CPU的指令翻译到物理机的CPU指令）。负责做这件事情的结构被称为Hypervisor，又称为虚拟机监控器（virtual machine monitor，缩写为 VMM）。\n根据工作方式的不同，Hypervisor分为以下两种。\n第一种是我们比较熟悉的情况，本质上就是在主操作系统（Host Operating System）上安装了一个虚拟化软件，它来负责充当虚拟机的管理者，并通过主操作系统的系统调用来完成对物理机硬件的使用。VMware Workstation、Virtual Box、Qemu等都属这类虚拟化软件。除了这个虚拟化软件之外，主操作系统上还会运行其他“正常”的应用程序，比如，你在用VMware Workstation的同时还能听歌聊天等。\n第二种Hypervisor则直接舍弃了主操作系统（因为毕竟隔着一层，性能会有损失），而是直接把Hypervisor部署在硬件上。在这种情况下，物理机变成了更纯粹的“为虚拟化而生”的机器。Hypervisor能够直接与硬件沟通，其实在某种程度上也承担了主操作系统的角色（管理硬件），因此，我们也可以把这种Hypervisor看作是一种为虚拟化特制的操作系统。这其中典型的就是VMware ESXi。\n因为我们不可能要求每位同学都制备一套硬件来安装学习VMware ESXi，所以需要首先使用VMware Workstation来模拟出一套硬件。但VMware Workstation仅仅起一个前置作用，在实际的实验中并不会涉及到。请大家首先理清这层关系。\n实验指南 # 0. 安装VMware Workstation # 使用分配的虚拟机的桌面上的安装包安装即可。\n安装完成后需重启机器。\n打开VMware Workstation时， 选择试用即可。\n1. 安装VMware ESXi # 使用桌面上的ESXi镜像VMware-VMvisor-Installer-6.7.0.update03-19898906.x86_64-DellEMC_Customized-A18.iso创建虚拟机。\n注意选择客户操作系统的类型。\n虚拟机创建完成后，直接打开电源即可启动ESXi操作系统的安装流程，这一过程可能需要等待较长时间。\n安装流程中总是保持默认选项即可，其中设置的root密码应至少包含字母、数字和特殊符号，并且请务必牢记设置的root密码。\n在此流程中，可能需要使用使用到某些快捷键，这些快捷键可能会首先被你本机的操作系统捕获，在本机的系统设置中暂时屏蔽该快捷键即可。\n安装完成后，可以看到如下界面。\n可以看到，ESXi系统获得了一个IPv4地址192.168.80.128，并且这个地址是通过DHCP的方式获得的。这里用到的DHCP服务器其实是VMware Workstation内置的。也就是说，192.168.80.128这个地址只有在安装VMware Workstation的机器上才是有效的。\n2. 访问ESXi # 可以直接使用浏览器访问ESXi。访问的地址就是ESXi的地址，用户名和密码与vSphere Client的相同。\n如下图所示，可以为ESXi分配许可证。\n可用的KEY：\n0A65P-00HD0-3Z5M1-M097M-22P7H 3. 观察和体验vSphere Client提供的功能 # Client侧界面主要包含导航器、主体内容和任务事件这三部分。\n请浏览左侧导航栏的不同模块和不同模块下不同选项卡的内容，对ESXi提供的功能有个大致的了解。\n其中，存储部分可以查看ESXi虚拟机可访问的数据内容。\n可以通过使用“数据存储浏览器”查看、下载、上传、下载“存储”中的文件。\n4. 新建和安装虚拟机 # ESXi最主要的功能就是对虚拟机的管理。\nvSphere Client和Web端都有显著的入口供用户创建虚拟机。\n创建虚拟机的流程与使用VMware Workstation创建虚拟机没有太大区别，按照创建向导进行即可。\n在创建过程的“自定义设置”阶段，需要手动配置CD/DVD驱动器，插入桌面上的CentOS或Tiny Core Linux的安装镜像，以使虚拟机在开机时，能自动进入安装镜像的安装引导界面。\n当然，你也可以在虚拟机创建完成后，打开电源之前，手动编辑虚拟机配置，添加对应的镜像文件。\n完成后，打开虚拟机电源，即可进入操作系统的安装引导流程。\n5. 虚拟机文件系统格式和种类 # 添加虚拟机后，可以在“存储”中看到每台虚拟机中包含的文件内容。\n了解这些不同格式的文件和含义和作用。\n6. 虚拟机导出 # 关闭虚拟机电源后，即可将虚拟机导出。\n观察导出文件格式与虚拟机正常文件的区别。\n7. VMware Tools（选做） # 可以在如下图所示的虚拟机控制台的“操作”选项中找到“安装VMware Tools”的选项。\n点击“安装VMware Tools”后，ESXi会给虚拟机挂载一个包含了安装脚本的光盘。根据安装的操作系统的不同，具体的安装方式也有区别。对于Linux系统，可以在/dev目录下找到该光盘，并将其挂载到文件系统中，然后进入其中，执行安装脚本。\n8. 角色和用户（选做） # 可以在Web端创建不同的角色和用户，根据角色的不同，用户将获得不同的权限。尝试使用不同的用户账号登录Client，以体验权限限制带来的差异。\n9. 创建与配置集群（选做） # 后面这两部分实验内容比较复杂，有兴趣的同学可以选择尝试。\n在上面的实验中，我们进行了ESXi的部署，并使用ESXi创建了一些虚拟机。在实际应用中，往往需要多个ESXi主机组成集群，来提供更多的资源，或者提高可用性。在接下来的实验中，我们将使用vCenter Server管理多个ESXi主机，来管理所有的虚拟机和ESXi“物理机”集群。\n在VMware Workstation中新建一个新的ESXi虚拟机。该虚拟机内存可以给大一些，比如14G，磁盘大小也可以大一些，比如100G。以方便后续在其上安装vCenter Server。\n可以在VMware Workstation中给机器开大内存和大容量磁盘。\n因为内存和磁盘是虚拟的，所以即使本机物理内存和磁盘容量不够，也把握得住。\n如果后续实验中，确实出现了内存或磁盘容量不够的情况，请联系助教扩容。\n点击打开vCenter Server安装包VMware-VCSA-all-6.7.0-19832974.iso，可以看到README文件，其中包含着具体的安装指引。之后的安装步骤，如无特殊说明，无需改动默认选项，直接下一步即可。 我们刚刚启动的只是一个安装器程序，vCenter Server本质上还是要安装在一个特定的ESXi主机中。所以，在选择安装目标时，需要填入刚才新建的那个内存和磁盘容量都比较大的ESXi虚拟机的相关信息。 在最后的网络配置这里，选择DHCP 安装完成后，浏览器访问虚拟机地址或者使用vSphere Client均可访问到vCenter。 创建数据中心。\n添加主机。把本次实验创建的两个ESXi主机都添加进去。在添加主机时会弹出安全警示，选择“是”继续添加即可。输入主机的用户名和密码，一路默认即可（一定要禁用锁定模式）。 10. 分布式交换机（选做） # 在两台ESXi主机上分别启动一台Tiny Core Linux虚拟机，使用ifconfig命令查看这两台虚拟机的IP地址，使用ping命令测试两台机器之间的网络是否连通。\n默认情况下，vCenter会在两台主机上创建虚拟交换机，以保证虚拟机之间的网络通信。 如果你的集群出现网络不通的情况，可以查找相关资料，在配置好的集群上，创建分布式虚拟交换机。并分别在不同的主机上部署虚拟机，实现它们之间的网络互通。\n实验报告模板 # # Lab02 虚拟化实验 \u0026gt; 班级： \u0026gt; 学号： \u0026gt; 姓名： --- ## 实验内容 ### 1. 安装VMware ESXi ### 2. 访问ESXi #### ESXi分配许可证 ### 3. 观察和体验vSphere Client提供的功能 ### 4. 新建和安装虚拟机 ### 5. 虚拟机文件系统格式和种类 #### 不同格式的文件和含义和作用 ### 6. 虚拟机导出 #### 导出文件格式与虚拟机正常文件的区别 ### 7. VMware Tools（选做） ### 8. 角色和用户（选做） ### 9. 创建与配置集群（选做） ### 10. 分布式虚拟交换机（选做） "},{"id":2,"href":"/doc/cloud-labs/cloud/container_docker/","title":"容器与Docker综合实验","section":"云计算实验","content":" 容器与Docker综合实验 # 💡 文档中对各个概念的定义和阐述并不是严谨的，文档中用语尽量通俗易懂，大多数只是为了让同学们明白这个概念大致在说什么东西，能在头脑中有个感性的认识（这对知识的掌握和学习非常重要）。概念的严格定义还请参考对应的官方文档。 💡 容器和Docker的内容非常繁杂，实验文档不可能面面俱到，因此很多内容以链接的形式给出。请同学们在阅读文档本身的同时，不要忘记学习链接中指出的内容。 💡 同学们在执行命令时，一定要认真阅读命令的输出和log，通过阅读这些输出，你很容易了解你所执行的命令具体执行了哪些操作，这非常有助于理解其背后的运行原理。 💡 目前流行的绝大多数容器运行时都是用Go语言编写，著名的容器编排工具Kubernetes也是用Go语言编写的，基于Docker和Kubernetes建立起来的整个云计算生态中的绝大部分项目也都是用Go语言实现的。因此，如果想深入了解和学习云计算相关内容的话，建议同学们学习和掌握Go语言。当然，这并不是本次实验和这门课的要求:) 注意事项\n本次分配的机器的账户和密码为： buaa: \u0026amp;shieshuyuan21 务必首先修改机器的root和buaa账户的密码\n请务必阅读 虚拟机使用说明。\n分配的虚拟机中，已经安装了Docker，无需重复安装；并设置了Docker镜像地址（该地址指向校内地址），理论上docker.io中的镜像不用联网即可拉取。例如可以直接在虚拟机上docker pull nginx。\n实验目的 # 理解容器的概念，了解实现容器所使用的的底层技术，理解容器与虚拟机的区别\n理解容器与Docker之间的关系\n掌握Docker的基本使用方法和常见命令，可以使用Dockerfile构建镜像\n实验要求 # 请参考本实验文档，并查阅相关资料，回答以下问题并完整记录实验过程：\n数据持久化。容器是 “一次性的” 和 “脆弱的”（请大家务必牢记容器这一特性），容器很容易因为各种原因被kill（如资源不足等等）。而容器产生的数据文件是和容器绑定在一起的，当容器被删除时，这些数据文件也会被删除，这是我们不想看到的。\n比如，我们在机器上启动了一个mysql容器，在写入了一些重要数据后，因为某种原因该容器被意外删除了。此时即使重新启动一个mysql容器也找不会之前的数据了。请结合实验文档中的内容和查阅相关资料，讨论应该通过何种方式启动容器来避免出现这一问题？你能得出几种方案？每种方案的优劣如何？并请分别使用这些方案模拟mysql容器 创建 - 写入数据 - 销毁 - 重新创建 - 重新读到之前写入的数据 的场景，以证明方案的有效性。\n请从ubuntu镜像开始，构建一个新的包含Nginx服务的ubuntu镜像，并修改Nginx主页内容为你的学号，请分别使用docker commit 和 Dockerfile两种方式完成， 并将这个新构建的镜像推送到软院的image registry中。这个镜像推送的地址应该是 harbor.scs.buaa.edu.cn/\u0026lt;你的学号\u0026gt;/ubuntu-nginx:${TAG}，其中，使用docker commit构建的镜像的TAG为dockercommit；使用Dockerfile构建的镜像的TAG为 dockerfile。\n在测评时，助教会分别对你push的两个镜像执行以下命令（假设镜像名称为example_image_name）：\ndocker run -d -p 8899:80 example_image_name; sleep 5; curl localhost:8899 请保证上述命令的输出中包含你的学号。\nHint:\nharbor.scs.buaa.edu.cn 这个网页可以打开\nharbor.scs.buaa.edu.cn 的用户名为你的学号，默认密码为Newpass@2021\n如果你使用的分配的校园网的虚拟机，默认是无法联网的。如果你的容器内部需要联网，可以在启动容器前执行 export http_proxy=http://10.251.0.37:3128;export https_proxy=http://10.251.0.37:3128\n上述方式本质上只是修改当前bash进程的HTTP_PROXY，如果你有要求更高的联网需求，可以按照 此说明进行配置（配置中，需要使用的hostname是本次分配的虚拟机的名称，即docker_lab_学号。完成此说明中的操作后，虚拟机将持续处于联网状态。\n背景 # 我们知道，运行在操作系统中的软件，不仅包含我们实际编写的业务代码，还包含各种各样的运行时（runtime）（比如运行Java程序时需要依赖JRE，运行JS时需要依赖node、deno或浏览器环境，运行Python代码时需要Python环境，等等）和依赖库（lib）（比如，即使用C语言写个最简单的Hello World也得include一个stdio.h，以获取基本的向计算机屏幕打印字符串的能力）。在当前计算机性能普遍超越单个程序所需的性能的情况下，我们绝大多数情况下不会在一台机器中运行单个程序，而不同编程语言或不同类型的软件所需要的运行时和依赖库的类型和版本不尽相同，甚至同一个类型的运行时的不同版本之间还相互不兼容，这就需要我们在同一台机器中维护多个不同类型不同版本的运行时和依赖库，所以，大家在日常的学习和开发中，大概率会遇到下面这些问题：\n我想下载和使用一个软件，所有步骤都按照官方Guide一步步执行，但最后就怎么也启动不了，总是会报这样或那样的错误（经常用npm的同学应该深有体会）。 我想下载和使用一个软件，结果总是提示依赖库缺失或版本冲突，最后好不容易解决了，结果把自己本地的环境搞的一团糟，甚至最后不得不重装系统。 我在本地写好的代码，明明我的机器上跑的好好的，怎么到你那里就有bug了？！！ 从网上下载好的一个来源不明程序，莫名奇妙地向你申请各种系统权限，不给权限就罢工。 ……等等 在传统情况下，我们可以使用虚拟机技术来解决这些问题。虚拟机技术允许我们在一台计算机中模拟多个相互隔离的计算机硬件环境。我们可以创建一台虚拟机，把与软件相关的所有东西都塞到这个虚拟机中。当其他人需要这个软件时，我们只需要将该虚拟机打包成特定格式的文件，然后分发给对方就可以了。对方只需要在兼容的虚拟机环境中导入该虚拟机文件，就能保证获得同样的运行效果。\n虚拟机的解决方案简单有效，但有些简单粗暴：且不提Hypervisor层带来的性能损耗，为了运行一个应用程序就要虚拟化一整个计算机环境，并配套上一个完整的操作系统功能（维持客户机运行的各种守护进程、OS调度器等等），未免太奢侈了。\n有没有一种能够隔离两个不同的应用程序的更轻量级的解决方案呢？有的，那就是本文的主角——容器（Container）。\n容器（Container） # 在具体讨论容器（Container）之前，我们先大致了解一下容器所依赖的两个Linux机制：namespace和cgroups。\nnamepace # 我们知道，隔离两个应用程序本质上其实是隔离这两个应用程序所属的两个（或两组）进程。而如果能把两个进程使用的各项系统资源（文件系统、进程ID、网络、用户组等） 都隔离开，实际上也就达到了隔离两个进程的目的。\nLinux的namespace就提供了这样一种内核级别的资源隔离机制。它把系统的各项资源放到不同的namespace下，不同namespace下的资源是完全隔离的互不影响，而每个进程在使用某类资源时，只能归属于某个特定的namespace。这样就可以达到隔离不同进程所使用的的资源的目的。\n实际上，我们可以通过proc文件系统看到当前机器上的每个进程使用的各项资源所属的namespace，下图中展示的就是286486号进程的namespace使用情况：\n下面我们通过举例来进一步理解namespace机制。\nPID Namespace # 我们知道，在Linux中，所有进程都是从init进程（1号进程）或它的子进程fork出来的，进而，它们会组成一个树状结构：\n每个进程都会被分配一个独一无二的进程号（PID），不同进程间的相互识别、通信也都是基于进程号进行的。\n大家应该还记得Linux创建进程时使用的clone系统调用。我们可以用该函数写出下面简单的demo：\n#define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; static char child_stack[1048576]; static int child_fn() { printf(\u0026#34;子进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getppid()); printf(\u0026#34;子进程视角的子进程的PID: %ld\\n\u0026#34;, (long)getpid()); return 0; } int main() { printf(\u0026#34;父进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getpid()); pid_t child_pid = clone(child_fn, child_stack+1048576, SIGCHLD, NULL); printf(\u0026#34;父进程视角的子进程的PID: %ld\\n\u0026#34;, (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } 代码很简单，我们fork了一个子进程，并分别父进程和子进程中打印父进程和子进程的PID，得到的输出如下：\n父进程视角的父进程的PID: 53927 父进程视角的子进程的PID: 53928 子进程视角的父进程的PID: 53927 子进程视角的子进程的PID: 53928 结果完全符合我们的预期，对应的进程树的图景应该是：\n下面我们修改代码，在调用clone函数时，添加CLONE_NEWPID标志。该标志表示在fork子进程时会创建一个新的PID Namespace，并将新创建的这个子进程放入该namespace中：\n#define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; static char child_stack[1048576]; static int child_fn() { printf(\u0026#34;子进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getppid()); printf(\u0026#34;子进程视角的子进程的PID: %ld\\n\u0026#34;, (long)getpid()); return 0; } int main() { printf(\u0026#34;父进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getpid()); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); printf(\u0026#34;父进程视角的子进程的PID: %ld\\n\u0026#34;, (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } 这时程序的输出是这样的：\n父进程视角的父进程的PID: 54539 父进程视角的子进程的PID: 54540 子进程视角的父进程的PID: 0 子进程视角的子进程的PID: 1 可以看到，父进程输出的结果是符合预期的，但子进程却认为它的父进程PID为0，即它没有父进程；而认为自己的PID为1，即自己是1号进程。大致情形如下图所示：\n可以看到，在调用clone函数时，同时创建了一个新的PID namespace，新创建的子进程成为这个新的PID namespace中的新的进程树的根（PID=1），这个子进程不知道操作系统上还存在其他进程。可以想到，如果父进程再使用同样的方法fork子进程2，那么新创建的子进程2也会被放入一个新的PID namespace中，并成为那个新的namespace中的进程树的根。这样，子进程1和子进程2都互相不知道对方的存在，实现了互相之间对进程号的隔离。\n注意，上面仅仅是我们隔离进程的第一步。上面创建的子进程1和子进程2依然可以访问相同的文件系统、网络接口等其他资源。这些资源需要使用其他类型的namespace进行隔离。\nNetwork Namespace # 我们知道，Linux中的进程是通过网络接口（network interface）（可以理解为不同的网卡，这些网卡可能是物理存在的，也可以是虚拟的）进行网络通信的。网络接口的配置决定了进程可以以怎样的方式与网络上的哪些计算机进行通信（比如网络接口的配置包括了该接口的IP、DHCP、路由配置等）。\n可以通过ip link命令来查看当前机器上的网络接口：\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens192: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:92:39:60 brd ff:ff:ff:ff:ff:ff altname enp11s0 可以看到，当前机器上有一个名为ens192的物理网卡和一个loopback网络接口，这些网络接口属于全局默认的network namespace；默认情况下，从init进程fork出来的进程都属于这个全局的network namespace，意即每个进程都可以访问这些网络接口进行网络通信。而如果我们在使用clone函数fork进程的时候传递CLONE_NEWNET标志，就可以在fork进程的同时，创建一个新的network namespace，并使新进程使用这个新的network namespace中的网络接口。\n💡 loopback是一个特殊的虚拟网络接口，用于进程与本机的其他server进程通信。 我们通过下面的示例代码来说明这个问题：\n#define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; static char child_stack[1048576]; static int child_fn() { printf(\u0026#34;子进程所在的network namespace:\\n\u0026#34;); system(\u0026#34;ip link\u0026#34;); printf(\u0026#34;\\n\\n\u0026#34;); return 0; } int main() { printf(\u0026#34;父进程所在的network namespace:\\n\u0026#34;); system(\u0026#34;ip link\u0026#34;); printf(\u0026#34;\\n\\n\u0026#34;); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } 代码很简单，我们使用clone函数创建了子进程，并分别在父子进程中使用ip link获取当前进程能使用的网络接口。程序输出如下：\n父进程所在的network namespace: 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens192: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:92:39:60 brd ff:ff:ff:ff:ff:ff altname enp11s0 子进程所在的network namespace: 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 可以看到，子进程和父进程可以访问的网络接口是不同的（虽然两者都有lo网络接口，但本质上是不同的loopback接口）。network namespace 成功实现了网络资源的隔离。\nMount Namespace # Linux的文件系统一般会包含多个挂载点。我们可以在Linux的目录树中的指定位置挂载或卸载特定的文件系统来达到修改整个文件系统结构的目的。\n比如，在现有的目录树上挂载一个右侧的文件系统：\n就可以得到下图这样的目录树：\nLinux的mount namespace可以保证不同namespace的挂载点不受影响。在使用clone函数时，可以传递CLONE_NEWNS标志。这时在创建子进程时，操作系统会自动创建一个新的mount namespace，并把该子进程加入到这个新的namespace中。这样，子进程可以任意修改自己的挂载点而不会影响其他进程。\n比如，我们从上图的目录树开始，使用CLONE_NEWNS标志fork一个子进程：\n子进程所处的mount namespace虽然会直接继承父进程的mount namespace中的挂载点信息，但子进程对挂载点的修改不会影响到父进程的mount namespace。比如，如果我们这时可以在/root/a处再挂载一个文件系统：\n这时，子进程将获得与其他进程都不相同的一个目录树，如果方框中的文件系统只挂载在子进程的该节点上的话，那么这部分文件系统对子进程来说就是独享的。从这个角度来说，Linux通过mount namespace实现了不同进程之间文件系统的“隔离”。\n其他的Namespace # 除了PID namespace，network namespace，mount namespace之外，Linux还提供了以下3类namespace：\nUTS: 隔离主机名和域名信息 IPC: 隔离进程间通信 User: 隔离用户和用户组的ID 综合使用以上6种namespace，可以实现对进程所涉及的各项资源的隔离，进而达到隔离不同的进程的目的。\n💡 本节中的代码都是用C语言演示的，代码中使用的系统调用是Linux Kernel提供的，原则上可以使用任意语言实现（当然可能需要使用C语言绑定），包括但不限于Go，Rust等。 cgroups # 使用namespace机制进行隔离的进程，虽然可以被限制只能使用哪些资源，但没有被限制资源用量。比如，被隔离的进程依旧可以不受限制地执行大量CPU密集任务（消耗大量CPU时间），或占用消耗大量的内存。要想做到这种限制，就需要使用另一个Linux机制：cgroups。\ncgroups全称是“Control Groups”，顾名思义，cgroups可以将进程分组，并为每个进程组分配设定特定的CPU和内存限额，该进程组中的每个进程的CPU和内存用量都不能超过这个限额。\n简单来讲，namespace控制进程能看到什么（what can see），cgroups控制进程能用什么（what can use）。\n容器（Container） # 到现在为止，我们知道，在Linux中，可以使用namespace和cgroups机制隔离一个或一组进程。我们把这样的一个或一组被隔离的进程称作“容器（Container）”。当我们提到“启动（或创建）一个容器”时，其实就是在说“使用namespace和cgroups机制创建一个或一组进程”。\n💡 从这个角度看，容器的实现并不复杂，实际上，互联网上有很多相关的教程教你怎么“自己写一个Docker”。比如，这个 视频展示了怎么用Go语言从零开始实现一个简单的容器。 容器与虚拟机 # 虚拟机和容器的最大的区别在于，虚拟机模拟了一整个计算机环境，其上有一个完整的操作系统，它有自己的进程调度、内存管理等；而容器仅仅是被隔离的一个或一组进程，这些进程和在宿主操作系统看来跟其他普通进程没什么区别，他们会被Kernel以同样的方式调度，以同样的方式使用内存管理。\n镜像（Image） # 正如本文开头提到的，容器的运行依赖进程命令本身的可执行文件、运行时、依赖库和相关资源文件。为了使不同的容器所使用的这些文件之间能够相互隔离，我们可以利用在“mount namespace”一节中提到的那个“特殊的文件系统”，即，把可执行文件、运行时、依赖库和相关资源文件等都放到该“特殊的文件系统”中，当容器启动后，它只需要读写这部分“特殊的文件系统”即可。\n不难发现，这个“特殊的文件系统”包含了对容器行为的各种描述，可以认为它就是容器的“模板”，即，这个“特殊的文件系统”长啥样，对应的容器就应该表现出啥样。这个“特殊的文件系统”与容器的关系可以类比为类和对象之间的关系。更进一步地，即使我们在不同的机器上启动容器，只要这个“特殊的文件系统”相同，那么总会得到运行效果完全相同的容器。\n在日常使用中，这个“特殊的文件系统”会被打包成压缩包以方便在不同的机器中传递，而这个压缩包被称作“镜像（Image）”。通常情况下，容器总是从一个镜像启动的。\n💡 真正生产环境中所用的镜像不止包含上面提到的这些文件，还会包含一些元信息文件，包括但不限于环境变量、容器启动的入口、容器对外暴露的端口等等。 容器运行时（Container Runtime） # 到目前为止，我们启动的被隔离的子进程都是非常简单（只是一个非常简单的函数），在实际生产中，容器的启动过程要复杂的多：\n解析容器镜像 设置进程的启动入口 挂载读写卷 ……等等 更进一步地，同一台机器上可能会启动多个不同的容器，有些容器需要动态地停止和启动。这些工作显然不可能通过手动操作来完成。这时，就需要一个工具来帮我们完成对这些容器的管理工作，我们称这种工具为“容器运行时（Container Runtime）”。\n容器运行时一般都提供了非常简洁的指令入口，只需要非常简短的命令就可以启动一个复杂的容器，或者随时停止和重启一个容器。\n目前应用最广泛的容器运行时是 runc，我们常见的各种容器管理工具，如Docker、Podman、containerd等都是以runc为基础构建的。\n💡 在通常情况下，我们认为Docker、Podman、containerd等容器管理工具是广义的容器运行时。 在上面列举的这些容器运行时中，Docker是使用最广泛的容器管理工具，甚至在很多人的认识中，Docker和容器简直就是同义词。虽然Kubernetes将在1.24版本中不再支持使用Docker作为容器运行时，Podman和Containerd也在逐步蚕食Docker的市场，但Docker依然处于垄断地位，很多其他的容器运行时甚至也特意将自己的使用方法设计地和Docker一模一样（如Podman）。所以，我们本次课程实验还是将主要讲解Docker的原理、架构和使用方法。入门了Docker后，了解和学习其他容器运行时也会变得非常简单。\nDocker # 上图是Docker的经典Logo，一个白鲸载着集装箱的形象。“Docker”这个词是从“Dock”演变来的。Dock意为“码头”，Docker自然可以引申为“承载集装箱的工具”。“Container”本身也有“集装箱”的含义，Docker作为一个容器（Container）管理工具，这样的logo可谓是非常生动形象了。\n我们平常所说的“Docker”，其实是一个巨大的结合体。从下图中可以看出，当用户使用Docker时，要经过多层组件的调用。虽然它们中的很多部分都可以单独作为一个独立软件来用（比如Containerd、runc），但我们在谈论Docker时，通常认为它们是整个Docker软件的一部分。在安装Docker时，这些组件也会被同时默认安装。\nDocker本身自诩是开源软件，它的上游构建组件确实是开源的，可以在这里找到它的代码 moby/moby。\n💡 对于Dodcker的学习，互联网上有很多完整可靠的教程，比如Docker官方的 Docker 101 tutorial。这些都是非常不错的学习资料。 Docker的安装 # 💡 如果你是使用本次实验分配的虚拟机，请忽略本节所有内容。由于实验人数较多，云平台压力较大，可能出现虚拟机卡顿的情况，因此，建议同学们优先使用本地环境进行实验。 macOS # 直接下载 Docker Desktop安装即可\nWindows # 如果你正在使用WSL2，直接按照该 指南安装Docker Desktop即可 如果你没有使用WSL2，可以选择先安装或升级到WSL2，然后执行上一步；也可以直接安装一个Linux虚拟机，然后在Linux中安装Docker Linux # 请参考 文档，并从“使用脚本自动安装”开始读起。\n线上环境 # 实际上，有很多网站提供了免费的线上Docker环境供大家学习和体验，免去了在本地配置环境的繁琐，而且没有网络问题，在浏览器中就可以学习Docker各种概念。比如，你可以注册并登录 Play with Docker尝试看看。在这些线上环境中同样可以完成本次实验。\n配置镜像源 # 我们常用的Docker官方的image registry在国内的连接非常不稳定，拉取镜像时很可能非常缓慢，这时可以配置镜像源，请参考 国内镜像加速。另外，如果你在校园网内，可以考虑使用软院的镜像加速器地址：http://10.251.0.37:5000\n⚠️ 请再次注意，我们在本次实验中讨论的容器是使用namespace和cgroups隔离的进程。所以理论上，这些容器只能在有Linux内核的操作系统上启动并运行。虽然可以通过Docker Desktop在macOS或Windows上启动容器，但本质上是因为Docker Desktop自动在你的机器上安装了一个Linux虚拟机，这些容器是启动在这个虚拟机里的。 Docker的简单使用 # 我们首先启动一个非常简单的Ubuntu容器，下面这条命令可以将会在一个隔离Ubuntu环境中执行/bin/bash进程：\ndocker run -it --rm ubuntu /bin/bash 可以看到，docker成功为我们启动了一个Ubuntu环境中的/bin/bash进程，当前我们所在的命令行环境已经不是原来主机上的环境了。可以使用下面的命令进行验证。\n容器环境拥有与宿主机不同的hostname：\n容器环境的发行版标记与宿主机不同（容器环境是Ubuntu，而主机是Debian）：\n容器环境中可以看到启动的进程只有/bin/bash，并且该进程为容器中的1号进程：\n我们在容器中对文件的增删并不会对宿主机中的环境造成任何影响：\n但是，当我们查看内核版本时，可以发现两者内核完全相同，这说明我们启动的容器不包含完整的操作系统，本质上只是一个被隔离的进程而已：\nDocker的架构 # 上图展示了一条docker命令是如何被执行的：\n用户输入命令\ndocker cli解析这条命令，转化成相应格式的请求，通过读写 Unix socket 的方式与docker engine通信，告诉docker engine应该执行怎样的操作\n正如我们前面提到的，镜像是容器的模板，我们使用docker命令创建容器需要先得到容器对应的镜像才行。当docker engine发现本机上没有对应的容器镜像时，就会根据镜像的名称从远程仓库（image registry）（可以将其想象成一个网盘）中下载对应的镜像到本地。实际上，你可能注意到我们在前面启动Ubuntu容器时就发生过这个下载操作：\n图中标出来的这段log清晰地说明了发生了什么：无法在本地找到image ‘ubuntu:latest’ ，于是从远程仓库中将这个镜像pull了下来。\n当镜像准备就绪后，docker engine就可以将任务向下传递，最终使用系统调用（namespace和cgroups）创建或管理容器\n⚠️ 上述步骤只是对docker工作流程的一个非常非常简化的讨论。 综上，我们可以看到docker实际上主要由三部分组成docker cli、docker engine、image registry。在后面的实验中，我们将逐步加深对着三部分的理解。\n💡 实际上，docker cli并不是必须的，任何可以读写docker engine所暴露的Unix socket（通常这个socket的文件名是/var/run/docker.sock）的程序都可以通过docker engine来实现docker的功能；甚至docker engine还可以对外暴露tcp端口，使外部程序使用特定的HTTP接口发起调用。 docker image # 镜像（Image）是容器的基础，所以我们先从镜像谈起。\n💡 从这里开始，我们将会涉及到大量的命令，这些命令看似繁杂，但平时常用的就那么几个，并且大部分都有规律，请同学们注意体会。当忘记某条命令时，可以随时使用docker —help查看帮助；或者访问 Docker官网的文档。注意，本实验的目的不是死记硬背这些命令，而是理解相关概念并对容器技术和Docker有个感性认识。 基本命令 # docker中镜像与相关的操作都包含在image子命令中，如：\ndocker image ls 可以列出当前机器的所有容器镜像：\nroot@template-debian11-base:/## docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest ba6acccedd29 5 months ago 72.8MB docker image pull \u0026lt;image_name\u0026gt; 可以从image registry中拉取名称为image_name的镜像：\nroot@template-debian11-base:/## docker image pull harbor.scs.buaa.edu.cn/library/mysql:8 8: Pulling from library/mysql b380bbd43752: Pull complete f23cbf2ecc5d: Pull complete 30cfc6c29c0a: Pull complete b38609286cbe: Pull complete 8211d9e66cd6: Pull complete 2313f9eeca4a: Pull complete 7eb487d00da0: Pull complete a5d2b117a938: Pull complete 1f6cb474cd1c: Pull complete 896b3fd2ab07: Pull complete 532e67ebb376: Pull complete 233c7958b33f: Pull complete Digest: sha256:882e55f40d61034a2bb8a1abab1353571ad2a33866f382350788eb34740528b5 Status: Downloaded newer image for harbor.scs.buaa.edu.cn/library/mysql:8 harbor.scs.buaa.edu.cn/library/mysql:8 镜像名 # 一般地，镜像名完整格式为{image registry地址}/{仓库名}/{镜像名}:TAG。\n例如本例中使用的 harbor.scs.buaa.edu.cn/library/mysql:8 ，其中：\nharbor.scs.buaa.edu.cn为image registry的地址，用来告诉docker去哪里pull这个镜像； library表示仓库名，表示这个镜像的所有者是谁； mysql表示镜像的名称； 8是镜像的TAG，一般用来表示镜像的版本号。如果一个镜像名没有TAG，那么将会被认为TAG为latest，即harbor.scs.buaa.edu.cn/library/mysql等同于harbor.scs.buaa.edu.cn/library/mysql:latest。 你可能已经注意到，我们在最初创建Ubuntu容器的时候也没那么麻烦，直接用ubuntu就表示了镜像名。那是因为当仅使用一个单词表示镜像名时，docker自动为它补上仓库名library、自家的image registry地址docker.io，以及TAGlatest；即ubuntu等同于docker.io/library/ubuntu:latest。\n镜像ID # 你可能还注意到，当使用docker image ls时，有一列叫做IMAGE ID，这一列中的字符串其实就是每个镜像对应的独一无二的ID，它是一个镜像独一无二的标识。我们可以使用这个ID对镜像做各种操作，比如删除一个镜像：docker image rm 3218b38490ce。\n为啥需要镜像ID呢？用镜像名标识镜像不好吗？不好。因为镜像名是可以任意改变的。我们可以使用docker tag命令来为镜像设置别名。比如，对上图中列出的ubuntu镜像，我们可以：\n可以看到，docker tag之后，生成了一个新的镜像areyouok，但这个镜像拥有和原来的ubuntu一样的ID，说明areyouok和ubuntu这两个不同的镜像名指向同一个实体。\nimage registry # 在介绍镜像名称时，同学们可能会疑惑，image registry有很多个吗，为啥还需要地址来标识？是的，image registry有很多个。image registry有的是公开的，任何人都可以访问，并从中拉取镜像；也有私有的，需要特殊的口令访问。目前，世界上最大的几个公开的image registry有Docker公司提供的 docker.io（目前也是世界上最大、使用最广泛的image registry，如果你需要通过浏览器访问的话，需要使用这个地址： hub.docker.com）、Redhat提供的 quay.io、Google提供的 gcr.io（很可惜，这个地址在国内被*了）；当然还有我们软院的image registry： harbor.scs.buaa.edu.cn。\nimage registry不仅可以下载已经存在的镜像，还可以上传和保存自己制作的新的镜像。任何人都可以在上述registry网站创建账户和自己的仓库。对于用户上传到image registry中的镜像，用户可以自行选择是否对其他用户公开访问（公开或私有）。如果是私有镜像，则需要在每次上传和下载镜像前，在本地执行 docker login操作。\n容器管理 # 可以使用docker ps命令查看当前机器上处于活跃状态的容器：\ndocker ps -a 可以列出所有状态的容器（包括活跃的和不活跃的）：\n从上图中的输出可以看出，和镜像一样，每个容器也都有一个唯一的ID作为标识，我们对容器的各项操作也是通过该ID进行的。除了ID之外，每个容器也都有一个独一无二的name，我们也可以使用name来唯一指定一个容器。\n在Docker的管理下，容器有以下6种状态：\n当用户输入docker run命令后，容器被创建，进入短暂的Created状态 当容器进程启动完毕后，容器进入Running状态，这表示容器正在正常工作 当用户使用docker stop显式地终止一个容器，或容器运行出错时，容器进入Exited状态。这个状态是不活跃的，处于这个状态的容器不会消耗任何资源 对于处于Exited的容器，可以被手动使用docker start重启，重新进入Running状态；也可能被Docker管理服务重启，短暂进入Restarting状态后，重新回到Running状态 可以手动使用docker pause暂停容器，此时容器将进入Paused状态。在这种状态中，容器将停止运行，即不会消耗任何CPU，但依旧会占据内存（以便随时从这个被pause的状态恢复运行） 当使用docker rm删除容器，但容器中的一些资源依旧被外部进程使用时（即无法立即删除时），容器将进入Dead状态 关于容器状态的更加详细的讨论，可以参考 这篇文章。\n启动容器 # 对于一般的用户，docker run命令是最常用也是最复杂的命令之一。docker run命令用于创建并运行容器。完整的命令参数及用法参见 Docker官方文档，下面我们会以两个例子来讨论一些常见的用法。\n基本结构 # docker run命令的结构是\ndocker run [一堆各种各样的参数] \u0026lt;image_name\u0026gt; [启动命令] [命令所使用的参数]。\n其中，只有镜像名image_name是必须的，其余全是可选的。\n启动命令 # 回想我们创建第一个docker镜像时使用的命令：docker run -it --rm ubuntu /bin/bash。这里的参数是-it —rm，镜像名是ubuntu，启动命令是/bin/bash，没有命令参数。\n这里/bin/bash的含义是，启动ubuntu容器后，执行/bin/bash命令，即启动一个bash shell。\n我们可以将这里的/bin/bash换成ls /usr看看效果：\n这时，启动命令是ls，命令参数是/usr，表示启动ubuntu容器，并在其中执行ls /usr命令，即列出/usr目录下的所有目录和文件。可以看到，执行效果确实如此。并且，请注意，因为我们没有执行/bin/bash命令，在ls命令执行完返回后，并没有进入容器的命令行中，而是回到了宿主机。\n不难发现，启动命令决定了容器启动后的具体行为。事实上，启动命令是可以省略的，如果省略的话，docker会执行镜像指定的启动命令（这个信息一般会作为元信息打包到镜像中）。\n-it # 参数i表示interactive，参数t表示创建一个虚拟的TTY（pseudo-TTY）。简单来说，-it参数可以让我们进入一个可以与容器进行交互的终端环境。\n比如，如果我们在启动命令中去掉-it，启动的ubuntu容器将会在后台运行，我们将无法和它交互：\n—-rm # 参数—-rm表示当容器退出后，自动删除容器。\n如果在执行容器时，不加—-rm参数，则当从容器退出后，容器进入Exited状态，继续存在在机器上（虽然此时为非活跃状态，不消耗任何资源），并且我们可以用docker restart等命令重启容器，或使用docker exec命令查看容器中的文件。\n在上图中，我们首先使用—rm参数启动了一个ubuntu容器，在与其交互后，退出容器（即结束/bin/bash进程，此时容器进入Exited状态）。因为使用了—rm参数，所以此时容器会被Docker自动删除，使用docker ps -a命令找不到该容器的存在。\n而如果启动命令中不加—rm参数，则容器退出后，将继续存在在机器上：\n这时，只能使用docker rm命令手动删除容器。\n-v # 到现在为止，我们使用的容器的文件系统都是与宿主机完全隔离的。但在很多时候，容器需要与宿主机共享一些目录，比如位于宿主机上的一些进程希望能方便地看到容器运行中产生的文件，或者通过修改一些文件来影响容器的行为。\n为了解决这一问题，我们可以使用-v参数将容器的某个目录和宿主机的某个目录绑定起来，使得容器在读写某个目录时，相当于在同时读写宿主机的某个目录。\n参数-v的使用方法是docker run -v \u0026lt;宿主机的目录\u0026gt;:\u0026lt;容器的目录\u0026gt; \u0026lt;image_name\u0026gt;。还是以前面使用的ubuntu容器举例。比如，我们希望将宿主机的/opt/mycontainer和容器中的/root目录绑定起来：\n可以看到，我们在容器的/root目录下创建了test.txt文件，然后在宿主机上的/opt/mycontianer确实观察到了该文件。同样地，在宿主机上对应目录的修改，也会被容器观察到：\n注意，-v参数是可以重复的，也就是说，可以在一条docker run命令中同时指定多个共享目录，比如：docker run -it --rm -v /opt/mycontainer:/root -v /another/host/dir:/another/contianer/dir ubuntu /bin/bash\n—-name # 参数—-name可以为启动的容器添加名字。我们之前的docker run命令都没有使用该参数，那么这时docker自己会为该容器分配一个随机字符串作为name。\n-p, —-publish # 参数—-publish（可以简写为-p）表示将容器的某个端口暴露到宿主机的某个端口。\n在详细讨论该参数之前，我们先从Nginx容器说起。 Nginx是目前应用非常广泛的网页服务器（当然它的用处不仅限于此），在一个启动了Nginx服务的系统中，可以通过80端口访问到网页。Nginx同样提供了开箱即用的docker镜像，只需要一条简单的命令docker run nginx就可以创建一个Nginx进程：\n可以看到，Nginx进程已经成功启动了，这说明它能在80端口监听HTTP请求。我们可以打开一个新的终端，使用curl尝试访问一下：\n访问失败！为啥呢？还记得“network namespace”吗？我们启动的Nginx容器进程和当前启动的curl进程不属于同一个network namespace！它们自然无法相互通信。当前这个Nginx进程对80端口的监听仅在它所在的network namespace有效。\n但这不满足需求啊，我们启动Nginx容器的目的就是向外提供服务啊，Nginx在自己的容器里自娱自乐怎么行！这时就要用到参数—-publish（可以简写为-p）了，它可以将容器的端口“发布”（publish）到宿主机的某个端口。用法是-p \u0026lt;宿主机端口\u0026gt;:\u0026lt;容器端口\u0026gt;。\n比如：\n这样，Nginx容器的80端口就会和宿主机的80端口绑定，访问宿主机的80端口也就相当于访问Nginx容器的80端口。不信试试看：\n成功🌶️！而且，对应的Nginx进程也打印出来请求日志：\n注意！宿主机的端口不一定要和容器的端口相同（一般都是不同的），比如：\n和-v参数类似，-p参数也可以同时有多个。\n-d, --detach # 大家可能注意到，上节中启动的Nginx容器都是在前台运行的，它在不断地输出日志，占据了整个终端，导致我们无法进一步和shell交互，以至于要执行curl操作时还得新打开一个终端。如果想终止这种输出，就得手动Ctrl+c终止Nginx容器。而且，如果此时关闭终端的话，也会同时退出这个容器。\n在很多时候我们不希望容器运行在前台，而只是想让它安安静静地作为后台进程提供服务。这时，就可以使用—-detach（简写为-d）参数。比如：\n可以看到，在输出了容器的ID后，Nginx安安静静地在后台运行了。\n那么，我们有时候又需要查看容器的日志怎么办呢？很简单，使用docker logs命令：\n进入容器内部 # 对于一个在后台运行的容器（如上节中的Nginx容器），如果想进入容器中查看当前容器的文件结构或执行一些命令用于debug，该怎么办呢？可以使用docker exec命令。\n比如，如果需要进入容器中的bash shell的话，首先需要找到容器对应的ID\n然后执行docker exec即可：\n构建新镜像 # 到现在为止，我们只是在使用他人已经提前做好的镜像。如何制造我们自己的镜像呢？下面给出两种方法。\ndocker commit # 假设我们正在使用一个ubuntu容器，并且在该容器的根目录下创建了一个非常重要的数据文件：\n如果我想把当前容器的状态保存下来，以便下次启动容器的时候可以重新使用该文件；或者我想把当前容器发送给别人，让别人也看到我当前看到的容器的样子，该怎么办呢？可以使用docker commit命令将当前容器打包成一个新的镜像。\n重新打开一个终端，查看一下当前容器的ID：\n然后直接docker commit \u0026lt;container_id\u0026gt; \u0026lt;new_image_name\u0026gt;即可：\n这时，使用docker image ls可以看到这个新生成的镜像：\n我们尝试运行一下这个新生成的镜像，可以看到刚才创建的文件果然存在：\n💡 实际上，docker commit还支持很多参数，详情请见 官方文档。 Dockerfile # docker commit虽然可以非常直观地从当前容器创建一个新的镜像。但整个过程不够规范，也很难实现自动化，一般情况下，我们都是使用Dockerfile来构建镜像的。\n所谓的Dockerfile，其实就是一个配置文件，里面描述了构建镜像的步骤。对于上一小节中构建出的镜像，如果使用Dockerfile来写的话，是这样的：\nFROM ubuntu RUN echo \u0026#39;something important\u0026#39; \u0026gt; /important.data 很显然，上述文件内容是自解释的。一般Dockerfile的开头一条FROM语句，表示从以哪个镜像为基础进行构建。下面的RUN语句表示在容器中执行一条命令。\n可以将上述Dockerfile保存在一个干净的目录中，然后在该目录中执行docker build -t my_new_image_from_dockerfile -f ./Dockerfile .（请注意，命令的最后有个.）：\n可以看到，我们成功build了一个新的镜像。下面详细解释一下上面那条docker build命令的含义：\n-t表示要构建的新的镜像的名称 -f表示Dockerfile文件的路径 命令中最后的单词表示表示构建镜像的上下文路径，上图中这个最后的单词是.，则表示上下文路径是当前目录。在镜像构建开始的时候，docker cli会把“上下文路径”指定的目录中的所有内容打包，然后整个发送给docker engine 事实上，Dockerfile还支持非常多的指令，具体请查阅 官方文档。\n发布镜像 # 截止目前，我们构建的镜像全是在本机上，别人根本访问不到，也没法使用。我们可以将镜像push到image registry上，然后通知对方从该image registry拉取即可。具体可以参考 docker push的文档。\n"},{"id":3,"href":"/doc/ns-labs/resources/os/","title":"操作系统","section":"Resources","content":" 操作系统 # 常见操作系统 ISO 镜像可至官网、 MSDN，我告诉你、 Next, ITELLYOU 等网站上去下载。\nWindows Server 2019 # ED2K 下载：ed2k://|file|cn_windows_server_2019_updated_july_2020_x64_dvd_2c9b67da.iso|5675251712|19AE348F0D321785007D95B7D2FAF320|/\n使用 KMS 激活方式如下：\n# 使用管理员身份打开PowerShell DISM /online /Set-Edition:ServerDatacenter /ProductKey:WMDGN-G9PQG-XVVXX-R3X43-63DFG /AcceptEula # 打开 CMD (管理员) slmgr.vbs /ipk WMDGN-G9PQG-XVVXX-R3X43-63DFG slmgr.vbs /skms kms.teevee.asia slmgr.vbs /ato "},{"id":4,"href":"/doc/cloud-labs/cloud/kube-single-3/","title":"Kubernetes实验（三）","section":"云计算实验","content":" Kubernetes实验（三） Kubernetes的基本使用 # 实验目的 # 了解Kubernetes的各种特性 掌握Kubernetes的常用功能 注意事项\n本次分配的机器的账户和密码为： buaa: \u0026amp;shieshuyuan21 root: \u0026amp;\u0026amp;shieshuyuan21 务必首先修改机器的root和buaa账户的密码\n请务必阅读 虚拟机使用说明。\n分配的虚拟机中，已经安装了Docker，无需重复安装；并设置了Docker镜像地址（该地址指向校内地址），理论上docker.io中的镜像不用联网即可拉取。例如可以直接在虚拟机上docker pull nginx。\n创建Kubernetes集群\n在实验开始前，请利用云平台提供的虚拟机或者自己的本地资源，创建一个至少包含两节点的Kubernetes集群。\n详情请参考： 附录：创建Kubernetes集群\n配置资源的两种方式 # 使用命令 # 回想之前我们之前初始化Kubernetes集群后，执行过的命令：\nkubectl create deployment nginx-test --image=harbor.scs.buaa.edu.cn/library/nginx --replicas=2 --port=80 使用kubectl命令创建资源时，会将Pod的配置都写入命令参数中。以我们执行过的kubectl create命令（在集群中使用指定镜像启动容器）为例，命令中包含了镜像名（--image）、伸缩情况（--replicas）。当然，kubectl命令还有许多其他的功能，详情可以在 Kubernetes文档-kubectl命令看到。\n直接使用kubectl创建资源简单、直观、快捷，很适合临时测试或者试验。但实际上，创建资源时通常需要一系列的配置，如果单纯使用kubectl命令会比较麻烦，因此通常在配置资源会使用yaml配置文件。\n使用配置文件 # 在之前的实验中，我们使用了这个命令来安装Weave Scope\nkubectl apply -f https://git-v1.scs.buaa.edu.cn/iobs/static_files/raw/main/kube/weave_scope/scope.yml 这个命令实际上从后面的地址读取了一个yaml配置文件，并根据这个文件来配置资源。如果你将这个文件打开，你会看到这样的内容（省略了很多内容）：\napiVersion: v1 kind: List items: - apiVersion: v1 kind: Namespace metadata: name: weave annotations: cloud.weave.works/version: v1.0.0-302-g76376bb - apiVersion: v1 kind: ServiceAccount metadata: name: weave-scope labels: name: weave-scope namespace: weave YAML # YAML是一种可读性高、用来表达数据序列化的格式。YAML使用空白字符和分行来分隔数据，使用一些符号来标记清单、散列表、标量等资料形态。\n常用的YAML语法：\n对象（键值对）：用冒号结构表示。注意，键值对的key无需使用引号。 纯量：数值直接以字面量表示，布尔值用true和false表示，当值为空时，用~表示。一般情况下，字符串可以不用引号，但是当字符串中包含空格时则必须使用引号（单双均可）。字符串也可以写成多行，但从第二行开始，必须有一个单空格缩进。换行符会被转换为空格。对于多行字符串，使用|可以保留换行符，+表示保留文字块末尾的换行，-表示删除字符串末尾的换行。 数组：数组的成员前以-标记，每个成员一行；也可以在同一行内使用[]。例如： 事实上，我们常用的用于数据传递的JSON语法是YAML1.2版的子集，将上面的YAML文件转换为我们可能更加熟悉的JSON格式的话：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;List\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;weave\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;cloud.weave.works/version\u0026#34;: \u0026#34;v1.0.0-302-g76376bb\u0026#34; } } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ServiceAccount\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;weave-scope\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;weave-scope\u0026#34; }, \u0026#34;namespace\u0026#34;: \u0026#34;weave\u0026#34; } } ] } 这个例子应该可以帮助你理解YAML格式。节选的YAML配置文件会让Kubernetes进行如下工作：\n创建一个Namespace，名字为\u0026quot;weave\u0026quot;，并且将版本信息加到了注释上 创建一个ServiceAccount，名字为\u0026quot;weave-scope\u0026quot;，并且也注释了一些信息 调度Pod # 前面我们已经稍微了解到，Kubernetes通过各种Controller来管理Pod的生命周期，Kubernetes也提供了数种不同功能的内置Controller。每个控制器管理集群状态的一个特定方面。最常见的情况是：一个特定的控制器使用一种类型的资源作为它的期望状态，控制器管理控制另外一种类型的资源向它的期望状态发展。\nDeployment # 回忆我们运行的第一个Pod：\n运行一个Nginx镜像 ```bash kubectl create deployment nginx-test --image=harbor.scs.buaa.edu.cn/library/nginx --replicas=2 --port=80 ``` 查看创建结果 ```bash sudo kubectl get deployment nginx-test ``` 可以发现我们在查看它的时候使用的是get deployment命令。事实上我们已经部署了包含两个副本的Deployment，它的名字就是nginx-test。\n一个Deployment控制器为Pods和ReplicaSets提供描述性的更新方式：描述Deployment中的期望状态，并且Deployment控制器以受控速率更改实际状态，以达到期望状态。\n在这个例子中，我们的“期望状态”就是启动一个有两个副本的Nginx镜像。在这个过程中，我们创建了一个Deployment对象，通过Deployment生成了对应的ReplicaSet并完成了Pod副本的创建过程。除此之外，Deployment的主要功能还有以下几个：\n检查Deployment的状态来看部署动作是否完成（Pod副本的数量是否达到了预期的值）\n例如，在本例中，如果你执行kubectl create命令后马上执行kubectl get deployment命令，你可能会看到READY一栏并不是2/2，AVAILABLE一栏也不是2，也就是说，部署动作尚未完成，Pod副本数量没有达到预期的值（2）。 更新Deployment以创建新的Pod（比如镜像升级）\n回滚之前的Deployment版本\n暂停Deployment（修改Pod中的镜像信息，之后再恢复Deployment进行新的发布）\n扩展Deployment（以应对高负载情况）\n清理不再需要的旧版本ReplicaSet\n查看更详细的信息：kubectl describe deployment nginx-test\nName: nginx-test Namespace: default CreationTimestamp: Fri, 14 May 2021 15:46:51 +0800 Labels: app=nginx-test Annotations: deployment.kubernetes.io/revision: 1 Selector: app=nginx-test Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx-test Containers: nginx: Image: harbor.scs.buaa.edu.cn/library/nginx Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-test-6884fd56dd (2/2 replicas created) Events: \u0026lt;none\u0026gt; 大多数内容都是自解释的，比如Deployment的名字、命名空间、创建时间等等。我们主要关注最后的一些信息：NewReplicaSet和Events。这些信息告诉我们Deployment创建了一个新的ReplicaSet，其中包含2个副本。\nDeployment配置文件 # 以之前的nginx-test为例，它的配置文件可以写成\n#nginx-test apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test spec: replicas: 2 selector: matchLabels: run: nginx-test template: metadata: labels: run: nginx-test spec: containers: - name: nginx image: harbor.scs.buaa.edu.cn/library/nginx ports: - containerPort: 80 protocol: TCP 将上述内容写入文件nginx.yml中，然后执行：\nkubectl apply -f nginx.yml 其效果与我们在实验三中直接执行kubectl create deployment nginx-test --image=harbor.scs.buaa.edu.cn/library/nginx --replicas=2 --port=80效果是完全一样的。\n在上述这个文件nginx.yml中：\napiVersion 当前配置所使用的API版本。其格式为组/版本号，例如刚刚使用的配置文件中，使用Deployment要表明API版本为apps/v1。 kind 当前配置资源的类型 metadata 当前配置资源的元数据。其中name是必填项。 spec 当前资源的规格，此处为Deployment的规格 replicas 副本数 selector 选择控制的Pod matchLabels template Pod的模板 metadata Pod的元数据，至少要定义一个label。每个label是一个键值对，key和value都可以自定义。为了和使用kubectl run创建的Deployment保持一致，这里用了run: nginx-test。 labels spec Pod的规格。此部分定义Pod中每一个容器的属性，对于每一个容器，name和image是必须的。 ports中详细指明了该容器需要向外暴露哪些端口。 ReplicaSet # ReplicaSet确保任何时间都有指定数量的Pod副本在运行。虽然ReplicaSet可以独立使用，但如今它主要被Deployment用作协调Pod创建、删除和更新的机制。Deployment是一个更高级的概念，它管理ReplicaSet，并向Pod提供声明式的更新以及许多其他有用的功能（比如版本记录、回滚、暂停升级等高级特性）。实际上，我们可能永远不需要操作ReplicaSet对象，而是使用Deployment。\n查看由Deployment创建的ReplicaSet\nkubectl get replicaset NAME DESIRED CURRENT READY AGE nginx-test-6884fd56dd 2 2 2 6h7m 查看ReplicaSet的详细信息\nkubectl describe replicaset nginx-test-6884fd56dd Name: nginx-test-6884fd56dd Namespace: default Selector: app=nginx-test,pod-template-hash=6884fd56dd Labels: app=nginx-test pod-template-hash=6884fd56dd Annotations: deployment.kubernetes.io/desired-replicas: 2 deployment.kubernetes.io/max-replicas: 3 deployment.kubernetes.io/revision: 1 Controlled By: Deployment/nginx-test Replicas: 2 current / 2 desired Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=nginx-test pod-template-hash=6884fd56dd Containers: nginx: Image: harbor.scs.buaa.edu.cn/library/nginx Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; Controlled By指明了此ReplicaSet是由我们之前创建的Deployment所创建的。那么Pod是由什么创建的呢？在之前我们查看Pod时已经知道了Pod的Name，直接查看Pod的详细信息：\n查看Pod详细信息\nkubectl describe pod nginx-test-6884fd56dd-8j7sg Name: nginx-test-6884fd56dd-8j7sg Namespace: default Priority: 0 Node: k8s-master/10.251.254.114 Start Time: Fri, 14 May 2021 15:46:51 +0800 Labels: app=nginx-test pod-template-hash=6884fd56dd Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.42.0.17 IPs: IP: 10.42.0.17 Controlled By: ReplicaSet/nginx-test-6884fd56dd Containers: nginx: Container ID: containerd://135a7109de612ec0b0e35e9b9cf3599ffd66f8e383d3a0b02c86d514fd4288fd Image: harbor.scs.buaa.edu.cn/library/nginx Image ID: harbor.scs.buaa.edu.cn/library/nginx@sha256:42bba58a1c5a6e2039af02302ba06ee66c446e9547cbfb0da33f4267638cdb53 Port: 80/TCP Host Port: 0/TCP State: Running Started: Fri, 14 May 2021 15:46:52 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2fhnf (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-2fhnf: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: \u0026lt;nil\u0026gt; DownwardAPI: true QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: \u0026lt;none\u0026gt; 同样地，从Controlled By中我们可以看到，这里的Pod是由ReplicaSet创建的。根据以上的创建结果，我们可以大概推断出Kubernetes在这个过程中都做了什么：\n用户通过kubectl创建了Deployment，名为nginx-test 该Deployment创建了ReplicaSet，名为nginx-test-6884fd56dd 该ReplicaSet创建了两个Pod，名为nginx-test-6884fd56dd-8j7sg和nginx-test-6884fd56dd-rqjxv 同时我们也可以发现，Kubernetes的对象命名方式：对象名=父对象名+字符串\n现在，使用kubectl create创建的Pod已经完成了它的使命，之后我们会用新的方式创建新的Pod。\n删除之前部署的Nginx\nsudo kubectl delete deployment nginx-test 伸缩 # 伸缩是指在线增加或减少Pod的副本数。\n使用前面所说的yml文件的方式再次创建一个名为nginx-test的Deployment。\nkubectl apply -f nginx.yml 我们创建的Deployment有两个副本，执行kubectl get pod -o wide可以看到，两个副本分别运行在master节点和普通节点上：\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 17s 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Running 0 17s 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 修改yaml配置文件，将replicas改为5，并再次执行\nkubectl apply -f nginx.yml 再次查看Pod\nkubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 66s 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Running 0 66s 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-8fl7v 1/1 Running 0 5s 10.42.1.93 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-dgqn5 1/1 Running 0 5s 10.42.1.95 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-zfpj2 1/1 Running 0 5s 10.42.1.94 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Pod副本数已经变为了5个。\n再次修改yaml配置文件，将replicas修改为3，并应用更改，再次查看Pod\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 106s 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Running 0 106s 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-dgqn5 1/1 Running 0 45s 10.42.1.95 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Pod副本数变为了3个。\n故障转移 # 由于服务器资源紧张的原因，本节涉及的开关机操作比较危险，很可能导致虚拟机长时间无响应，因此，本节内容只阅读理解相关内容即可，可以不实践操作。\n这里，我们使用直接关机的方式来模拟Kubernetes集群中有一台机器故障的情况。\n将node节点关机，这一步可以在云平台上进行，也可以直接登录node节点，使用sudo shutdown now来直接关机。\n关机后，master节点会找不到node节点，因此node节点会变为NotReady状态。\n可能需要等待一段时间才能看到节点状态变为NotReady。不过如果用curl访问node节点上的Nginx服务器，可以马上看到Nginx已经无响应了。\n查看节点状态 kubectl get node -o wide\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master Ready control-plane,master 14d v1.21.0+k3s1 10.251.254.114 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-73-generic containerd://1.4.4-k3s1 k8s-node NotReady \u0026lt;none\u0026gt; 14d v1.21.0+k3s1 10.251.254.110 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-73-generic containerd://1.4.4-k3s1 可以看到，node节点的状态已经更改为了NotReady\n稍等几分钟后，查看Pod kubectl get pod -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 32m 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-dgqn5 1/1 Terminating 0 31m 10.42.1.95 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Terminating 0 32m 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-n7mmj 1/1 Running 0 5m52s 10.42.0.16 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-jndr8 1/1 Running 0 5m52s 10.42.0.17 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 原来在node节点上的两个Pod副本，都处于Terminating状态。并且master节点上新增了两个Pod副本。处于正常运行状态的Pod副本的总数依然维持在三个。\n进入云平台，打开刚才关掉的服务器，一段时间后再次查看node列表和Pod列表，可以看到节点已经恢复为了Ready状态，处于Terminating的Pod已经被删除。\nJob # Job会创建一个或多个Pod，并确保指定数量的Pod成功终止。当Pod成功完成时，Job将追踪成功完成的情况。当达到指定的成功完成次数时，Job就完成了。删除一个Job将清除它所创建的Pod。Job一般用于定义并启动一个批处理任务。批处理任务通常并行（或串行）启动多个计算进程去处理一批工作项，处理完成后，整个批处理任务结束。\nKubernetes支持一下几种Job:\n非并行Job: 通常创建一个Pod直至其成功结束 固定结束次数的Job: 设置.spec.completions，创建多个Pod，直到.spec.completions个Pod成功结束 带有工作队列的并行Job: 设置.spec.Parallelism但不设置.spec.completions，当所有Pod结束并且至少一个成功时，Job就认为是成功。 一个运行一次的Job例子:\napiVersion: batch/v1 kind: Job metadata: name: pi spec: # completions: 1 # parallelism: 1 template: metadata: name: pi spec: containers: - name: pi image: harbor.scs.buaa.edu.cn/library/perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] restartPolicy: Never Job yaml格式 # RestartPolicy: Pod的重启策略，在这里仅支持Never或OnFailure completions: 标志Job结束需要成功运行的Pod个数，默认为1 parallelism: 标志并行运行的Pod的个数，默认为1 activeDeadlineSeconds: 标志失败Pod的重试最大时间，超过这个时间不会继续重试 container中的command: 以string数组的格式输入待执行指令 (样例中为perl输出2000位数字的pi的语句) 运行该job:\nkubectl create -f job.yaml 几分钟后，可以看到pod的创建情况 kubectl get pod：\nNAME READY STATUS RESTARTS AGE pi-5qng4 0/1 Completed 0 2m30s 此时Job已经按照预设的任务完成了，使用kubectl logs \u0026lt;podname\u0026gt;查看Pod的日志也能看到按照预设的command输出的2000位数字的$\\pi$。\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901 Cron Job定时任务 # Kubernetes还提供了定时计划任务：\napiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: \u0026#34;*/1 * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: hello image: harbor.scs.buaa.edu.cn/library/busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 与一般的Job相比，CronJob的主要不同点在于额外有了schedule及jobTemplate两个字段。schedule是定时表达式，其格式与Linux Cron的格式基本相同，如示例的cron表达式为每分钟执行一次。jobTemplate一节与Job中的template格式相同。\n创建定时任务 kubectl create -f cron.yml\n查看定时任务 kubectl get cronjob\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 2s 47s get命令只会输出上一次创建Job的时间，而不是所有。可以使用watch参数来更加直观地观察Job的创建过程 kubectl get jobs --watch\nNAME COMPLETIONS DURATION AGE hello-27032437 1/1 1s 2m34s hello-27032438 1/1 1s 94s hello-27032439 1/1 1s 34s 可以看到，每隔60s就会创建一个容器。\n获取其中一个job的Pod kubectl get pods --selector=job-name=hello-27032439 --output=jsonpath={.items[*].metadata.name}\n执行上述命令后，会输出对应pod的名字\nhello-27032439-cxt45 查看Pod输出 kubectl logs hello-27032439-cxt45\nTue May 25 12:59:24 UTC 2021 Hello from the Kubernetes cluster 最后，记得将计划任务删除，否则会一直运行。kubectl delete -f cron.yml 其他Controller # DaemonSet # DaemonSet用于管理在集群中每个Node上运行且仅运行一份Pod的副本实例，一般来说，在以下情形中会使用到DaemonSet：\n在每个Node上都运行一个存储进程 在每个Node上都运行一个日志采集程序 在每个Node上都运行一个性能监控程序 StatefulSet # StatefulSet用来搭建有状态的应用集群（比如MySQL、MongoDB等）。Kubernetes会保证StatefulSet中各应用实例在创建和运行的过程中，都具有固定的身份标识和独立的后端存储；还支持在运行时对集群规模进行扩容、保障集群的高可用等功能。\nService # Service可以将运行在一组Pods上的应用程序公开为网络服务，简单地实现服务发现、负载均衡等功能。\nk8s的Pods具有自己的生命周期，同一时刻运行的Pod集合与稍后运行的Pod集合很有可能不同（如发生更新、node故障等），Pods的IP地址可能会随时发生变化。这就会导致一个问题：如果一组后端Pods为集群内其他前端Pods提供功能，那么前端Pods该如何找出并跟踪需要连接的IP地址？通过Service，能够解耦这种关联，方便的通过Service地址访问到相应的Pods，前端不应该也没必要知道怎么访问、访问到的具体是哪一个Pod。\nService一共有4种类型：\nClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort： 通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。仅作了解。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如，在集群内查找my-service.my-namespace.svc时，k8s DNS service只返回foo.bar.example.com这样的CNAME record）。没有任何类型代理被创建，网络流量发生在DNS层面。由于ExternalName要求kube-dns而我们使用的是coredns，也只作了解。 创建Service # Service通常通过selector来选择被访问的Pod。\n继续沿用我们之前所创建的nginx-test。查看Pod详细信息：\nkubectl describe pod nginx-test-6478975c66-jtd2r\n在Labels栏能看到如下的标签（selector使用该标签来选择被访问的Pod）：\nLabels: pod-template-hash=6478975c66 run=nginx-test 因此可以通过下列yaml文件创建Service (将下面的内容写入nginx-service.yaml)\n#nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-test-service labels: svc: nginx-test-svc spec: ports: - port: 80 protocol: TCP selector: run: nginx-test port：Service暴露在集群IP上的端口。集群内通过\u0026lt;clusterIP\u0026gt;:\u0026lt;port\u0026gt;可以访问Service。 targetPort：被代理的Pod上的端口。默认与port相同。 nodePort：Service暴露在节点外部IP上的端口。集群外通过\u0026lt;nodeIP\u0026gt;:\u0026lt;nodePort\u0026gt;可以访问Service。仅在spec.type=NodePort时可用(spec.type默认为ClusterIP)。 name：端口名称，当Service具有多个端口时必须为每个端口提供唯一且无歧义的端口名称。 创建Service\nkubectl apply -f nginx-service.yaml 查看service kubectl get svc\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 15d nginx-test-service ClusterIP 10.43.149.18 \u0026lt;none\u0026gt; 80/TCP 11s 可以看到，第二个就是我们刚才创建的service，其中，它有一个cluster-ip：10.43.149.18。\n验证是否可以通过Service访问Pod，注意，上述这个IP是“cluster-ip”，也就是说，它是一个集群内ip，因此，只能在集群中的机器上访问：\ncurl 10.43.149.18 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 查看当前三个Pods的IP地址 kubectl get pod -l run=nginx-test -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 5h48m 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-n7mmj 1/1 Running 0 5h22m 10.42.0.16 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-jndr8 1/1 Running 0 5h22m 10.42.0.17 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 删除这三个Pods并等待Deployment重新创建\n~$ kubectl delete pods -l run=nginx-test pod \u0026#34;nginx-test-79cd7499bf-vrlss\u0026#34; deleted pod \u0026#34;nginx-test-79cd7499bf-zd42s\u0026#34; deleted ~$ kubectl get pod -l run=nginx-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-7ktl4 1/1 Running 0 21s 10.42.1.109 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-cffrr 1/1 Running 0 21s 10.42.1.111 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-jssj4 1/1 Running 0 21s 10.42.1.110 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 可以看到重新创建的三个Pods的IP地址已经发生变化，再次通过Service，仍能访问对应的Pod\ncurl 10.97.91.103 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 暴露端口 # 之前创建的Service并没有指定类型，因此为默认的ClusterIP，只能在集群内部访问。如果需要将服务端口暴露在公网，可以使用NodePort类型。\n将nginx-service.yaml修改为下面的内容\n#nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-test-service labels: svc: nginx-test-svc spec: type: NodePort ports: - port: 80 nodePort: 32180 protocol: TCP name: http selector: run: nginx-test 修改Service kubectl apply -f nginx-service.yaml\n查看service kubectl get svc nginx-test-service\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-test-service NodePort 10.43.115.127 \u0026lt;none\u0026gt; 80:32180/TCP 17s 此时，从集群内任一节点IP的32180端口均可访问到某个Pod的80端口。\n比如，你的两台虚拟机的IP分别为10.255.9.80和10.255.9.81，那么，你在校园网内的任何一台机器上，执行curl http://10.255.9.80:32180或者curl http://10.255.9.81:32180，都能得到如下的输出：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 或者在浏览器中也可以访问：\n可以尝试删除Pods并等待新的Pods创建完成，仍可以通过上述方式访问。\n滚动更新 # 为了在更新服务的同时不中断服务，kubectl支持滚动更新，它一次更新一个Pod，而不是停止整个服务。\n使用Deployment可以查看升级详细进度和状态，当升级出现问题的时候，可以使用回滚操作回滚到指定的版本，每一次对Deployment的操作，都会保存下来，方便进行回滚操作，另外对于每一次升级都可以随时暂停和启动，拥有多种升级方案：Recreate删除现在的Pod，重新创建；RollingUpdate滚动升级，逐步替换现有Pod，对于生产环境的服务升级，显然这是一种最好的方式。\n创建Deployment\napiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: harbor.scs.buaa.edu.cn/library/nginx:1.16 使用kubectl create -f nginx-deploy.yaml，由于replicas为3，因此会创建三个Pod\nNAME READY STATUS RESTARTS AGE nginx-77bf7f47fd-9q7fw 1/1 Running 0 80s nginx-77bf7f47fd-567nf 1/1 Running 0 80s nginx-77bf7f47fd-k795n 1/1 Running 0 80s 此时尝试访问Nginx主页，会看到版本提示。\n注意：使用的镜像并不是官方原版镜像，而是修改了默认的index.html后的镜像。访问Nginx主页，既可以自己创建一个service进行访问，也可以查看pod的IP后直接在虚拟机上使用curl访问。\n通过配置文件更新：\n在刚刚的配置文件中将镜像修改为harbor.scs.buaa.edu.cn/library/nginx:1.17，然后在spec中添加滚动升级策略，改动后如下\napiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: minReadySeconds: 5 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: harbor.scs.buaa.edu.cn/library/nginx:1.17 minReadySeconds: Kubernetes在等待设置的时间后才进行升级 如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了 如果没有设置该值，在某些极端情况下可能会造成服务服务正常运行 maxSurge: 升级过程中最多可以比原先设置多出的POD数量 例如：maxSurage=1，replicas=5,则表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有5+1个POD。 maxUnavaible: 升级过程中最多有多少个POD处于无法提供服务的状态 当maxSurge不为0时，该值也不能为0 例如：maxUnavaible=1，则表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态。 然后执行命令 kubectl apply -f nginx-deploy.yaml\n接着查看deployment的升级情况 kubectl rollout status deploy/nginx\nWaiting for deployment \u0026#34;nginx\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx\u0026#34; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026#34;nginx\u0026#34; rollout to finish: 2 of 3 updated replicas are available... Waiting for deployment \u0026#34;nginx\u0026#34; rollout to finish: 2 of 3 updated replicas are available... deployment \u0026#34;nginx\u0026#34; successfully rolled out 执行kubectl rollout status deploy/nginx可以实时观测滚动更新的进度，在更新过程中，可以使用kubectl rollout pause(resume) deployment \u0026lt;deployment\u0026gt;来暂停(继续)更新\n更新结束后，查看Replica Set的状态 kubectl get replicaset\nNAME DESIRED CURRENT READY AGE nginx-77bf7f47fd 0 0 0 8m28s nginx-5b8c7c5877 3 3 3 3m7s 可以看到旧的Replica Set和新的Replica Set的状态，此时查看任意一个Pod的镜像信息\n...... Containers: nginx: Container ID: containerd://0790761a1248fcb4dc8d9432aebeb37ff7ee96b7267a83fe0172812323764e85 Image: harbor.scs.buaa.edu.cn/library/nginx:1.17 Image ID: harbor.scs.buaa.edu.cn/library/nginx@sha256:6f3b7f003243dca41dae46fe442e826df08e056a76cdd3e11e6f41cdaef ...... 其中image的信息已经得到改动了。\n此时再次尝试访问Nginx主页，会看到版本已经变化。\n回滚Deployment 首先，查看Deployment的升级历史 kubectl rollout history deploy/nginx\ndeployment.apps/nginx REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; 其中，1和2为历史版本，可以带上参数来查看该次的版本信息 kubectl rollout history deploy/nginx --revision=1\ndeployment.apps/nginx with revision #1 Pod Template: Labels:\tapp=nginx pod-template-hash=77bf7f47fd Containers: nginx: Image:\tharbor.scs.buaa.edu.cn/library/nginx:1.16 Port:\t\u0026lt;none\u0026gt; Host Port:\t\u0026lt;none\u0026gt; Environment:\t\u0026lt;none\u0026gt; Mounts:\t\u0026lt;none\u0026gt; Volumes:\t\u0026lt;none\u0026gt; 可以使用kubectl rollout undo deploy/nginx来回退到上一个版本，也可以在后面加上参数--to-revision=3来回退到3所指定的历史版本\n数据管理 # Pod中的数据默认并没有进行持久化，它们会随着Pod的销毁而一同被消灭。 我们都知道，Pod并不总是稳定可靠的，它有可能会被频繁销毁并重建，我们并不希望重要的数据（例如MySQL中的数据）跟着Pod一同被销毁。这就需要引入一种持久的存储系统。\nVolume和Persistent Volume Kubernetes提供了两种储存介质，它们是Volume和Persistent Volume。简单来说，Volume中的数据不能被持久化，Pod销毁，数据消失；而Persistent Volume中的数据则独立于Pod，即使Pod销毁，数据依然可以永久保存，除非你不希望。\n常用的(Persistent) Volume emptyDir(非Persistent Volume) hostPath(Persistent Volume) nfs(Persistent Volume) 使用hostPath # 使用hostPath，将容器的/storage目录挂载到容器所在主机的/home/storage目录下\napiVersion: v1 kind: Pod metadata: name: hostpath-test spec: containers: - name: busybox image: harbor.scs.buaa.edu.cn/library/busybox:1.24 volumeMounts: - name: storage mountPath: /storage #将容器的 /storage 目录作为挂载点 args: - /bin/sh - -c - echo \u0026#34;test file\u0026#34; \u0026gt; /storage/test.txt restartPolicy: \u0026#34;Never\u0026#34; volumes: - name: storage hostPath: path: /home/buaa/storage # 挂载到主机的 /home/buaa/storage目录下 volumeMounts: container中需要被mount的目录 volumes: 根据name来对应container中的volumeMounts并选择mount到本地的路径 使用kubectl get pod -o wide可查看pod所在主机，进入主机查看/home/buaa/storage下的文件\n$ ls /home/buaa/storage test.txt $ cat /home/storage/test.txt test file hostPath的缺点为若使用一个pod需要跨主机到另外一台主机上重建，数据将会丢失，这时我们需要PersistentVolume来进行持久化\n使用PV和PVC # 两者的关系为：PV用于定义存储，而PVC用于向已有存储申请存储空间\n用nfs来实现PV比较容易，因此用nfs来做实例\nnfs:网络文件系统（英语：Network File System，缩写作 NFS） nfs需要一个server端，一个client端，对于实验的Kubernetes集群来说，可将Master节点作为server同时为client(因为设置了Master节点开启工作负载)，其他所有从节点为client。\n配置nfs server端 # 在nfs服务器上安装nfs-kernel-server （理论上实验用的机器已经安装，如果没有，请自行联网安装）\napt install nfs-kernel-server 首先创建一个用于存放数据的文件夹\nmkdir /data 接下来修改nfs的配置文件\n$ vim /etc/exports #添加如下内容 /data *(rw,sync,no_root_squash) 其中：\n/data： 共享的目录 * ： 谁可以访问(这里设置为所有人能访问) (rw,sync,no_root_squash): 权限设置 rw: 能读写 sync: 同步 no_root_squash: 不降低root用户的权限(不安全，不过本次实验选择这样做更方便) 重启nfs服务：\nservice nfs-kernel-server restart 在所有节点安装nfs-common （理论上实验用的机器已经安装，如果没有，请自行联网安装）\napt install nfs-common 安装好后可使用如下指令查看是否能看到nfs服务器的挂载点(x.x.x.x为nfs服务器的ip地址)\n$ showmount -e x.x.x.x Export list for x.x.x.x: /data * 这时候说明已经能够访问到这个挂载点了，使用以下指令将挂载点挂载到本地的/mnt路径下(x.x.x.x同样为nfs服务器地址)\nmount x.x.x.x:/data /mnt 无提示则说明挂载成功，此时在nfs客户端上进行测试\n$ cd /mnt $ touch success\n若看到nfs服务器上/data目录中出现success文件，则说明nfs已经部署成功，将集群所有机器都进行挂载(包括服务器本机)，在一个挂载点删除后，所有客户端和服务器也会同步删除\n创建PV # 准备好nfs环境后，创建PersistentVolume(pv)\napiVersion: v1 kind: PersistentVolume metadata: name: nfspv spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /data #要mount到的路径 server: x.x.x.x #nfs服务器ip accessModes: ReadWriteOnce \u0026ndash; PV能以read-write模式mount到单个节点 ReadOnlyMany \u0026ndash; PV能以read-only模式mount到多个节点 ReadWriteMany \u0026ndash; PV能以read-write模式mount到多个节点 persistentVolumeReclaimPolicy Retain \u0026ndash; 需要管理员手工回收 Recycle \u0026ndash; 清除PV中的数据，效果相当于执行rm -rf /thevolume/* Delete \u0026ndash; 删除Storage Provider上的对应存储资源 nfs path的路径要存在，若不存在，Pod无法正常运行，请提前手动创建好(动态pv无需手动创建) 使用指令kubectl create -f \u0026lt;filename\u0026gt;按照上述yaml文件创建PV后\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfspv 1Gi RWO Retain Available nfs 3s 接下来创建PVC\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfspvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs 创建后查看PVC和PV的情况\nkubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfspvc Bound nfspv 1Gi RWO nfs 5s kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfspv 1Gi RWO Retain Bound default/nfspvc nfs 82m 发现pvc已经和pv绑定了，因为Kubernetes将查找满足申领要求的pv，并将pvc绑定到具有相同StorageClass的适当的pv上\n接下来在裸Pod中使用PVC\nkind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: harbor.scs.buaa.edu.cn/library/busybox:1.24 command: - \u0026#34;/bin/sh\u0026#34; args: - \u0026#34;-c\u0026#34; - \u0026#34;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34; volumeMounts: - name: nfs-pvc mountPath: \u0026#34;/mnt\u0026#34; #此处为容器内的被挂载点，根据pv中的设置，将这个目录直接挂载到nfs服务器的/data目录下 restartPolicy: \u0026#34;Never\u0026#34; volumes: - name: nfs-pvc persistentVolumeClaim: claimName: nfspvc 这个pod使用busybox来执行一段语句，在需要mount的路径下新建一个名为SUCCESS的文件。创建好pod后，等待pod的Status变为Completed之后，查看所有/mnt文件夹以及/data文件夹\n$ kubectl get pod NAME READY STATUS RESTARTS AGE test-pod 0/1 Completed 0 10s #pod已经完成，查看被挂载以及挂载目录 #nfs-server: $ ls /data SUCCESS #nfs-client $ls /mnt SUCCESS 通过busybox创建文件已经通过pv存储在了nfs服务器中，并同步到了各个客户端的挂载点上。\n实际上，只需要nfs服务器即可达到实验效果，使用客户端可加深对nfs的理解。\n动态PV 动态PV是指使用PVC之前不需要创建PV，而是在PVC申请存储空间的时候自动根据条件创建，也叫做动态供给（Dynamical Provision）。动态供给的基础是StorageClass，详细可参考： StorageClass\n动手做 # 复现手册中提到的Kubernetes操作，并理解操作所涉及到的Kubernetes特性/功能\n部署nginx服务\n创建nginx的Deployment\n复现故障转移和伸缩功能\n创建nginx的Service\nServiceType应为NodePort\nDeployment的replicas至少为2\n为每个Nginx Pod创建不同的service.html\n通过Service NodePort对Pod的service.html进行多次curl，要求每次结果都不相同(因为被代理到了不同的Pod，且默认的选择算法为round-robin)，例如：\n~$ curl \u0026lt;nodeIP\u0026gt;:\u0026lt;nodePort\u0026gt;/service.html 2 ~$ curl \u0026lt;nodeIP\u0026gt;:\u0026lt;nodePort\u0026gt;/service.html 1 Tips:\n可以通过kubectl exec -it \u0026lt;podName\u0026gt; -- bash进入Pod内部 要创建的文件位于/usr/share/nginx/html/service.html 容器内并未安装文本编辑器，可以通过echo和cat命令对service.html进行编辑，如\necho \u0026#39;single string\u0026#39; \u0026gt; service.html cat \u0026gt; service.html \u0026lt;\u0026lt;EOF complicated string with multiple lines EOF 尝试使用滚动更新，对比前后主页版本 (可以自行创建Service通过代理访问网页，也可以直接curl ClusterIP访问。注意浏览器缓存影响)\n部署持久化服务\n在集群中部署nfs环境 创建pv和pvc 创建mysql的Deployment和Service，确保绑定了pvc (mysql的mount目录为/var/lib/mysql)，创建MySQL时，可以使用镜像harbor.scs.buaa.edu.cn/library/mysql:8 查看nfs server的mount目录，mysql数据文件已经写入 "},{"id":5,"href":"/doc/cloud-labs/cloud/cloud_lab/","title":"云PaaS平台开发","section":"云计算实验","content":" 云PaaS平台开发 # 实验要求 # 基于Kubernetes，设计并实现一PaaS平台。该平台的终极目标，用户可以通过该平台，实现从源代码到可访问的服务之间的整个自动化流程。\n比如，在传统方式下，用户编写了一个React应用，为了能让其他人通过互联网访问到他的应用，他需要首先在云厂商购买服务器，然后在服务器上安装操作系统和NodeJS，然后手动拉取依赖和编译，然后手动将编译好的静态文件放到合适的地方（配置Nginx等）。\n而在通过这个PaaS平台，用户只需要提供自己的源代码，并按照平台的需求，编写一个配置文件（可能会包含一个Dockerfile，也可能是某种模板的形式），剩下的工作可以放心地交给PaaS平台来完成。\n那么PaaS平台是如何实现这个功能的呢？首先，它根据用户提供的源代码和配置文件，将源代码编译成一个OCI镜像；然后将该镜像交给一个容器管理平台部署为一个容器组（一般是Kubernetes中的Deployment或StatefulSet）；然后根据用户提供的配置文件，将容器组的特定端口暴露到互联网上（例如使用Service的NodePort或LoadBalancer）。\n进一步地，一个真正的生产级别的应用不可能由一个容器组提供的功能支撑（最简单的一个前后端分离的应用，就要包含一个前端、一个后端，可能还有数据库和缓存服务等），为了完成整个应用的部署，用户一般会通过配置文件来指定多个不同的容器。这些容器之间的逻辑是相互联系，平台需要能够将它们在逻辑上与其他的应用容器区分开。\n因此，可以看到，一个完整的PaaS平台可以包含（最终小组作品可以包含下述一个或多个功能点）：\n镜像管理 用户可以向平台添加自己需要的镜像，可以直接提供一个Dockerfile，也可以直接提供镜像的压缩包，也可以提供源代码（可以是代码的压缩包，也可以是一个代码仓库地址等等），也可以直接让平台去拉公共的镜像仓库 用户可以浏览、更改和删除自己添加的镜像 在镜像管理部分，可以自己在本地通过Docker Image管理，也可以考虑搭建一个私有的 Docker Registry 容器管理 用户可创建和修改容器，并监控容器状态 （考虑使用Kubernetes进行部署和管理） 应用部署 用户可以直观地将若干个逻辑上统一的容器编排成完成的应用，并发布。这里这个“应用”的概念是Kubernetes本身没有的，需要你自己去抽象。比如，可以把不同应用的容器组使用namespace隔离等等。在KubeSphere上，这个概念其实就是一个“项目”） 希望同学们在实现上述功能的过程中加深对Kubernetes各项概念的理解，并体会云计算为应用的发布和运维带来的便利。\n开发过程中，需要使用到Kubernetes和Docker Engine对外提供的API，可以直接调用他们的OpenAPI，也可以使用官方或第三方封装好的Client SDK。\nKubernetes的OpenAPI描述文件可以在 这里找到，这里列出了比较流行的一些Kubernetes 客户端库。\nDocker的OpenAPI描述文件可以在 这里找到，这里列出了比较流行的一些Docker 客户端库。\n实验代码管理与部署 # 实验代码请托管到软院代码托管平台 BuGit上。\n首次使用代码托管平台时需要激活账户。激活账户时，请注意邮箱的正确性，并牢记密码。 系统开发将分小组进行，需要小组在 BuGit上创建项目，并邀请所有小组成员加入。\n可以使用的资源 # KubeSphere，该平台的初始账号密码与BuGit相同（如果登录失败，可以尝试使用密码Newpass@2021登录），并且其上的项目与BuGit同步。在BiGit上创建项目后，可在KubeSphere对应的项目中部署容器。\nHarbor，该平台的初始账号密码与BuGit相同，并且其上的项目与BuGit同步。在BiGit上创建进行代码仓库的构建后，可在Harbor对应的项目中查看到创建的镜像。\n校内的Docker Hub镜像地址：10.251.0.37:5000。\n"},{"id":6,"href":"/doc/ns-labs/table-of-contents/","title":"Table of Contents","section":"网络存储实验规划","content":" 你好，网存！ # 欢迎来到网络存储的实验课堂！\n目录 # 实验目录可见左侧边栏\n"},{"id":7,"href":"/doc/cloud-labs/cloud/appendix_create_kubernetes/","title":"附录：创建Kubernetes集群","section":"云计算实验","content":" 创建Kubernetes集群 # Kubernetes生态发展至今已非常完善，部署一个Kubernetes集群已经不再是一件非常繁琐和困难的事情。社区有大量简单可靠的解决方案。\n下面给出几种可选的方案，根据自己的实际情况，选择其一即可。\nkubeadm（不推荐） # Kubernetes官方推荐使用 kubeadm 来初始化一个Kubernetes集群。通过它，用户可以获得是一个相对“纯净”的Kubernetes集群。但该方法相对繁琐，而且对国内用户非常不友好，因此不推荐这种方式部署。\n有兴趣的同学可以尝试。\nKubeKey（推荐） # KubeKey 是由 Kubesphere （一个国内公司主导的开源的Kubernetes管理平台）开源的Kubernetes和Kubesphere部署工具。\nKubeKey使用声明式的配置方式，用户只需要通过一个YAML配置文件给出所需集群的相关配置，即可通过KubeKey创建集群或修改集群的状态。更详细内容和使用方式请参考 KubeKey的文档。\n创建集群 # 在本次实验分配的虚拟机中，已经提前完成了KubeKey的部分配置，只需按照下面几步操作即可完成Kubernetes集群的创建。\n注意\n为了减少不必要的麻烦，请直接使用root账户登录虚拟机，并完成相关操作。\n进入/root/kubesphere目录\ncd /root/kubesphere 修改配置文件中的IP和登录密码。可以按需修改主机的hostname。如果修改了hostname，也需要同步修改roleGroups中的值。\napiVersion: kubekey.kubesphere.io/v1alpha1 kind: Cluster metadata: name: main-cluster spec: hosts: - {name: node1, address: 1.1.1.1, internalAddress: 1.1.1.1, user: root, password: \u0026#39;\u0026amp;\u0026amp;shieshuyuan21\u0026#39;} - {name: node2, address: 1.1.1.1, internalAddress: 1.1.1.1, user: root, password: \u0026#39;\u0026amp;\u0026amp;shieshuyuan21\u0026#39;} roleGroups: etcd: - node1 master: - node1 worker: - node1 - node2 controlPlaneEndpoint: domain: lb.kubesphere.local address: \u0026#34;\u0026#34; port: 6443 kubernetes: version: v1.20.4 imageRepo: kubesphere clusterName: cluster.local network: plugin: calico kubePodsCIDR: 172.20.0.0/16 kubeServiceCIDR: 172.21.0.0/16 registry: registryMirrors: [] insecureRegistries: [] addons: [] 保存文件并退出后，执行以下命令即可：\n./kk create cluster -f config.yaml 等待完成即可。完成后，可以直接在机器中使用kubectl get node来验证。\n删除集群 # 如果需要删除创建好的集群，只需要执行下述删除命令即可：\ncd /root/kubesphere ./kk delete cluster -f config.yaml RKE（一般推荐） # 使用RKE时，需要向目标机器传递公钥。请不要使用虚拟机默认生成的公钥。\n因为所有实验机器密钥对都相同，当你向目标机器传递了公钥后，本次实验的其他同学将可以无障碍登录你的目标机器。\nRKE（Rancher Kubernetes Engine）是rancher提供的一个Kubernetes管理工具。\n与KubeKey一样，RKE同样提供了声明式的配置方式，你可以在RKE的引导下，创建配置文件，并以此创建集群和管理集群的状态。\n本次实验分配的虚拟机中，提供了rke的可执行文件，可以直接使用。对此感兴趣的同学可以参考 rke的安装说明文档。\nk3s（一般推荐） # k3s是一个非常轻量的Kubernetes发行版。其安装和配置方法在其 官方文档中写的很详细。\n其他（单机推荐） # 如果需要在本地创建Kubernetes集群，可以选择使用Minikube、Docker Desktop等。\n"},{"id":8,"href":"/doc/ns-labs/resources/","title":"Resources","section":"网络存储实验规划","content":" 课程相关资源 # 此处存放了课程相关资源，如系统 ISO 镜像和各种软件的下载地址。\n"},{"id":9,"href":"/doc/02_bugit/build_deploy/","title":"自动构建与部署","section":"BuGit代码托管与自动部署平台","content":" 自动构建与部署 # 原理 # 本质上，自动构建与部署的过程是，系统根据用户提供的包含在代码仓库中的配置文件，将代码编译成一个OCI规范的镜像，然后上传到镜像中心，最后通知Kubernetes集群拉取镜像运行，以对外提供可用服务。\n自动构建与部署的几个步骤：\n用户编写配置合法的配置文件，并包含在代码仓库的根目录中。\n用户触发自动构建与部署。目前支持自动监听代码推送（git push）动作，和在前端手动点击按钮触发。\n系统拉取用户指定的仓库的指定的某次提交的代码，并根据指定的Dockerfile进行镜像构建。\n系统将构建完成的镜像将推送到镜像中心（harbor.scs.buaa.edu.cn）。\n系统通知Kubernetes拉取镜像，并部署之。\n配置文件 # 配置文件一共包含两个：Dockerfile 和 .bugit.yaml。\nDockerfile # Dockerfile用来描述该代码仓库希望被如何编译和打包成一个OCI镜像。具体的编写规则可以参考 Dockerfile Reference。\n.bugit.yaml # .bugit.yaml是一个YAML文件（名称.bugit.yml也是合法的，并且请注意文件名最前面那个.）。它是对整个构建和部署过程的描述。\n下面是一个.bugit.yaml文件支持的全部指令的示例（请注意缩进）。\n下方示例中，提到的非必需字段，都可以在.bugit.yaml省略不写。 # 必需字段。表示当前的.bugit.yaml 所适用的构建与部署流程的版本号，目前仅支持 0.0.1 version: 0.0.1 # on 字段中的内容用来表示在哪个分支发生什么事件时，自动启动构建与部署流程 # 该字段中可以包含若干组内容，每一组的 key （比如，下方示例中的 main 和 master） 都是分支名称，其 value （比如下方示例中的 [push]）是一个数组，表示希望系统监听哪些事件的发生 # 比如下面的示例就表示，希望系统在远程仓库的 main 分支和 master 分支发生代码推送事件（git push）时，自动启动构建与部署流程 # 如果希望开启“自动”构建与部署的功能，那么该字段是必需的 on: main: [\u0026#34;push\u0026#34;] master: [\u0026#34;push\u0026#34;] # 必需字段。build 字段用来描述如何系统如何构建OCI镜像 build: name: build-1 # 必需。名称标识，目前没有太大意义。可以是任意字符串，但请不要带空格 type: docker # 必需。构建的类型，目前仅支持docker docker_tag: simple # 非必需。表示希望给构建好的字段加的额外tag dockerfile: ./Dockerfile # 必需。表示使用的 Dockerfile 与代码仓库根目录的相对路径 # 非必需字段。build 字段用来描述如何构建好的镜像将会被如何部署。如果不希望使用部署功能的话，该字段可以忽略。 deploy: # 必需。其值为一个列表。表示希望在哪几个分支的代码中开启部署功能。（“部署”是一个非常重的操作，需要用户明确确认） on: [\u0026#34;main\u0026#34;, \u0026#34;master\u0026#34;] # 必需(至少包含一个端口）。表示运行起来的容器将向外暴露哪些端口 ports: - name: name-1 # 名称，该端口的一个标识。必须为小写的英文字母和数字组合，可以包含短横线。但数字不能作为开头，短横线不能作为结尾。 protocol: tcp # 使用的协议，支持tcp和udp，默认是tcp port: 80 # 容器向外暴露的端口 - name: name-2 protocol: udp port: 9934 # 非必需。envs 表示服务部署时需要使用的环境变量。key 和 value 一一对应。 envs: SOME_ENV_1: \u0026#34;some_env_1\u0026#34; SOME_ENV_2: \u0026#34;some_env_2\u0026#34; # 非必需。stateful 表示该服务是有状态的还是无状态的。该字段默认为false，即默认无状态。 stateful: false # 非必需。work_dir 表示容器开始运行时，执行的命令所在的目录。如果该字段为空，默认使用镜像中指定的 workDir work_dir: /path/to/work_dir # 非必需。cmd表示容器启动时指定的命令。其又分为两部分（两个列表），其中，command用来指定命令， args 用来指定命令需要使用的参数 # 如下方的示例，对应我们常见的命令形式就是 java -jar awesome.jar，表示使用java命令运行一个jar cmd: command: [\u0026#34;java\u0026#34;] args: [\u0026#34;-jar\u0026#34;, \u0026#34;awesome.jar\u0026#34;] # 非必需。cpu限额，表示该服务最多使用多少CPU资源。默认为250m（四分之一个CPU核心） cpu: 250m # 可以直接使用数字，如 3、100，分别表示使用3个CPU核心、100个CPU核心；也可以使用m作为单位，一个CPU核心是1000m，那么250m就表示使用四分之一个CPU核心 # 非必需。内存限额，表示该服务最多使用多少内存资源。默认为0.5G memory: 512Mi # 请使用单位 Mi，Gi，或 M，G，如 0.5G，512Mi 等 BuGit平台的每个个人和项目的资源配额为2核4G。所以请合理调配每个代码仓库的资源配额。 对于典型的资源消耗大户（如Java应用对内存的消耗），需要注意在容器的启动命令中手动限制资源配额（如手动指定jvm的内存参数等），防止容器因为资源有限而使服务启动失败。 示例 # BuGit平台中的 test-project项目包含的每个项目都进行了自动构建和部署的配置，都可以作为参考。\n特别地，下面给出了一些典型的示例。\nStatic Web # 适用于纯静态文件的网站部署（如，仅包含html，css，js等文件）。\n可参考项目 static-web。\nDockerfile # FROM nginx # 下面的 . 表示使用的是当前这个Dockerfile所在的目录作为网站的根目录 # 如果你的 index.html 所在的位置与此不同，请根据实际情况修改 COPY . /usr/share/nginx/html .bugit.yaml # version: 0.0.1 on: master: [push] main: [push] build: name: build-static-nginx type: docker docker_tag: web dockerfile: ./Dockerfile deploy: on: [main, master] ports: - name: web protocol: tcp port: 80 "},{"id":10,"href":"/doc/01_common/virtual_machine_help/","title":"虚拟机使用说明","section":"云平台使用手册","content":" 虚拟机使用说明 # 连接虚拟机 # Linux系统 # 首先从云平台中获取虚拟机的IP和登录名，之后即可在本地通过任意ssh客户端登录。\nMacOS 使用系统自带的Terminal.app登录即可。\n为了更好的使用体验，推荐使用 iterm2登录。\n当然，你也可以使用 termius进行多个ssh连接的管理。\nLinux 如果你是Linux Desktop用户，那么你肯定已经有了自己喜爱的终端模拟器，此处不再赘述。 Windows 一般来讲，Windows 10（及以上）自带的cmd.exe都自带ssh client，打开cmd后直接ssh foo@x.x.x.x即可登录。\n为了更好的使用体验，推荐下载使用 Windows Terminal。\n当然，你也可以使用 termius或者其他工具（如 Xshell等）进行多个ssh连接的管理。\n校外跳板 通过 d.buaa.edu.cn 跳转登录即可。 联网 # Linux系统 # 可以依次尝试以下两种方式。\n校园网登录脚本 # 这里推荐使用 srun工具。\n下载并安装登录工具：\nsudo wget https://scs.buaa.edu.cn/scsos/tools/linux/buaalogin -O /usr/local/bin/buaalogin sudo chmod +x /usr/local/bin/buaalogin 配置校园网登录使用的用户名和密码：\nbuaalogin config 登录校园网：\nbuaalogin login 登出校园网：\nbuaalogin logout wukuard 服务 # 鉴于校网络中心的某些限制，上述联网方式可能在某短时间内无法使用，这里特别给出软院信息化小组的基于 Wireguard的曲线救国方案。其本质上是将虚拟机加入一个 wireguard虚拟内网，然后覆盖默认路由指向一个可以联网的内网机器，从而实现虚拟机本身与互联网的联通。\n请注意，以下步骤是针对Debian系发行版（包括Debian、Ubuntu等）给出的，其他发行版请自行对照着修改命令。 首先需要安装wireguard-tools（这里需要短暂联网，但完整整个步骤之后就不需要了）：\nexport http_proxy=http://10.251.0.37:3128;export https_proxy=http://10.251.0.37:3128 sudo apt update \u0026amp;\u0026amp; sudo apt install wireguard-tools -y 如果上述命令执行中，apt没有命中http_proxy，可以手动配置apt的proxy: 创建文件/etc/apt/apt.conf.d/proxy.conf，并在其中写入以下内容\nAcquire::http::Proxy \u0026#34;http://10.251.0.37:3128\u0026#34;; Acquire::https::Proxy \u0026#34;http://10.251.0.37:3128\u0026#34;; 然后重新执行 apt update \u0026amp;\u0026amp; apt install wireguard-tools -y\n注意：完成所有配置后，请将该文件删除。\n然后安装 wukuard（作为wireguard管理工具）\nsudo wget https://scs.buaa.edu.cn/scsos/tools/linux/wukuard -O /usr/local/bin/wukuard sudo chmod +x /usr/local/bin/wukuard 然后配置wukuard-client服务\nsudo wget https://scs.buaa.edu.cn/scsos/tools/linux/wukuard-client.service -O /etc/systemd/system/wukuard-client.service sudo systemctl enable --now wukuard-client 然后找管理员所要虚拟机应该使用的hostname，然后配置机器的hostname：\nsudo hostnamectl set-hostname ${your_hostname} # hostname的值找管理员要 传输文件 # Linux系统 # MacOS \u0026amp; Linux 可以使用使用SCP命令进行服务器与本地之间的文件交换。 Windows 除了在终端中使用SCP命令外，\n还可以使用 WinSCP进行图形化的文件管理。\n校外跳板 d.buaa.edu.cn 的Linux界面已经提供了比较完善的文件管理工具。 "},{"id":11,"href":"/doc/cloud-labs/cloud/faq/","title":"FAQ","section":"云计算实验","content":" FAQ # "},{"id":12,"href":"/doc/02_bugit/","title":"BuGit代码托管与自动部署平台","section":"软院云平台文档","content":" BuGit代码托管与自动部署平台 # "},{"id":13,"href":"/doc/01_common/","title":"云平台使用手册","section":"软院云平台文档","content":" 云平台使用手册 # "},{"id":14,"href":"/doc/cloud-labs/cloud/","title":"云计算实验","section":"大数据和云计算综合实践","content":" 云计算实验 # "},{"id":15,"href":"/doc/cloud-labs/","title":"大数据和云计算综合实践","section":"软院云平台文档","content":" 大数据和云计算综合实践 # "},{"id":16,"href":"/doc/ns-labs/","title":"网络存储实验规划","section":"软院云平台文档","content":" 网络存储实验规划 # 提交方式 # 我们会提供实验的 Markdown 模板，请在完成实验后导出 PDF 上传至北航软件学院云平台。命名方式见各实验的详细说明。 实验进度 # 预计总共有 3 个实验和一个课设实验 RAID 阵列实验 虚拟化实验 Ceph 实践 助教联系方式 # 李楠（微信 zhanoan619） 诚信说明 # 所有的参考资料请注明来源。实验报告将严格查重，若发现有作业抄袭现象，作业按零分处理。\n"}]