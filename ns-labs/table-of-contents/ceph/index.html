<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Lab03 Ceph 存储集群实践 # 实验目的 # 了解 Ceph 存储的基本工作原理
建立对分布式存储的初步认识
实验说明 # 按照实验指南的指导，完成实验。
按照实验报告模板，撰写实验报告，将重要的实验步骤截图，填入实验报告中，并回答相应问题。
本次实验由小组内成员分工完成，虚拟机已分发至每组第一位同学的账户中，每组一台虚拟机，实验报告每个人都需要提交，同组的实验报告内容可以一致
本次实验的分数构成如下：
必做（7分）： Ceph部署 三选一（3分）： Ceph Filesystem Ceph RGW 对象存储 Ceph RBD 其中，Ceph Filesystem、Ceph RGW、Ceph RBD任选其一完成即可得到“三选一”部分的分数，多做没有额外的加分。如果你想在网络存储课程上投入更多的精力，欢迎参与课程设计申优答辩。
在实验过程中，在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果。
实验中遇到的困难请及时在课程微信群中抛出。 除本指导书外，实验的主要的参考资料还包括： Ceph 官方文档、 Ceph 部署指南、互联网上的技术博客等。
请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf的格式。
概述 # Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？
试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。
最初，你将网站部署在一个装有 500G 硬盘的服务器上。随着时间的流逝，500G 的硬盘逐渐被填满。现在你有两种选择。
纵向拓展。在服务器上加装硬盘，甚至你可以使用 LVM 将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。
横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。
第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。
但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：
如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？
如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。
随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是 1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。
……
上面这些问题，正是 Ceph 这类分布式存储系统所要解决的问题。简单来说，Ceph 是一个能将大量廉价的存储设备统一组织起来，并对外提供统一的服务接口的，提供分布式、横向拓展、高度可靠性的存储系统。
对分布式系统感兴趣的同学，可以趁下学期或大四空闲的时候听一下 MIT 6.824的课程，并尽量完成它的全部实验。
在互联网上搜索“MIT 6.824”能得到大量的资料，比如，B 站上有 翻译好的熟肉。"><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="Lab03 Ceph存储集群实践"><meta property="og:description" content="Lab03 Ceph 存储集群实践 # 实验目的 # 了解 Ceph 存储的基本工作原理
建立对分布式存储的初步认识
实验说明 # 按照实验指南的指导，完成实验。
按照实验报告模板，撰写实验报告，将重要的实验步骤截图，填入实验报告中，并回答相应问题。
本次实验由小组内成员分工完成，虚拟机已分发至每组第一位同学的账户中，每组一台虚拟机，实验报告每个人都需要提交，同组的实验报告内容可以一致
本次实验的分数构成如下：
必做（7分）： Ceph部署 三选一（3分）： Ceph Filesystem Ceph RGW 对象存储 Ceph RBD 其中，Ceph Filesystem、Ceph RGW、Ceph RBD任选其一完成即可得到“三选一”部分的分数，多做没有额外的加分。如果你想在网络存储课程上投入更多的精力，欢迎参与课程设计申优答辩。
在实验过程中，在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果。
实验中遇到的困难请及时在课程微信群中抛出。 除本指导书外，实验的主要的参考资料还包括： Ceph 官方文档、 Ceph 部署指南、互联网上的技术博客等。
请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf的格式。
概述 # Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？
试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。
最初，你将网站部署在一个装有 500G 硬盘的服务器上。随着时间的流逝，500G 的硬盘逐渐被填满。现在你有两种选择。
纵向拓展。在服务器上加装硬盘，甚至你可以使用 LVM 将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。
横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。
第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。
但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：
如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？
如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。
随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是 1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。
……
上面这些问题，正是 Ceph 这类分布式存储系统所要解决的问题。简单来说，Ceph 是一个能将大量廉价的存储设备统一组织起来，并对外提供统一的服务接口的，提供分布式、横向拓展、高度可靠性的存储系统。
对分布式系统感兴趣的同学，可以趁下学期或大四空闲的时候听一下 MIT 6.824的课程，并尽量完成它的全部实验。
在互联网上搜索“MIT 6.824”能得到大量的资料，比如，B 站上有 翻译好的熟肉。"><meta property="og:type" content="article"><meta property="og:url" content="https://scs.buaa.edu.cn/doc/ns-labs/table-of-contents/ceph/"><meta property="article:section" content="ns-labs"><meta property="article:modified_time" content="2022-11-23T22:41:14+08:00"><title>Lab03 Ceph存储集群实践 | 软院云平台文档</title><link rel=manifest href=/doc/manifest.json><link rel=icon href=/doc/favicon.png type=image/x-icon><link rel=stylesheet href=/doc/book.min.46181bc93375ba932026e753b37c40e6ff8bb16a9ef770c78bcc663e4577b1ba.css integrity="sha256-RhgbyTN1upMgJudTs3xA5v+LsWqe93DHi8xmPkV3sbo=" crossorigin=anonymous><script defer src=/doc/flexsearch.min.js></script>
<script defer src=/doc/en.search.min.d85a5bf36f4a5517a694095db8e8f45d68d069cf12b7ce04ccd34e8d777e402c.js integrity="sha256-2Fpb829KVRemlAlduOj0XWjQac8St84EzNNOjXd+QCw=" crossorigin=anonymous></script>
<script defer src=/doc/sw.min.c6e13cf2801d746c82b18aace5ed50254554a6ab28bdd01a6fd9fdc0d082e427.js integrity="sha256-xuE88oAddGyCsYqs5e1QJUVUpqsovdAab9n9wNCC5Cc=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/doc/><span>软院云平台文档</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=https://scs.buaa.edu.cn/doc/02_bugit/>BuGit代码托管与自动部署平台</a><ul><li><a href=https://scs.buaa.edu.cn/doc/02_bugit/build_deploy/>自动构建与部署</a></li></ul></li><li><a href=https://scs.buaa.edu.cn/doc/01_common/>云平台使用手册</a><ul><li><a href=https://scs.buaa.edu.cn/doc/01_common/virtual_machine_help/>虚拟机使用说明</a></li></ul></li><li><a href=https://scs.buaa.edu.cn/doc/cloud-labs/>大数据和云计算综合实践</a><ul><li><a href=https://scs.buaa.edu.cn/doc/cloud-labs/cloud/>云计算实验</a><ul><li><a href=https://scs.buaa.edu.cn/doc/cloud-labs/cloud/container_docker/>容器与Docker综合实验</a></li><li><a href=https://scs.buaa.edu.cn/doc/cloud-labs/cloud/cloud_course_design/>云计算课程设计</a></li><li><a href=https://scs.buaa.edu.cn/doc/cloud-labs/cloud/kube-single-3/>Kubernetes综合实验</a></li><li><a href=https://scs.buaa.edu.cn/doc/cloud-labs/cloud/faq/>FAQ</a></li></ul></li></ul></li><li><a href=https://scs.buaa.edu.cn/doc/ns-labs/>网络存储实验规划</a><ul><li><a href=https://scs.buaa.edu.cn/doc/ns-labs/table-of-contents/>Table of Contents</a><ul><li><a href=https://scs.buaa.edu.cn/doc/ns-labs/table-of-contents/raid/>Lab01 RAID 阵列</a></li><li><a href=https://scs.buaa.edu.cn/doc/ns-labs/table-of-contents/virtualization/>Lab02 虚拟化实验</a></li><li><a href=https://scs.buaa.edu.cn/doc/ns-labs/table-of-contents/ceph/ class=active>Lab03 Ceph存储集群实践</a></li></ul></li><li><a href=https://scs.buaa.edu.cn/doc/ns-labs/resources/>Resources</a><ul><li><a href=https://scs.buaa.edu.cn/doc/ns-labs/resources/os/>操作系统</a></li></ul></li></ul></li></ul><ul><li><a href=https://github.com/bugitt/doc target=_blank rel=noopener>Github</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/doc/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Lab03 Ceph存储集群实践</strong>
<label for=toc-control><img src=/doc/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#lab03-ceph-存储集群实践>Lab03 Ceph 存储集群实践</a><ul><li><a href=#实验目的>实验目的</a></li><li><a href=#实验说明>实验说明</a></li><li><a href=#概述>概述</a></li><li><a href=#重要概念>重要概念</a><ul><li></li></ul></li><li><a href=#实验环境介绍>实验环境介绍</a></li><li><a href=#ceph-部署>Ceph 部署</a><ul><li><a href=#cephadm介绍>Cephadm介绍</a></li><li><a href=#bootstrap>BootStrap</a></li><li><a href=#ceph-dashboard>Ceph Dashboard</a></li><li><a href=#添加其他节点>添加其他节点</a></li><li><a href=#创建-osd-进程>创建 OSD 进程</a></li></ul></li><li><a href=#ceph-filesystem-选>Ceph Filesystem (选)</a><ul><li><a href=#部署-cephfs>部署 CephFS</a></li><li><a href=#挂载-cephfs>挂载 CephFS</a></li></ul></li><li><a href=#ceph-rgw-对象存储-选>Ceph RGW 对象存储 (选)</a><ul><li><a href=#deploy-rgw>Deploy RGW</a></li><li><a href=#使用对象存储>使用对象存储</a></li></ul></li><li><a href=#ceph-rbd-选>Ceph RBD (选)</a></li><li><a href=#实验报告模板>实验报告模板</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=lab03-ceph-存储集群实践>Lab03 Ceph 存储集群实践
<a class=anchor href=#lab03-ceph-%e5%ad%98%e5%82%a8%e9%9b%86%e7%be%a4%e5%ae%9e%e8%b7%b5>#</a></h1><h2 id=实验目的>实验目的
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e7%9b%ae%e7%9a%84>#</a></h2><ol><li><p>了解 Ceph 存储的基本工作原理</p></li><li><p>建立对分布式存储的初步认识</p></li></ol><h2 id=实验说明>实验说明
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e8%af%b4%e6%98%8e>#</a></h2><ol><li><p>按照实验指南的指导，完成实验。</p></li><li><p>按照实验报告模板，撰写实验报告，将重要的实验步骤截图，填入实验报告中，并回答相应问题。</p></li></ol><p>本次实验由小组内成员分工完成，虚拟机已分发至每组第一位同学的账户中，每组一台虚拟机，实验报告每个人都需要提交，同组的实验报告内容可以一致</p><blockquote class="book-hint info"><p>本次实验的分数构成如下：</p><ul><li>必做（7分）：<ul><li>Ceph部署</li></ul></li><li>三选一（3分）：<ul><li>Ceph Filesystem</li><li>Ceph RGW 对象存储</li><li>Ceph RBD</li></ul></li></ul><p>其中，Ceph Filesystem、Ceph RGW、Ceph RBD任选其一完成即可得到“三选一”部分的分数，多做<strong>没有</strong>额外的加分。如果你想在网络存储课程上投入更多的精力，欢迎参与课程设计申优答辩。</p></blockquote><p>在实验过程中，<strong>在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果</strong>。</p><p><strong>实验中遇到的困难请及时在课程微信群中抛出。</strong> 除本指导书外，实验的主要的参考资料还包括：
<a href=https://docs.ceph.com/en/pacific/>Ceph 官方文档</a>、
<a href=https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/5/html/installation_guide/index>Ceph 部署指南</a>、互联网上的技术博客等。</p><p>请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：<code>lab03-学号-姓名.pdf</code>的格式。</p><h2 id=概述>概述
<a class=anchor href=#%e6%a6%82%e8%bf%b0>#</a></h2><p>Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？</p><p>试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。</p><p>最初，你将网站部署在一个装有 500G 硬盘的服务器上。随着时间的流逝，500G 的硬盘逐渐被填满。现在你有两种选择。</p><ol><li><p>纵向拓展。在服务器上加装硬盘，甚至你可以使用 LVM 将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。</p></li><li><p>横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。</p></li></ol><p>第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。</p><p>但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：</p><ul><li><p>如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？</p></li><li><p>如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。</p></li><li><p>随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是 1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。</p></li><li><p>……</p></li></ul><p>上面这些问题，正是 Ceph 这类分布式存储系统所要解决的问题。简单来说，Ceph 是一个能将大量廉价的存储设备统一组织起来，并对外提供统一的服务接口的，提供<strong>分布式</strong>、<strong>横向拓展</strong>、<strong>高度可靠性</strong>的存储系统。</p><blockquote class="book-hint info"><p>对分布式系统感兴趣的同学，可以趁下学期或大四空闲的时候听一下
<a href=https://pdos.csail.mit.edu/6.824/>MIT 6.824</a>的课程，并尽量完成它的全部实验。</p><p>在互联网上搜索“MIT 6.824”能得到大量的资料，比如，B 站上有
<a href=https://www.bilibili.com/video/BV1R7411t71W>翻译好的熟肉</a>。</p></blockquote><p>除此之外，Ceph 的独特之处还在于，它在一个存储系统上，对外提供了三种类型的访问接口：</p><ul><li><p>文件存储。简单来说，你可以将 Ceph 的存储池抽象为一个文件系统，并挂载到某个目录上，然后像读写本地文件一样，在这个新的目录上创建、读写、删除文件。并且该文件系统可以同时被多台机器同时挂载，并被同时读写。从而实现多台机器间的存储共享。</p></li><li><p>对象存储。Ceph 提供了对象存储网关，并同时提供了 S3 和 Swift 风格的 API 接口。你可以使用这些接口上传和下载文件。</p></li><li><p>块存储。Ceph 还能提供块存储的抽象。即客户端（集群外的机器）通过块存储接口访问的“所有数据按照固定的大小分块，每一块赋予一个用于寻址的编号。”客户端可以像使用硬盘这种块设备一样，使用这些块存储的接口进行数据的读写。（一般这种块设备的读写都是由操作系统代劳的。操作系统会对块设备进行分区等操作，并在其上部署文件系统，应用程序和用户看到直接看到的是文件系统的接口（也就是文件存储））。</p></li></ul><p><img src=https://cdn.loheagn.com/123326.jpg alt></p><p>需要注意的是，虽然 Ceph 对外提供了上面这三种不同类型的存储接口，但其底层会使用相同的逻辑对接收的数据进行分块和存储。</p><h2 id=重要概念>重要概念
<a class=anchor href=#%e9%87%8d%e8%a6%81%e6%a6%82%e5%bf%b5>#</a></h2><p>一个 Ceph 集群必须包含三种类型的进程：Monitor、OSD 和 Manager。其中，Monitor 和 OSD 是最核心的两类进程。</p><h4 id=monitor-和-osd>Monitor 和 OSD
<a class=anchor href=#monitor-%e5%92%8c-osd>#</a></h4><p>Monitor 进程负责维护整个系统的状态信息，这些状态信息包括当前的 Ceph 集群的拓扑结构等，这些信息对 Ceph 集群中各个进程的通信来说非常关键。除此之外，Monitor 进程还负责充当外界（也就是官方文档中总是提到的“Client”）与 OSD 进程交流的媒介。</p><p>OSD 进程则负责进行真正的数据存储。如下图所示，外界传送给 Ceph 集群的数据（不管是通过文件存储、对象存储还是块存储的接口）都将被转化为一个个对象（object）。这些 object 将经由 OSD 进程存储到磁盘中。</p><p><img src=https://cdn.loheagn.com/134513.jpg alt></p><p>简单来说，当一个 Client 试图向 Ceph 集群读写数据时，将发生以下步骤：</p><ol><li><p>Client 向 Monitor 进程请求一个 token 校验信息</p></li><li><p>Monitor 生成 token 校验信息，并将其返回给 Client</p></li><li><p>Monitor 同时会将 token 校验信息同步 OSD 进程</p></li><li><p>Client 携带着 Monitor 返回的 token 校验信息向对应的 token 发送数据读写请求</p></li><li><p>OSD 进程将数据存储到合适的位置，或从合适的位置读出数据</p></li><li><p>OSD 进程向 Client 返回数据</p></li></ol><blockquote class="book-hint info"><p>以上读写数据的流程是经过极致简化的，主要是为了帮助大家建立对 Monitor 进程和 OSD 进程所起的作用的感性认识。</p><p>想要了解详情，请阅读
<a href=https://docs.ceph.com/en/pacific/architecture/>Ceph 的文档 - Architecture</a>。</p></blockquote><h4 id=manager>Manager
<a class=anchor href=#manager>#</a></h4><p>Manager 进程主要负责跟踪当前集群的运行时状况，包括当前集群的存储利用率、存储性能等等。同时，它还负责提供 Ceph Dashboard、RESTful 接口的外部服务。</p><blockquote class="book-hint info"><p>需要注意的是，Ceph 集群中有<strong>三类</strong>这样的进程，但不是每个进程只有<strong>一个</strong>。</p><p>我们之前提到过，Ceph 是一个有很高容错性的分布式系统，而达到高容错性的一个很重要的方式就是“<strong>冗余</strong>”。</p><p>比如，对于 Monitor 进程来讲，集群中仅有一个就够用了。但如果运行这一个 Monitor 进程的机器挂了，那么整个集群就会瘫痪（Client 将不知道该跟谁通信来拿到校验信息和集群状态信息等）。因此，一个高可用的 Ceph 集群中会包含多个执行几乎相同任务的运行在不同机器上的 Monitor 进程；这样挂了一个，Client 还可以跟剩下的通信，整个集群依旧可以正常对外提供服务。同样的道理，OSD 进程和 Manager 进程也有多个副本。</p><p>另外，Ceph 为了保证数据的可靠性（也就是说 Client 存储进来的数据不能丢失）——注意区分其与整个系统可靠性的区别——在默认情况下，会将每份数据存储<strong>3 份</strong>，每份都会存储在不同的 OSD 上（鸡蛋不能放到同一个篮子里）。这样，即使有部分 OSD 挂掉，也能保证大部分数据不会丢失。因此，一个健康的 Ceph 集群要求至少同时存在三个健康的 OSD 进程（当然，这个默认的数值可以更改）。</p></blockquote><h4 id=pool-与-placement-group-pg-与-placement-group-for-placement-purpose-pgp>Pool 与 Placement Group (PG) 与 Placement Group for Placement purpose (PGP)
<a class=anchor href=#pool-%e4%b8%8e-placement-group-pg-%e4%b8%8e-placement-group-for-placement-purpose-pgp>#</a></h4><p>请查阅 Ceph 的相关文档，阐述 Pool、PG、PGP 与 OSD 之间的关系。可定性/定量分析 OSD、PG、PGP、OSD、PG_NUM 之间的数量关系。</p><h4 id=ceph-集群的-health-status>Ceph 集群的 HEALTH STATUS
<a class=anchor href=#ceph-%e9%9b%86%e7%be%a4%e7%9a%84-health-status>#</a></h4><p>你在实验过程中，打 <code>ceph -s</code> 都遇到过哪些 HEALTH STATUS？（<code>ceph health detail</code> 能看到更为详细的状态信息）如果是 <code>WARN/ERROR</code> 都是哪些原因？</p><h4 id=pg_num-的相关计算>PG_NUM 的相关计算
<a class=anchor href=#pg_num-%e7%9a%84%e7%9b%b8%e5%85%b3%e8%ae%a1%e7%ae%97>#</a></h4><p>请查阅 Ceph 的相关文档。</p><h2 id=实验环境介绍>实验环境介绍
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e7%8e%af%e5%a2%83%e4%bb%8b%e7%bb%8d>#</a></h2><p>本次实验发给大家了三台Centos 7虚拟机。它们的命名格式为<code>ceph-&lt;序号>-&lt;学号></code>，如<code>ceph-01-20210000</code>。</p><p>关于虚拟机的连接、文件传输以及连接互联网功能，可以参阅
<a href=https://scs.buaa.edu.cn/doc/01_common/virtual_machine_help/>虚拟机使用说明</a></p><blockquote class="book-hint info"><p>在实验开始前，你需要保证这三台虚拟机处于开机状态、用<code>buaalogin</code>连接互联网，并且设置它们的主机名与名称一致。</p><p>例如在<code>ceph-03-20210000</code>机器上，你需要执行（设置后必须重启生效）</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>echo <span style=color:#e6db74>&#39;ceph-03-20210000&#39;</span> &gt; /etc/hostname
</span></span><span style=display:flex><span>reboot
</span></span></code></pre></div><p>注意每台机器的主机名不能相同。</p></blockquote><h2 id=ceph-部署>Ceph 部署
<a class=anchor href=#ceph-%e9%83%a8%e7%bd%b2>#</a></h2><p>本节内容的目标是创建一个可用的 Ceph 集群。其中包括，至少一个 Monitor 进程、至少一个 Manager 进程、至少三个 OSD 进程。</p><p>Ceph 官方提供了
<a href=https://docs.ceph.com/en/pacific/install/>多种部署方式</a>。</p><p>在本实验文档中，我们采用
<a href=https://docs.ceph.com/en/pacific/cephadm/#cephadm>Cephadm</a>作为部署工具。Cephadm 也是官方推荐的部署和管理 Ceph 集群的工具，它不仅可以用来部署 Ceph，还可以在安装完成后，用来管理集群（添加和移除节点、开启 rgw 等）。</p><h3 id=cephadm介绍>Cephadm介绍
<a class=anchor href=#cephadm%e4%bb%8b%e7%bb%8d>#</a></h3><p>Cephadm 是基于“容器技术（Container）”进行工作的，每个 Ceph 的工作进程都运行在相互隔离的容器中。Cephadm 支持使用 Docker 和 Podman 作为容器运行时。在部署时，Cephadm 将首先检查本机中安装的容器运行时类型，当 Docker 与 Podman 并存时，将首先使用 Podman（毕竟 Podman 也是 Red Hat 的产品）。</p><p>在部署时，Cephadm 会首先在本地启动一个 mini 的 Ceph 集群，其中包括 Ceph 集群最基本的 Monitor 进程和 Manager 进程（当然，这两个进程都是通过容器形式运行起来的）。这个 mini 集群在某种程度上来说也是合法的，只不过其基本不能对外提供任何功能。随后，我们将继续使用 Cephadm 提供的工具，将其他机器（后文中也会称之为“节点”）加入到集群中（也就是在其他节点中启动 Ceph 的 Monitor、Manager、OSD 等进程），从而构建一个完整可用的集群。</p><p>下面是 Red Hat 给出使用 Cephadm 构建的集群的架构图。</p><p><img src=https://cdn.loheagn.com/090719.jpg alt></p><p>图中的“Container”指的就是我们上面所说的“容器”。</p><p>图中最左边的 Bootstrap Host 就是我们执行 Cephadm 相关命令的机器（事实上，在整个集群的构建过程中，除了修改 IP 和联网等操作外，我们都只会在这个 Bootstrap Host 上进行操作）。我们在 Bootstrap Host 执行的针对其他节点的操作，都是 Cephadm 通过 ssh 的方式发送给对应节点的。</p><p>这个图目前大家心里有个印象就好，在后续的实验操作中，可以反复回来对照检查。</p><blockquote class="book-hint info"><p>给大家提供的机器上已经安装好Cephadm，大家无需自行安装</p><p>在后续操作前，请务必保证所有机器已经连接<strong>互联网</strong>。</p><p>基于 Cephadm 的便捷性，后续的操作只需要在<strong>选定的一台</strong>机器（我们把这台机器称为 Bootstrap Host）上执行即可。</p></blockquote><h3 id=bootstrap>BootStrap
<a class=anchor href=#bootstrap>#</a></h3><p>我们首先需要在一台选定的机器上，使用<code>cephadm</code>启动一个 mini 集群。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cephadm --image scs.buaa.edu.cn:8081/library/ceph:v16 bootstrap  --mon-ip *&lt;mon-ip&gt;*
</span></span></code></pre></div><p>请将<code>*&lt;mon-ip>*</code>替换为你执行这命令的机器的 IP。如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cephadm --image scs.buaa.edu.cn:8081/library/ceph:v16 bootstrap  --mon-ip 10.251.252.182
</span></span></code></pre></div><p>上面这条命令中，<code>--image</code>制定了 Cephadm 启动容器时使用的镜像名称，<code>--mon-ip</code>指定了 Cephadm 要在哪个机器上启动一个 mini 集群。</p><p>更详细地，这条命令将会做如下事情：</p><blockquote><ul><li><p>Create a monitor and manager daemon for the new cluster on the local host.</p></li><li><p>Generate a new SSH key for the Ceph cluster and add it to the root user&rsquo;s /root/.ssh/authorized_keys file.</p></li><li><p>Write a copy of the public key to /etc/ceph/ceph.pub.</p></li><li><p>Write a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster.</p></li><li><p>Write a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring.</p></li><li><p>Add the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring.</p></li></ul></blockquote><p>命令执行完成后，我们可以通过<code>ceph -s</code>查看当前集群的状态。</p><p><img src=https://cdn.loheagn.com/074508.png alt></p><p>可以看到，确实启动了一个 Monitor 进程和一个 Manager 进程。</p><p>另外，我们还注意到，当前集群的健康状态是<code>HEALTH_WARN</code>，原因下面也列出来了：<code>OSD count 0 &lt; osd_pool_default_size 3</code>。这是因为当前 Ceph 集群默认的每个 Pool 的副本数应该是 3（即，Ceph 中存储的每份数据必须复制 3 份，放在 3 个不同的 OSD 中），但我们 OSD 的进程数是 0。不用着急，马上我们就会创建足够的 OSD 进程（Bootstrap Host、另外两台主机都为他们创建 OSD 进程）。</p><p><img src=https://cdn.loheagn.com/074750.png alt></p><p>mini 集群启动完成后，可以使用<code>docker ps -a</code>查看当前启动的容器的状态：</p><p><img src=img/fig01.jpg alt></p><p>可以看到，最下面这两个容器，对应的就是 ceph 集群的 Monitor 进程和 Manager 进程。</p><h3 id=ceph-dashboard>Ceph Dashboard
<a class=anchor href=#ceph-dashboard>#</a></h3><p>注意看<code>bootstrap</code>指令的输出，你可以看到一段这样的内容：</p><p><img src=https://cdn.loheagn.com/074911.png alt></p><p>显然，这是在告诉我们，cephadm 同样启动了一个<code>Ceph Dashboard</code>，这是一个 Ceph 的管理前端。通过访问这个页面，我们就可以以可视化的方式观察到当前集群的状态。</p><p>在内网环境中，签发 SSL 证书的过程太过繁琐，我们可以手动禁用 SSL：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph config set mgr mgr/dashboard/ssl false
</span></span></code></pre></div><p>禁用 SSL 后，Dashboard 服务将默认监听 8080 端口。但在 CentOS 中，8080 端口默认是被防火墙屏蔽的。</p><p>你可以选择手动打开防火墙的 8080 端口；也可以像下面这样，将 Dashboard 服务的监听端口手动改为 8443（因为这个端口就是使用 HTTPS 时 Dashboard 的监听端口，在刚才的 Bootstrap 时已经在防火墙中打开了）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph config set mgr mgr/dashboard/server_port <span style=color:#ae81ff>8443</span>
</span></span></code></pre></div><p>然后，重启该 Dashboard 服务，使刚才的配置生效。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph mgr module disable dashboard
</span></span><span style=display:flex><span>ceph mgr module enable dashboard
</span></span></code></pre></div><p>然后，查看当前的服务状态（如果输出为空的话，耐心多等一会儿）：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph mgr services
</span></span></code></pre></div><p><img src=https://cdn.loheagn.com/080831.png alt></p><p>访问<code>"dashboard"</code>后面的网址，如果它仍为<code>https</code>开头，则需要手动改成<code>http</code>开头。</p><p>现在，你应该可以正常访问 Dashboard 服务了。注意，用户名和密码是我们前面提到的 Bootstrap 命令输出的那堆信息中提到的。</p><p><img src=https://cdn.loheagn.com/081143.png alt></p><p>如果你忘记了密码，则可以用以下方式将密码重置为<code>@buaa21</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>echo <span style=color:#e6db74>&#39;@buaa21&#39;</span> &gt; passwd.txt
</span></span><span style=display:flex><span>ceph dashboard set-login-credentials admin -i passwd.txt
</span></span></code></pre></div><h3 id=添加其他节点>添加其他节点
<a class=anchor href=#%e6%b7%bb%e5%8a%a0%e5%85%b6%e4%bb%96%e8%8a%82%e7%82%b9>#</a></h3><p>接下来，我们将把其他机器添加到现有的这个 mini 集群中来。</p><p>前面提到过，cephadm 是通过 ssh 协议与其他机器通信的。所以，这里需要首先把 Bootstrap Host 机器的公钥 copy 到其他的所有机器：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ssh-copy-id -f -i /etc/ceph/ceph.pub root@*&lt;new-host&gt;*
</span></span></code></pre></div><p>例如，你的另一台机器的 IP 是<code>10.251.252.177</code>，那么这条命令应该是：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ssh-copy-id -f -i /etc/ceph/ceph.pub root@10.251.252.177
</span></span></code></pre></div><p>处理完所有的机器后，就可以正式将它们加入到 mini 集群中来了：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph orch host add *&lt;newhost&gt;* <span style=color:#f92672>[</span>*&lt;ip&gt;*<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>*&lt;label1&gt; ...*<span style=color:#f92672>]</span>
</span></span></code></pre></div><p>例如，你另一台机器的主机名是<code>ceph-02-20210000</code>，IP 是<code>10.251.252.177</code>，那么这条命令应该是：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph orch host add ceph-02-20210000 10.251.252.177
</span></span></code></pre></div><p>添加完成后，你可以通过<code>ceph -s</code>查看当前集群状态的变化。也可以通过 Ceph Dashboard 看到变化。</p><h3 id=创建-osd-进程>创建 OSD 进程
<a class=anchor href=#%e5%88%9b%e5%bb%ba-osd-%e8%bf%9b%e7%a8%8b>#</a></h3><p>我们知道，OSD 进程是真正用来做数据读写的进程。我们可以用一块专门的磁盘交给 OSD 进程来读写数据，ceph 集群所存储的数据就将保存在这些磁盘中。</p><p>这些被用来交给 OSD 进程管理的磁盘，应该满足以下条件：</p><blockquote><ul><li><p>The device must have no partitions.</p></li><li><p>The device must not have any LVM state.</p></li><li><p>The device must not be mounted.</p></li><li><p>The device must not contain a file system.</p></li><li><p>The device must not contain a Ceph BlueStore OSD.</p></li><li><p>The device must be larger than 5 GB.</p></li></ul></blockquote><p>简单来说，就是将一块干净的磁盘插入机器后，什么都不用做就好。</p><p>在实验提供的虚拟机中，每台机器都额外插入了一块这样干净的磁盘。</p><p>可以通过<code>fdisk -l</code>来查看：</p><p><img src=https://cdn.loheagn.com/090352.png alt></p><p>注意看上面这两块磁盘：</p><ul><li><p>第一个名称是<code>/dev/sda</code>，容量是 16G，有两个分区：<code>/dev/sda1</code>，<code>/dev/sda2</code>。这就是我们现在在使用的这个系统所用的磁盘，系统数据都存储在这个磁盘中。</p></li><li><p>第二个名称是<code>/dev/sdb</code>，容量是 10G，没有任何分区。这就是我们即将交给 OSD 管理的磁盘。</p></li></ul><p>使用下面的命令来创建 OSD 进程：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph orch daemon add osd *&lt;hostname&gt;*:*&lt;device-name&gt;*
</span></span></code></pre></div><p>比如，你要在主机<code>ceph-01-20210000</code>的名称为<code>/dev/sdb</code>的磁盘上创建 OSD 进程，那么命令应该是：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph orch daemon add osd ceph-01-20210000:/dev/sdb
</span></span></code></pre></div><blockquote class="book-hint info"><p>这条命令默认不会有输出创建 OSD 进程的详细信息，也就是说，如果该命令很耗时的话，那么你将在什么输出都没有的情况下等待较长时间，这可能令人发慌。你可以加上<code>--verbose</code>参数，来让它输出详细信息。</p><p>比如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph orch daemon add osd ceph-01:/dev/sdb --verbose
</span></span></code></pre></div></blockquote><p>执行完成后，可以使用<code>ceph -s</code>查看当前的集群状态，可以发现已经有一个 OSD 进程加入了进来。</p><p><img src=https://cdn.loheagn.com/105544.png alt></p><p>使用同样的方法，将所有节点的附加硬盘都加入进来。至此，我们可以再使用<code>ceph -s</code>查看当前集群的状态。</p><h2 id=ceph-filesystem-选>Ceph Filesystem (选)
<a class=anchor href=#ceph-filesystem-%e9%80%89>#</a></h2><p><a href=https://docs.ceph.com/en/latest/cephfs/>参考资料: CEPH FILE SYSTEM</a></p><p>Ceph 文件系统 (Ceph FS)是个 POSIX 兼容的文件系统，它使用 Ceph 存储集群来存储数据。 Ceph 文件系统与 Ceph 块设备、对象存储或者原生库 (librados) 一样，都使用着相同的 Ceph 存储集群系统。</p><p>Ceph 文件系统要求 Ceph 存储集群内至少有一个 Ceph 元数据服务器 MDS。</p><h3 id=部署-cephfs>部署 CephFS
<a class=anchor href=#%e9%83%a8%e7%bd%b2-cephfs>#</a></h3><p>以上其实都是在搭建 Ceph 集群的环境，我们添加了 3 个 OSD 进程组建起了 Ceph Cluster。接下来，我们便可以在此基础上来具体地使用到 Ceph 所提供的分布式存储能力，从 Ceph 文件系统 CephFS 开始~</p><p>查看各个节点的主机名</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph orch host ls
</span></span></code></pre></div><p>输入类似如下</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 可有类似输出</span>
</span></span><span style=display:flex><span>HOST   ADDR      LABELS  STATUS
</span></span><span style=display:flex><span>host1  10.1.2.3
</span></span><span style=display:flex><span>host2  10.1.2.4
</span></span><span style=display:flex><span>host3  10.1.2.5
</span></span></code></pre></div><div class=book-tabs><input type=radio class=toggle name="tabs-Deploy CephFS" id="tabs-Deploy CephFS-0" checked>
<label for="tabs-Deploy CephFS-0">Automatic Setup</label><div class="book-tabs-content markdown-inner"><p>创建 CephFS 的前提是需要至少一个的 MDS daemon 元数据服务器守护进程。其实以下的一条命令即可自动地创建好 MDS daemon、Pool 等：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph fs volume create &lt;fs_name&gt; <span style=color:#f92672>[</span>--placement<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&lt;placement spec&gt;&#34;</span><span style=color:#f92672>]</span>
</span></span></code></pre></div><p>其中，<code>fs_name</code> 是 CephFS 的名称，后面的 <code>--placement</code> 为可选参数，可以通过它来指定 daemon container 跑在哪几个 hosts 上（
<a href=https://docs.ceph.com/en/latest/cephfs/fs-volumes/>参考资料</a>
）。例如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph fs volume create ceph_fs --placement<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;ceph-01-20210000,ceph-02-20210000,ceph-03-20210000&#34;</span>
</span></span></code></pre></div></div><input type=radio class=toggle name="tabs-Deploy CephFS" id="tabs-Deploy CephFS-1">
<label for="tabs-Deploy CephFS-1">More Customized Setup</label><div class="book-tabs-content markdown-inner"><p><a href=https://amito.me/2018/Pools-and-Placement-Groups-in-Ceph/>参考：下列命令的一些具体参数含义</a></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph osd pool create cephfs_data <span style=color:#ae81ff>8</span> <span style=color:#ae81ff>8</span> <span style=color:#75715e># 后面的数量可以调，设大了会无法创建，数值和osd的数量有关，需要是2的倍数</span>
</span></span><span style=display:flex><span><span style=color:#75715e># pool &#39;cephfs_data&#39; created</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ceph osd pool create cephfs_metadata <span style=color:#ae81ff>8</span> <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span><span style=color:#75715e># pool &#39;cephfs_metadata&#39; created</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ceph fs new cephfs cephfs_metadata cephfs_data
</span></span><span style=display:flex><span><span style=color:#75715e># new fs with metadata pool 3 and data pool 2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ceph fs ls
</span></span><span style=display:flex><span><span style=color:#75715e># name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ceph orch apply mds cephfs --placement<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;3 node1 node2 node3&#34;</span> <span style=color:#75715e># 应用部署CephFS</span>
</span></span></code></pre></div></div></div><p>于是，我们便创建成功了 CephFS。</p><p>你可能会想到如何删除 CephFS，Ceph 中非常“贴心”地防止你误删除，所以删除起来会有一些麻烦。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph fs volume rm &lt;fs_name&gt; <span style=color:#f92672>[</span>--yes-i-really-mean-it<span style=color:#f92672>]</span> <span style=color:#75715e># 要加上这么一长串后缀，但这样其实还是没法删除</span>
</span></span><span style=display:flex><span>ceph config set mon mon_allow_pool_delete true <span style=color:#75715e># 还需要通过这条命令修改ceph config配置</span>
</span></span></code></pre></div><blockquote class="book-hint info"><ul><li>通过 <code>rados df</code> 命令可查看刚才创建的资源池 Pool 的相关信息。</li><li><code>ceph fs ls</code> 可列出 CephFS。</li><li><code>ceph fs status</code> 可查看 CephFS 状态，验证当前已有至少一个 MDS 处在 Active 状态。</li><li>还可经常性地执行 <code>ceph -s</code> 查看 Ceph 集群的状态。</li></ul></blockquote><h3 id=挂载-cephfs>挂载 CephFS
<a class=anchor href=#%e6%8c%82%e8%bd%bd-cephfs>#</a></h3><p><a href=https://people.redhat.com/bhubbard/nature/default/cephfs/fuse/>参考资料：MOUNT CEPHFS USING FUSE</a></p><p>CephFS 在创建后应当能被实际使用，如完成分布式存储文件的任务。在这一步，我们将把 CephFS 挂载到 Client 端，让 Client 能够创建和存储文件。你需要选择除 Bootstrap Host 之外的任意一台机器作为 Client 端。</p><p>我们先要对 Client 端进行一些配置，保证 Client 端能连接到 MON 主机，即 Bootstrap Host。</p><p>第一步：给 Client 端创建一个最小配置文件，放置在 /etc/ceph 目录下：</p><blockquote class="book-hint danger"><p><strong>这一步操作高危！</strong></p><p>以防万一，请先执行两个操作：</p><ol><li>在两台机器上执行<code>cat /etc/ceph/ceph.conf > /etc/ceph/backup_ceph.conf</code> 备份原来的 <code>ceph.conf</code></li><li>在 cephadm shell (即 bootstarp 主机) 里面，用 <code>ceph config generate-minimal-conf</code> 生成 config，将生成出来的内容也保存备份一下 (复制粘贴+截图大法 / 重定向输出)</li></ol></blockquote><p>在 Client 端执行（将<code>{user}</code>替换成<code>root</code>，<code>{mon-host}</code>替换成 Bootstrap Host 的 IP 地址）</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># on client host</span>
</span></span><span style=display:flex><span>mkdir /etc/ceph
</span></span><span style=display:flex><span>ssh <span style=color:#f92672>{</span>user<span style=color:#f92672>}</span>@<span style=color:#f92672>{</span>mon-host<span style=color:#f92672>}</span> <span style=color:#e6db74>&#34;sudo ceph config generate-minimal-conf&#34;</span> | sudo tee /etc/ceph/ceph.conf
</span></span><span style=display:flex><span>chmod <span style=color:#ae81ff>644</span> /etc/ceph/ceph.conf <span style=color:#75715e># 赋权</span>
</span></span></code></pre></div><p>如果不能成功，可直接到 MON 主机执行 <code>sudo ceph config generate-minimal-conf</code>，将输出的内容粘贴到 <code>/etc/ceph/ceph.conf</code>（下同）。如果上述操作导致 Client/Bootstrap Host 挂了，多半是 <code>/etc/ceph/ceph.conf</code> 被误清空了，将先前备份的 <code>ceph.conf</code> 写回即可恢复。</p><p>第二步：生成 CephX 用户名和密钥（将<code>{user}</code>替换成<code>root</code>，<code>{mon-host}</code>替换成 Bootstrap Host 的 IP 地址）:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># on client host</span>
</span></span><span style=display:flex><span>ssh <span style=color:#f92672>{</span>user<span style=color:#f92672>}</span>@<span style=color:#f92672>{</span>mon-host<span style=color:#f92672>}</span> <span style=color:#e6db74>&#34;sudo ceph fs authorize ceph_fs client.foo / rw&#34;</span> | sudo tee /etc/ceph/ceph.keyring
</span></span><span style=display:flex><span>chmod <span style=color:#ae81ff>600</span> /etc/ceph/ceph.keyring <span style=color:#75715e># 赋权</span>
</span></span></code></pre></div><p>在上述命令中，<code>ceph_fs</code> 是先前所创建的 CephFS 的名称，请将其替换。foo 是 CephX 的用户名，也可自己起。</p><p>以上是前置准备，完成后，我们可通过 ceph-fuse 工具实现文件挂接。如机器上没有，则需要连网安装一下。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>yum install -y ceph-fuse
</span></span></code></pre></div><p>安装完成后，我们可以创建一个被挂接的目录，如 <code>mycephfs</code>：<code>mkdir /mnt/mycephfs</code></p><p>执行 <code>ceph-fuse --id foo -m {mon-host}:6789 /mnt/mycephfs</code> 即可完成挂接，如<code>ceph-fuse --id foo -m 10.251.252.182:6789 /mnt/mycephfs</code>，如果此命令不能成功运行，可从
<a href=https://people.redhat.com/bhubbard/nature/default/cephfs/fuse/>参考资料：MOUNT CEPHFS USING FUSE</a>试一下其他的命令</p><p>若想取消挂接非常简单，只需 <code>umount /mnt/mycephfs</code>。</p><blockquote class="book-hint info"><p><strong>如何判断挂接成功？</strong></p><p>上述命令不报错是一方面，我们也可以通过一些命令来看挂接的情况。</p><ul><li><code>lsblk</code> 列出所有可用块设备的信息，还能显示他们之间的依赖关系</li><li><code>df -h</code> 查看磁盘占用的空间</li></ul><p>通过这些命令，应能看到挂接盘 mycephfs 的存在，查看到其容量等信息。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>文件系统                 容量  已用  可用 已用% 挂载点
</span></span><span style=display:flex><span>ceph-fuse                9.4G     <span style=color:#ae81ff>0</span>  9.4G    0% /mnt/mycephfs
</span></span></code></pre></div></blockquote><h2 id=ceph-rgw-对象存储-选>Ceph RGW 对象存储 (选)
<a class=anchor href=#ceph-rgw-%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8-%e9%80%89>#</a></h2><p>Ceph RGW(即 RADOS Gateway)是 Ceph 对象存储网关服务，是基于 LIBRADOS 接口封装实现的 FastCGI 服务，对外提供存储和管理对象数据的 Restful API。对象存储适用于图片、视频等各类文件的上传下载，可以设置相应的访问权限。目前 Ceph RGW 兼容常见的对象存储 API，例如兼容绝大部分 Amazon S3 API，兼容 OpenStack Swift API。</p><p>通俗理解是 RGW 作为一个协议转换层，把从上层应用符合 S3 或 Swift 协议的请求转换成 rados 的请求，将数据保存在 rados 集群中。</p><p><img src=https://docs.ceph.com/en/octopus/_images/1ae399f8fa9af1042d3e1cbf31828f14eb3fe01a6eb3352f88c3d2a04ac4dc50.png alt=rgw></p><blockquote class="book-hint info"><p><strong>内部概念</strong></p><ul><li>zone：包含多个 RGW 实例的一个逻辑概念。zone 不能跨集群，同一个 zone 的数据保存在同一组 pool 中。</li><li>zonegroup：一个 zonegroup 如果包含 1 个或多个 zone。如果一个 zonegroup 包含多个 zone，必须指定一个 zone 作为 master</li><li>zone，用来处理 bucket 和用户的创建。一个集群可以创建多个 zonegroup，一个 zonegroup 也可以跨多个集群。</li><li>realm：一个 realm 包含 1 个或多个 zonegroup。如果 realm 包含多个 zonegroup，必须指定一个 zonegroup 为 master</li><li>zonegroup， 用来处理系统操作。一个系统中可以包含多个 realm，多个 realm 之间资源完全隔离。</li></ul><p><strong>外部概念</strong></p><ul><li>user：对象存储的使用者，默认情况下，一个用户只能创建 1000 个存储桶。</li><li>bucket：存储桶，用来管理对象的容器。</li><li>object：对象，泛指一个文档、图片或视频文件等，尽管用户可以直接上传一个目录，但是 ceph 并不按目录层级结构保存对象， ceph 所有的对象扁平化的保存在 bucket 中。</li></ul><p><a href=https://durantthorvalds.top/2021/01/03/%E3%80%8C%E6%A0%B8%E5%BF%83%E3%80%8DCeph%E5%AD%A6%E4%B9%A0%E4%B8%89%E9%83%A8%E6%9B%B2%E4%B9%8B%E4%B8%83%EF%BC%9A%E5%AF%B9%E8%B1%A1%E5%AD%98%E5%82%A8%E7%BD%91%E5%85%B3RGW/>参考阅读</a></p></blockquote><h3 id=deploy-rgw>Deploy RGW
<a class=anchor href=#deploy-rgw>#</a></h3><p><a href=https://docs.ceph.com/en/latest/cephadm/services/rgw/>参考：RGW SERVICE</a></p><ul><li>To deploy a set of radosgw daemons, with an arbitrary service name name, run the following command:</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph orch apply rgw *&lt;name&gt;* <span style=color:#f92672>[</span>--realm<span style=color:#f92672>=</span>*&lt;realm-name&gt;*<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>--zone<span style=color:#f92672>=</span>*&lt;zone-name&gt;*<span style=color:#f92672>]</span> --placement<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;*&lt;num-daemons&gt;* [*&lt;host1&gt;* ...]&#34;</span>
</span></span></code></pre></div><p>其中，<code>[]</code>中的内容为可选项，可以都添上。如 <code>ceph orch apply rgw ceph_rgw --realm=default --zone=default --placement=3</code>。<code>--placement</code>参数的使用和先前实验也是类似的，还可以用 <code>3 node1 node2 node3</code> 来完成指定。</p><p>其实这一条命令就够了，然后我们可查看各个 rgw 节点是否已启动：<code>ceph orch ps --daemon-type rgw</code>。</p><p>应能看到 <code>rgw*</code> 均为 <code>running</code> 的 STATUS，则表明顺利启动。若为 <code>starting</code> 可稍等其转为 <code>running</code>。</p><blockquote class="book-hint info"><p>如一直显示 unknown/error 的 STATUS，也是因为 pg 资源不足（受 osd 数量所限）导致的，可以将前面创建的 CephFS 删掉，释放资源。</p><p>删除刚刚创建的 unknown/error 的 rgw 的命令为<code>ceph orch rm rgw.*&lt;rgw_name>*</code></p></blockquote><p>在执行上述命令的 Bootstrap host，<code>curl &lt;bootstrap_host_ip:80></code> 应能看到包含了 <code>&lt;Buckets/></code> 的 XML 形式的输出。</p><h3 id=使用对象存储>使用对象存储
<a class=anchor href=#%e4%bd%bf%e7%94%a8%e5%af%b9%e8%b1%a1%e5%ad%98%e5%82%a8>#</a></h3><p>我们为 rgw 创建用户：<code>radosgw-admin user create --uid=&lt;username> --display-name=&lt;your_display_name> --system</code>。如 <code>radosgw-admin user create --uid=s3 --display-name="object_storage" --system</code>。执行后，能看到类似输出：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;user_id&#34;</span>: <span style=color:#e6db74>&#34;s3&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;display_name&#34;</span>: <span style=color:#e6db74>&#34;objcet_storage&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;email&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;suspended&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;max_buckets&#34;</span>: <span style=color:#ae81ff>1000</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;subusers&#34;</span>: [],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;keys&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;user&#34;</span>: <span style=color:#e6db74>&#34;x&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;access_key&#34;</span>: <span style=color:#e6db74>&#34;xxxxxxxxkey01&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:#f92672>&#34;secret_key&#34;</span>: <span style=color:#e6db74>&#34;xxxxxxxxkey01&#34;</span>
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;swift_keys&#34;</span>: [],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;caps&#34;</span>: [],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;op_mask&#34;</span>: <span style=color:#e6db74>&#34;read, write, delete&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;system&#34;</span>: <span style=color:#e6db74>&#34;true&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;default_placement&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;default_storage_class&#34;</span>: <span style=color:#e6db74>&#34;&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;placement_tags&#34;</span>: [],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;bucket_quota&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;enabled&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;check_on_raw&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;max_size&#34;</span>: <span style=color:#ae81ff>-1</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;max_size_kb&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;max_objects&#34;</span>: <span style=color:#ae81ff>-1</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;user_quota&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;enabled&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;check_on_raw&#34;</span>: <span style=color:#66d9ef>false</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;max_size&#34;</span>: <span style=color:#ae81ff>-1</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;max_size_kb&#34;</span>: <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>    <span style=color:#f92672>&#34;max_objects&#34;</span>: <span style=color:#ae81ff>-1</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;temp_url_keys&#34;</span>: [],
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;type&#34;</span>: <span style=color:#e6db74>&#34;rgw&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:#f92672>&#34;mfa_ids&#34;</span>: []
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>从命令行的输出中，可以看到 <code>access_key: xxxxxxxxkey01</code> 和 <code>secret_key: xxxxxxxxkey01</code>，我们将其截图或复制保存下来，后面还要用。</p><p>使用 Ceph 的 RGW 对象存储，可以有很多工具，如 s3cmd、 minio-client 等，这里我们以 s3cmd 为例。有兴趣的还可以尝试
<a href=https://github.com/minio/minio>minio</a>可以可视化进行操作。</p><p>安装 s3cmd</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>yum -y install s3cmd
</span></span></code></pre></div><p>我们执行 <code>s3cmd --configure</code> 生成一个配置文件。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; Access Key: <span style=color:#e6db74>&#34;xxxxxxxxkey01&#34;</span>
</span></span><span style=display:flex><span>&gt; Secret Key: <span style=color:#e6db74>&#34;xxxxxxxxkey02&#34;</span>
</span></span><span style=display:flex><span>（没有写到的直接按回车）
</span></span><span style=display:flex><span>&gt; S3 Endpoint <span style=color:#f92672>[</span>s3.amazonaws.com<span style=color:#f92672>]</span>: <span style=color:#e6db74>&#34;&lt;bootstrap_host_ip:80&gt;&#34;</span>，如 <span style=color:#e6db74>&#34;10.1.1.2:80&#34;</span>
</span></span><span style=display:flex><span>&gt; DNS-style bucket....<span style=color:#f92672>[</span>%<span style=color:#f92672>(</span>bucket<span style=color:#f92672>)</span>s.s3.amazonaws.com<span style=color:#f92672>]</span>: <span style=color:#e6db74>&#34;&lt;bootstrap_host_ip:80&gt;/%(bucket)s&#34;</span>，如<span style=color:#e6db74>&#34;10.1.1.2:80/%(bucket)s&#34;</span>
</span></span><span style=display:flex><span>&gt; Use HTTPS protocol <span style=color:#f92672>[</span>Yes<span style=color:#f92672>]</span>: no
</span></span><span style=display:flex><span>（其余继续按回车）
</span></span><span style=display:flex><span>&gt; Save settings? <span style=color:#f92672>[</span>y/N<span style=color:#f92672>]</span> y
</span></span><span style=display:flex><span>Configuration saved to <span style=color:#e6db74>&#39;/root/.s3cfg&#39;</span>
</span></span></code></pre></div><p>进入到刚刚保存新建的 config：<code>/root/.s3cfg</code>中，继续修改<code>signature_v2</code>为<code>True</code>，可以自行检查其它选项是否填写正确</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>default<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>signature_v2 <span style=color:#f92672>=</span> True
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>查看 Bucket：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>s3cmd ls
</span></span></code></pre></div><p>一开始没有创建过 Bucket，故没有输出，我们来新建一个 <code>s3cmd mb s3://s3cmd-demo</code>，再执行 <code>s3cmd ls</code>，即可看到新创建的 bucket。</p><p><a href=https://www.cnblogs.com/sunhongleibibi/p/11661123.html>参考资料：使用 s3cmd</a></p><p>参考以上资料，可以尝试继续上传文件、上传文件夹、下载、<code>ls</code> 、删除等命令，体验 ceph-rgw 的 Bucket 与 S3 存储的交互。</p><blockquote class="book-hint info"><p>在 Dashboard 里有丰富的信息，可以多多尝试。如查看 rgw 的用户、查看 Bucket、Pool 等，欢迎多多体验。</p><p>如上传文件不成功，有提示：<code>ERROR: S3 error: 416 (InvalidRange)</code> 的错误，同样也可释放掉一些先前创建的资源，如 drop 掉 CephFS 再试试，Dashboard 里也能删除 Pool。OSD 只有 3 个，导致 PG 数量吃紧可能不够用。</p></blockquote><h2 id=ceph-rbd-选>Ceph RBD (选)
<a class=anchor href=#ceph-rbd-%e9%80%89>#</a></h2><blockquote class="book-hint info"><p><a href=https://docs.ceph.com/en/pacific/rbd/index.html>参考：CEPH BLOCK DEVICE</a></p><p>RBD 即 RADOS Block Device 的简称，RBD 块存储是最稳定且最常用的存储类型。RBD 块设备类似磁盘可以被挂载。RBD 块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在 Ceph 集群的多个 OSD 中。如下是对 Ceph RBD 的理解：</p><ul><li>RBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用；</li><li>resizable：这个块可大可小；</li><li>data striped：这个块在 Ceph 里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下；</li><li>thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U 盘，存了一个 2G 的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间；</li></ul><p>块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。</p><p>Ceph 可以通过内核模块和 librbd 库提供块设备支持。客户端可以通过内核模块挂在 rbd 使用，客户端使用 rbd 块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过 librbd 使用 ceph 块，典型的是云平台的块存储服务（如下图），云平台可以使用 rbd 作为云的存储后端提供镜像存储、volume 块或者客户的系统引导盘等。</p></blockquote><ul><li>创建 RBD</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph osd pool create rbd <span style=color:#ae81ff>8</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 值调小些，因为云平台资源有限，3个OSD的PG数默认是有上限的</span>
</span></span></code></pre></div><ul><li>application enable RBD</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ceph osd pool application enable rbd rbd
</span></span></code></pre></div><ul><li>创建 rbd 存储, 指定大小为 1GB</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>rbd create rbd1 --size <span style=color:#ae81ff>1024</span>
</span></span></code></pre></div><ul><li>查看 rbd 信息</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; rbd --image rbd1 info
</span></span><span style=display:flex><span>rbd image <span style=color:#e6db74>&#39;rbd1&#39;</span>:
</span></span><span style=display:flex><span>	size <span style=color:#ae81ff>1</span> GiB in <span style=color:#ae81ff>256</span> objects
</span></span><span style=display:flex><span>	order <span style=color:#ae81ff>22</span> <span style=color:#f92672>(</span><span style=color:#ae81ff>4</span> MiB objects<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>	snapshot_count: <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>	id: ace58352cf47
</span></span><span style=display:flex><span>	block_name_prefix: rbd_data.ace58352cf47
</span></span><span style=display:flex><span>	format: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
</span></span><span style=display:flex><span>	op_features:
</span></span><span style=display:flex><span>	flags:
</span></span><span style=display:flex><span>	create_timestamp: Wed Dec  <span style=color:#ae81ff>1</span> 03:55:11 <span style=color:#ae81ff>2021</span>
</span></span><span style=display:flex><span>	access_timestamp: Wed Dec  <span style=color:#ae81ff>1</span> 03:55:11 <span style=color:#ae81ff>2021</span>
</span></span><span style=display:flex><span>	modify_timestamp: Wed Dec  <span style=color:#ae81ff>1</span> 03:55:11 <span style=color:#ae81ff>2021</span>
</span></span></code></pre></div><p>继续执行以下命令：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>&gt; root@ceph:/mnt# ceph osd crush tunables hammer
</span></span><span style=display:flex><span>adjusted tunables profile to hammer
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# ceph osd crush reweight-all
</span></span><span style=display:flex><span>reweighted crush hierarchy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 关闭一些内核默认不支持的特性</span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# rbd feature disable rbd1 exclusive-lock object-map fast-diff deep-flatten
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看特性是否已禁用</span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# rbd --image rbd1 info | grep features
</span></span><span style=display:flex><span>	features: layering
</span></span><span style=display:flex><span>	op_features:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 映射到客户端(在需要挂载的客户端运行)</span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# rbd map --image rbd1
</span></span><span style=display:flex><span>/dev/rbd0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看映射情况</span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# rbd showmapped
</span></span><span style=display:flex><span>id  pool  namespace  image  snap  device
</span></span><span style=display:flex><span><span style=color:#ae81ff>0</span>   rbd              rbd1   -     /dev/rbd0
</span></span></code></pre></div><p>再继续！</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 格式化磁盘</span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# mkfs.xfs /dev/rbd0
</span></span><span style=display:flex><span>meta-data<span style=color:#f92672>=</span>/dev/rbd0              isize<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>    agcount<span style=color:#f92672>=</span>8, agsize<span style=color:#f92672>=</span><span style=color:#ae81ff>32768</span> blks
</span></span><span style=display:flex><span>         <span style=color:#f92672>=</span>                       sectsz<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>   attr<span style=color:#f92672>=</span>2, projid32bit<span style=color:#f92672>=</span>1
</span></span><span style=display:flex><span>         <span style=color:#f92672>=</span>                       crc<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>        finobt<span style=color:#f92672>=</span>1, sparse<span style=color:#f92672>=</span>1, rmapbt<span style=color:#f92672>=</span>0
</span></span><span style=display:flex><span>         <span style=color:#f92672>=</span>                       reflink<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>data     <span style=color:#f92672>=</span>                       bsize<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   blocks<span style=color:#f92672>=</span>262144, imaxpct<span style=color:#f92672>=</span>25
</span></span><span style=display:flex><span>         <span style=color:#f92672>=</span>                       sunit<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span>     swidth<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span> blks
</span></span><span style=display:flex><span>naming   <span style=color:#f92672>=</span>version <span style=color:#ae81ff>2</span>              bsize<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   ascii-ci<span style=color:#f92672>=</span>0, ftype<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>log      <span style=color:#f92672>=</span>internal log           bsize<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   blocks<span style=color:#f92672>=</span>2560, version<span style=color:#f92672>=</span>2
</span></span><span style=display:flex><span>         <span style=color:#f92672>=</span>                       sectsz<span style=color:#f92672>=</span><span style=color:#ae81ff>512</span>   sunit<span style=color:#f92672>=</span><span style=color:#ae81ff>16</span> blks, lazy-count<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>realtime <span style=color:#f92672>=</span>none                   extsz<span style=color:#f92672>=</span><span style=color:#ae81ff>4096</span>   blocks<span style=color:#f92672>=</span>0, rtextents<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 创建挂载目录, 并将 rbd 挂载到指定目录</span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# mkdir /mnt/rbd
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# mount /dev/rbd0 /mnt/rbd/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看挂载情况</span>
</span></span><span style=display:flex><span>&gt; root@ceph:/mnt# df -hl | grep rbd
</span></span><span style=display:flex><span>/dev/rbd0      1014M   40M  975M   4% /mnt/rbd
</span></span></code></pre></div><blockquote class="book-hint info">和 CephFS 类似，我们同样可在挂载的目录中创建修改文件，感受 Ceph 的能力——如分布式存储的容错啊，存储共享等。</blockquote><h2 id=实验报告模板>实验报告模板
<a class=anchor href=#%e5%ae%9e%e9%aa%8c%e6%8a%a5%e5%91%8a%e6%a8%a1%e6%9d%bf>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-markdown data-lang=markdown><span style=display:flex><span># Lab03 Ceph 存储实践
</span></span><span style=display:flex><span><span style=color:#66d9ef>
</span></span></span><span style=display:flex><span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>班级：
</span></span></span><span style=display:flex><span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>学号：
</span></span></span><span style=display:flex><span><span style=font-style:italic></span><span style=color:#66d9ef>&gt; </span><span style=font-style:italic>姓名：
</span></span></span><span style=display:flex><span><span style=font-style:italic></span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 基本概念思考
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span>回答下列问题：
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### 请阐述Placement Group (PG) 与 Placement Group for Placement purpose (PGP)的区别和关系
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>### Ceph 集群的 HEALTH STATUS
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Ceph 部署
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 本部分内容可以根据部署方式的不同进行不同的改动，这里是使用Cephadm部署的模板 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### 实验前置准备
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>### Bootstrap
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看ceph -s的输出 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### Ceph Dashboard
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看Ceph Dashboard登录后的界面 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### 添加其他节点并创建 OSD 进程
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看ceph -s的输出 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Ceph Filesystem (选)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>### 部署 CephFS (选)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看ceph fs status的输出 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### 挂载 CephFS (选)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看df -h和文件读写命令的输出 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Ceph RGW 对象存储 (选)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#75715e>### Deploy RGW (选)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看radosgw-admin user create的输出 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>### 使用对象存储 (选)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看上传文件、上传文件夹、下载、ls、删除等命令的输出 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## Ceph RBD (选)
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#75715e>&lt;!-- 截图查看rbd --image rbd1 info的输出 --&gt;</span>
</span></span><span style=display:flex><span><span style=color:#75715e>&lt;!-- 截图查看df -hl | grep rbd和文件读写命令的输出 --&gt;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## 自行扩展和设计内容 (选)
</span></span></span></code></pre></div></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/commit/5bc9ed516b616926bdba01b5e36f273ce8a6b63f title='Last modified by “h56983577” | November 23, 2022' target=_blank rel=noopener><img src=/doc/svg/calendar.svg class=book-icon alt=Calendar>
<span>November 23, 2022</span></a></div><div><a class="flex align-center" href=https://github.com/alex-shpak/hugo-book/edit/main/exampleSite/content/ns-labs/table-of-contents/ceph/index.md target=_blank rel=noopener><img src=/doc/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#lab03-ceph-存储集群实践>Lab03 Ceph 存储集群实践</a><ul><li><a href=#实验目的>实验目的</a></li><li><a href=#实验说明>实验说明</a></li><li><a href=#概述>概述</a></li><li><a href=#重要概念>重要概念</a><ul><li></li></ul></li><li><a href=#实验环境介绍>实验环境介绍</a></li><li><a href=#ceph-部署>Ceph 部署</a><ul><li><a href=#cephadm介绍>Cephadm介绍</a></li><li><a href=#bootstrap>BootStrap</a></li><li><a href=#ceph-dashboard>Ceph Dashboard</a></li><li><a href=#添加其他节点>添加其他节点</a></li><li><a href=#创建-osd-进程>创建 OSD 进程</a></li></ul></li><li><a href=#ceph-filesystem-选>Ceph Filesystem (选)</a><ul><li><a href=#部署-cephfs>部署 CephFS</a></li><li><a href=#挂载-cephfs>挂载 CephFS</a></li></ul></li><li><a href=#ceph-rgw-对象存储-选>Ceph RGW 对象存储 (选)</a><ul><li><a href=#deploy-rgw>Deploy RGW</a></li><li><a href=#使用对象存储>使用对象存储</a></li></ul></li><li><a href=#ceph-rbd-选>Ceph RBD (选)</a></li><li><a href=#实验报告模板>实验报告模板</a></li></ul></li></ul></nav></div></aside></main></body></html>