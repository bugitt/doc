[{"id":0,"href":"/doc/ns-labs/table-of-contents/raid/","title":"Lab01 RAID 阵列","section":"Table of Contents","content":" Lab01 RAID阵列 # 本指导书是实验过程的一个示例，供不熟悉实验环境的同学参考，无需严格按照指导书中的步骤来进行实验 :)\n实验内容 # 本实验通过软件 RAID 管理磁盘：\n通过工具查看磁盘列表 [仅Windows] C 盘扩展卷，检查其它磁盘是否联机，联机并初始化磁盘 [仅Ubuntu] 使用LVM扩展根目录/的容量 创建RAID阵列，测试读写文件并模拟磁盘损坏，观察有什么情况发生，数据是否损坏或者丢失？ 尝试RAID 0 尝试RAID 1 尝试RAID 5 [选做] 控制变量，测试RAID阵列的读写速率，并结合理论分析实验结果 [选做] 自行设计与扩展，体会 RAID 0 1 5 10 等方案下，在遇到磁盘损坏故障、冗余备份的效果等 [选做] 思考题： 为什么及在什么条件下选择用 RAID RAID \u0026amp; 分布式存储 \u0026amp; 集中存储的区别 其中，选做的题目不计入本实验的总得分，仅额外加分\n实验报告模板分别是lab01_win.md和lab01_ubuntu.md，供参考使用，最后请将实验报告按 lab01-学号-姓名.pdf 的命名格式提交\n实验准备 # Windows下连接Windows 10虚拟机 # 使用系统自带的“远程桌面连接”即可\nMacOS下连接Windows 10虚拟机 # 使用Microsoft Remote Desktop连接，云平台课程资源页提供该软件的下载\nWindows下连接Ubuntu虚拟机 # 一般来讲，Windows 10（及以上）自带的cmd.exe都自带ssh client，打开cmd后直接ssh foo@x.x.x.x即可登录\n为了更好的使用体验，推荐下载使用 Windows Terminal\n当然，你也可以使用 termius 或者其他工具（如 Xshell等）进行多个ssh连接的管理\nMacOS下连接Ubuntu虚拟机 # 使用系统自带的Terminal.app登录即可\n为了更好的使用体验，推荐使用 iterm2 登录\n当然，你也可以使用 termius 进行多个ssh连接的管理\n校外连接Ubuntu虚拟机 # 在云平台的虚拟机列表中点击“连接”即可 Ubuntu虚拟机联网 # 虚拟机已内置联网登录工具\nbuaalogin config buaalogin login 重要提醒 # 请登录实验虚拟机后，及时更改虚拟机密码，以避免以下情况发生：\n未意识到登录了他人的虚拟机， 帮助别人完成了实验工作 自己的虚拟机被别人登录，已经做完的实验被破坏 Windows环境 # 点击桌面上的“计算机管理”，在左侧菜单栏打开“计算机管理-存储-磁盘管理” 右键点击C盘，选择“扩展卷”，即可把C盘所在磁盘上未分区的80GB空间合并至C盘内 右键点击未分配的磁盘，看到可以选择创建跨区卷、带区卷、镜像卷、RAID-5卷，它们分别对应不做条带化的RAID 0、条带化的RAID 0、RAID 1、RAID 5，这里以RAID 5为例 选择阵列中包括的磁盘，下一步选中“快速格式化”，最后确认转化为动态磁盘，即可创建阵列 点击桌面上的“ATTO 磁盘基准测试”，选择RAID 5阵列所在的盘符，测试读写速度 在RAID 5阵列上保存文件，将其中的一块磁盘脱机，模拟磁盘损坏，文件仍然可以正常查看 Ubuntu环境 # 实验全程在root权限下进行\nsudo -i 使用LVM为根目录扩容 # 查看虚拟机上的磁盘列表，其中有sdb~sdf共五块磁盘可以供本次实验使用，sda是系统所在的磁盘，容量为50GB\n$ lsblk -d sda 8:0 0 50G 0 disk sdb 8:16 0 10G 0 disk sdc 8:32 0 10G 0 disk sdd 8:48 0 10G 0 disk sde 8:64 0 10G 0 disk sdf 8:80 0 10G 0 disk 查看根目录/的容量，为15GB，说明sda磁盘上有空闲空间没有利用上\n$ df -lh / Filesystem Size Used Avail Use% Mounted on /dev/mapper/ubuntu--vg-ubuntu--lv 15G 4.9G 9.1G 35% / 使用fdisk命令给空闲的磁盘空间分区\n在Command (m for help):后面输入n，一路回车，默认按磁盘最大容量分区\n在Command (m for help):后面输入t，输入4选择新创建的分区，输入31选择Linux LVM分区类型\n最后在Command (m for help):后面输入w保存\n$ fdisk /dev/sda Command (m for help): n Partition number (4-128, default 4): First sector (33552384-104857566, default 33552384): Last sector, +/-sectors or +/-size{K,M,G,T,P} (33552384-104857566, default 104857566): Created a new partition 4 of type \u0026#39;Linux filesystem\u0026#39; and of size 34 GiB. Command (m for help): t Partition number (1-4, default 4): 4 Partition type (type L to list all types): 31 Changed type of partition \u0026#39;Linux filesystem\u0026#39; to \u0026#39;Linux LVM\u0026#39;. Command (m for help): w The partition table has been altered. Syncing disks. 创建物理卷（PV），命名格式一般为磁盘名+分区号\npvcreate /dev/sda4 查看卷组（VG）列表，Ubuntu在系统安装时已创建好了一个默认的卷组，如下所示\n$ vgdisplay --- Volume group --- VG Name ubuntu-vg ... 将刚创建的物理卷加入卷组\nvgextend ubuntu-vg /dev/sda4 查看逻辑卷（LV）列表，Ubuntu系统安装在默认逻辑卷上\n$ lvdisplay --- Logical volume --- LV Path /dev/ubuntu-vg/ubuntu-lv ... 将逻辑卷扩容，然后为文件系统（ext4类型）扩容\nlvextend -l +100%FREE /dev/ubuntu-vg/ubuntu-lv resize2fs /dev/ubuntu-vg/ubuntu-lv 查看根目录容量，已扩容至49GB\n$ df -lh / Filesystem Size Used Avail Use% Mounted on /dev/mapper/ubuntu--vg-ubuntu--lv 49G 4.9G 42G 11% / RAID阵列搭建 # 安装mdadm工具\napt-get install mdadm mdadm命令的常用参数及作用如下\n参数 作用 -a 检测设备名称 -n 指定设备数量 -l 指定RAID级别 -C 创建 -v 显示过程 -f 模拟设备损坏 -r 移除设备 -Q 查看摘要信息 -D 查看详细信息 -S 停止RAID磁盘阵列 用命令创建 RAID 1，看到如下输出即为成功\n$ mdadm -Cv /dev/md1 -a yes -n 2 -l 1 /dev/sd{b,c} mdadm: chunk size defaults to 512K mdadm: Defaulting to version 1.2 metadata mdadm: array /dev/md1 started. 将创建好的RAID 1阵列格式化为ext4文件系统\nmkfs.ext4 /dev/md1 查看RAID 1阵列的摘要信息，可以看到两块10GB的磁盘组成的RAID 1阵列容量约为10GB\n$ mdadm -Q /dev/md1 /dev/md1: 9.99GiB raid1 2 devices, 0 spares. Use mdadm --detail for more detail. 将RAID 1阵列临时挂载到/mnt/raid1目录下，测试文件读写\nmkdir -p /mnt/raid1/ mount /dev/md1 /mnt/raid1 echo \u0026#39;Hello, world!\u0026#39; \u0026gt; /mnt/raid1/hello cat /mnt/raid1/hello 测试写文件速率，由于dd命令在计算时单位换算有误，因此我们自己计算出实际值为$8 \\times 300000 \\div 1024 \\div 9.462=247.70MB/s$\n$ time dd if=/dev/zero of=/mnt/raid1/test.bdf bs=8k count=300000 300000+0 records in 300000+0 records out 2457600000 bytes (2.5 GB, 2.3 GiB) copied, 9.46088 s, 260 MB/s real 0m9.462s user 0m0.159s sys 0m2.525s 测试读文件速率，实际值为$8 \\times 1309568 \\div 1024 \\div 37.837=270.40MB/s$\n$ time dd if=/dev/md1 of=/dev/null bs=8k 1309568+0 records in 1309568+0 records out 10727981056 bytes (11 GB, 10 GiB) copied, 37.836 s, 284 MB/s real 0m37.837s user 0m0.456s sys 0m7.154s 模拟阵列中的一块磁盘损坏（注意：此方法仅适用于RAID 1、RAID 5、RAID 10，模拟RAID 0损坏的方法见下一节）\n$ mdadm /dev/md1 -f /dev/sdb mdadm: set /dev/sdb faulty in /dev/md1 再次读取文件，发现文件完好无损\n$ cat /mnt/raid1/hello Hello, world! 删除该阵列，释放磁盘空间\numount /mnt/raid1 mdadm -S /dev/md1 mdadm --zero-superblock /dev/sd{b,c} 同理可创建RAID 0、RAID 5、RAID 10等阵列\nmdadm -Cv /dev/md0 -a yes -n 2 -l 0 /dev/sd{b,c} mdadm -Cv /dev/md5 -a yes -n 5 -l 5 /dev/sd{b,c,d,e,f} mdadm -Cv /dev/md10 -a yes -n 4 -l 10 /dev/sd{b,c,d,e} 模拟RAID 0阵列中磁盘损坏 # 在创建阵列之前，需要先给待使用的磁盘分区，以/dev/sdb为例\n在Command (m for help):后面输入n，一路回车，默认按磁盘最大容量分区\n最后在Command (m for help):后面输入w保存\n$ fdisk /dev/sdb Command (m for help): 新分区名称默认为/dev/sdb1\n同理，为/dev/sdc也创建一个分区/dev/sdc1\n在这两个分区上创建RAID 0阵列\nmdadm -Cv /dev/md0 -a yes -n 2 -l 0 /dev/sd{b1,c1} 再使用fdisk删除分区：fdisk /dev/sdb\n在Command (m for help):后面输入d，然后在Command (m for help):后面输入w保存\n更新分区表并重启\npartprobe reboot 重启后发现RAID 0阵列已损坏\n$ mdadm -Q /dev/md0 mdadm: cannot open /dev/md0: No such file or directory "},{"id":1,"href":"/doc/ns-labs/table-of-contents/virtualization/","title":"Lab02 虚拟化实验","section":"Table of Contents","content":" Lab02 虚拟化实验 # 实验目标 # 实施计算虚拟化，安装配置环境，熟悉计算虚拟化的概念，理解基本操作，掌握基础知识。 理解集中管理对于虚拟化的作用，通过部署集中vCenter体验集群的设置，分布式交换机的设置，了解主机从不同网络进行迁移的实际需求。 实验内容 # 按照实验指南的指导，完成实验。 按照实验报告模板，撰写实验报告，将重要的实验步骤截图，填入实验报告中，并回答相应问题。 本次实验以小组形式进行，虚拟机已分发至每组第一位同学的账户中，每组一台虚拟机，实验报告每个人都需要提交，同组的实验报告内容可以一致\n请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab02-学号-姓名.pdf的格式。\nHypervisor # 我们知道，一台计算机一般有以下的结构：\n操作系统负责管理硬件资源（CPU，内存，硬盘等），并向上提供相应的系统调用，供具体的应用程序使用。\n而我们平常提到的操作系统的虚拟化，本质上就是要模拟出一套硬件（包括虚拟CPU，虚拟内存，虚拟硬盘等），然后在这一套虚拟的硬件的基础上部署客户操作系统。客户操作系统完全不需要做任何修改，即可在这个“虚拟的机器”中顺利执行。但客户操作系统的运行结果（比如接收键盘输入，输出图像和声音等），最终都是要靠原始的“实实在在”的硬件（物理机）来完成的。也就是说，需要有那么一个结构，能够将这个“虚拟的机器”的行为翻译到物理机的行为（比如将虚拟CPU的指令翻译到物理机的CPU指令）。负责做这件事情的结构被称为Hypervisor，又称为虚拟机监控器（virtual machine monitor，缩写为 VMM）。\n根据工作方式的不同，Hypervisor分为以下两种。\n第一种是我们比较熟悉的情况，本质上就是在主操作系统（Host Operating System）上安装了一个虚拟化软件，它来负责充当虚拟机的管理者，并通过主操作系统的系统调用来完成对物理机硬件的使用。VMware Workstation、Virtual Box、Qemu等都属这类虚拟化软件。除了这个虚拟化软件之外，主操作系统上还会运行其他“正常”的应用程序，比如，你在用VMware Workstation的同时还能听歌聊天等。\n第二种Hypervisor则直接舍弃了主操作系统（因为毕竟隔着一层，性能会有损失），而是直接把Hypervisor部署在硬件上。在这种情况下，物理机变成了更纯粹的“为虚拟化而生”的机器。Hypervisor能够直接与硬件沟通，其实在某种程度上也承担了主操作系统的角色（管理硬件），因此，我们也可以把这种Hypervisor看作是一种为虚拟化特制的操作系统。这其中典型的就是VMware ESXi。\n因为我们不可能要求每位同学都制备一套硬件来安装学习VMware ESXi，所以需要首先使用VMware Workstation来模拟出一套硬件。但VMware Workstation仅仅起一个前置作用，在实际的实验中并不会涉及到。请大家首先理清这层关系。\n实验指南 # 0. 安装VMware Workstation # 使用分配的虚拟机的桌面上的安装包安装即可。\n安装完成后需重启机器。\n打开VMware Workstation时， 选择试用即可。\n1. 安装VMware ESXi # 使用桌面上的ESXi镜像VMware-VMvisor-Installer-6.7.0.update03-19898906.x86_64-DellEMC_Customized-A18.iso创建虚拟机。\n注意选择客户操作系统的类型。\n虚拟机创建完成后，直接打开电源即可启动ESXi操作系统的安装流程，这一过程可能需要等待较长时间。\n安装流程中总是保持默认选项即可，其中设置的root密码应至少包含字母、数字和特殊符号，并且请务必牢记设置的root密码。\n在此流程中，可能需要使用使用到某些快捷键，这些快捷键可能会首先被你本机的操作系统捕获，在本机的系统设置中暂时屏蔽该快捷键即可。\n安装完成后，可以看到如下界面。\n可以看到，ESXi系统获得了一个IPv4地址192.168.80.128，并且这个地址是通过DHCP的方式获得的。这里用到的DHCP服务器其实是VMware Workstation内置的。也就是说，192.168.80.128这个地址只有在安装VMware Workstation的机器上才是有效的。\n2. 访问ESXi # 可以直接使用浏览器访问ESXi。访问的地址就是ESXi的地址，用户名和密码与vSphere Client的相同。\n如下图所示，可以为ESXi分配许可证。\n可用的KEY：\n0A65P-00HD0-3Z5M1-M097M-22P7H 3. 观察和体验vSphere Client提供的功能 # Client侧界面主要包含导航器、主体内容和任务事件这三部分。\n请浏览左侧导航栏的不同模块和不同模块下不同选项卡的内容，对ESXi提供的功能有个大致的了解。\n其中，存储部分可以查看ESXi虚拟机可访问的数据内容。\n可以通过使用“数据存储浏览器”查看、下载、上传、下载“存储”中的文件。\n4. 新建和安装虚拟机 # ESXi最主要的功能就是对虚拟机的管理。\nvSphere Client和Web端都有显著的入口供用户创建虚拟机。\n创建虚拟机的流程与使用VMware Workstation创建虚拟机没有太大区别，按照创建向导进行即可。\n在创建过程的“自定义设置”阶段，需要手动配置CD/DVD驱动器，插入桌面上的CentOS或Tiny Core Linux的安装镜像，以使虚拟机在开机时，能自动进入安装镜像的安装引导界面。\n当然，你也可以在虚拟机创建完成后，打开电源之前，手动编辑虚拟机配置，添加对应的镜像文件。\n完成后，打开虚拟机电源，即可进入操作系统的安装引导流程。\n5. 虚拟机文件系统格式和种类 # 添加虚拟机后，可以在“存储”中看到每台虚拟机中包含的文件内容。\n了解这些不同格式的文件和含义和作用。\n6. 虚拟机导出 # 关闭虚拟机电源后，即可将虚拟机导出。\n观察导出文件格式与虚拟机正常文件的区别。\n7. VMware Tools（选做） # 可以在如下图所示的虚拟机控制台的“操作”选项中找到“安装VMware Tools”的选项。\n点击“安装VMware Tools”后，ESXi会给虚拟机挂载一个包含了安装脚本的光盘。根据安装的操作系统的不同，具体的安装方式也有区别。对于Linux系统，可以在/dev目录下找到该光盘，并将其挂载到文件系统中，然后进入其中，执行安装脚本。\n8. 角色和用户（选做） # 可以在Web端创建不同的角色和用户，根据角色的不同，用户将获得不同的权限。尝试使用不同的用户账号登录Client，以体验权限限制带来的差异。\n9. 创建与配置集群（选做） # 后面这两部分实验内容比较复杂，有兴趣的同学可以选择尝试。\n在上面的实验中，我们进行了ESXi的部署，并使用ESXi创建了一些虚拟机。在实际应用中，往往需要多个ESXi主机组成集群，来提供更多的资源，或者提高可用性。在接下来的实验中，我们将使用vCenter Server管理多个ESXi主机，来管理所有的虚拟机和ESXi“物理机”集群。\n在VMware Workstation中新建一个新的ESXi虚拟机。该虚拟机内存可以给大一些，比如14G，磁盘大小也可以大一些，比如100G。以方便后续在其上安装vCenter Server。\n可以在VMware Workstation中给机器开大内存和大容量磁盘。\n因为内存和磁盘是虚拟的，所以即使本机物理内存和磁盘容量不够，也把握得住。\n如果后续实验中，确实出现了内存或磁盘容量不够的情况，请联系助教扩容。\n点击打开vCenter Server安装包VMware-VCSA-all-6.7.0-19832974.iso，可以看到README文件，其中包含着具体的安装指引。之后的安装步骤，如无特殊说明，无需改动默认选项，直接下一步即可。 我们刚刚启动的只是一个安装器程序，vCenter Server本质上还是要安装在一个特定的ESXi主机中。所以，在选择安装目标时，需要填入刚才新建的那个内存和磁盘容量都比较大的ESXi虚拟机的相关信息。 在最后的网络配置这里，选择DHCP 安装完成后，浏览器访问虚拟机地址或者使用vSphere Client均可访问到vCenter。 创建数据中心。\n添加主机。把本次实验创建的两个ESXi主机都添加进去。在添加主机时会弹出安全警示，选择“是”继续添加即可。输入主机的用户名和密码，一路默认即可（一定要禁用锁定模式）。 10. 分布式交换机（选做） # 在两台ESXi主机上分别启动一台Tiny Core Linux虚拟机，使用ifconfig命令查看这两台虚拟机的IP地址，使用ping命令测试两台机器之间的网络是否连通。\n默认情况下，vCenter会在两台主机上创建虚拟交换机，以保证虚拟机之间的网络通信。 如果你的集群出现网络不通的情况，可以查找相关资料，在配置好的集群上，创建分布式虚拟交换机。并分别在不同的主机上部署虚拟机，实现它们之间的网络互通。\n实验报告模板 # # Lab02 虚拟化实验 \u0026gt; 班级： \u0026gt; 学号： \u0026gt; 姓名： --- ## 实验内容 ### 1. 安装VMware ESXi ### 2. 访问ESXi #### ESXi分配许可证 ### 3. 观察和体验vSphere Client提供的功能 ### 4. 新建和安装虚拟机 ### 5. 虚拟机文件系统格式和种类 #### 不同格式的文件和含义和作用 ### 6. 虚拟机导出 #### 导出文件格式与虚拟机正常文件的区别 ### 7. VMware Tools（选做） ### 8. 角色和用户（选做） ### 9. 创建与配置集群（选做） ### 10. 分布式虚拟交换机（选做） "},{"id":2,"href":"/doc/ns-labs/table-of-contents/ceph/","title":"Lab03 Ceph存储集群实践","section":"Table of Contents","content":" Lab03 Ceph 存储集群实践 # 实验目的 # 了解 Ceph 存储的基本工作原理\n建立对分布式存储的初步认识\n实验说明 # 按照实验指南的指导，完成实验。\n按照实验报告模板，撰写实验报告，将重要的实验步骤截图，填入实验报告中，并回答相应问题。\n本次实验由小组内成员分工完成，虚拟机已分发至每组第一位同学的账户中，每组一台虚拟机，实验报告每个人都需要提交，同组的实验报告内容可以一致\n本次实验的分数构成如下：\n必做（7分）： Ceph部署 三选一（3分）： Ceph Filesystem Ceph RGW 对象存储 Ceph RBD 其中，Ceph Filesystem、Ceph RGW、Ceph RBD任选其一完成即可得到“三选一”部分的分数，多做没有额外的加分。如果你想在网络存储课程上投入更多的精力，欢迎参与课程设计申优答辩。\n在实验过程中，在执行每一条命令之前请务必搞清楚它是用来干啥的，执行后会有什么结果。\n实验中遇到的困难请及时在课程微信群中抛出。 除本指导书外，实验的主要的参考资料还包括： Ceph 官方文档、 Ceph 部署指南、互联网上的技术博客等。\n请在云平台作业提交截止时间之前，将作业提交到云平台，命名为：lab03-学号-姓名.pdf的格式。\n概述 # Ceph(读音 /ˈsɛf/) 是一个分布式的存储集群。什么是分布式存储？我们为什么需要它？\n试想，你在搭建了一个网站对外提供服务。用户在使用网站的过程中会存储大量的数据，网站运行过程中也会产生大量的日志信息。\n最初，你将网站部署在一个装有 500G 硬盘的服务器上。随着时间的流逝，500G 的硬盘逐渐被填满。现在你有两种选择。\n纵向拓展。在服务器上加装硬盘，甚至你可以使用 LVM 将硬盘无缝拓展到原来的文件系统中，上层应用和用户根本看不出来有任何差别。但随着数据量的进一步积累，加装的硬盘还会被填满。即使你将服务器的硬盘槽位都插满，最终还是无法解决数据量逐渐增大的问题。数据是无限的，一台机器能承受的数据量总是有限的，氪金也无法解决这个问题。\n横向拓展。买一台新的服务器，用网线把它和原来的服务器连起来，把原来的服务器存不下的数据存储到这台新的服务器上。当需要使用到这些数据时，再从新的服务器上取出来。当第二台服务器被填满后，再添加新的服务器。\n第二种看起来是最可行的方法：随着业务的扩展，继续加机器就可以了。这种由多台网络互通的机器组成的存储系统即可被理解为“分布式存储系统”。\n但随着机器数量的增加，整个系统的复杂度也在上升。新的多机器系统会表现出与原来的单机系统很多不同的特性，会带来更多的问题，比如：\n如何划分数据？也就是说，如何决定网站接收的某份数据该存储到哪台机器上？每台机器的存储容量可能不同，存储性能也可能不同，如何平衡每台机器的存储容量？\n如何获取数据？我们将数据保存在不同的机器上时，通常保存的不是一个完整的文件，而是经过一个个切分后的数据块，每个数据块可能保存在不同的机器上。当获取数据时，我们需要知道要获取的文件包含哪些数据块，每个数据块存放在哪台机器的哪个位置。随着机器数量和数据量的增加，这不是一个简单的任务。\n随着机器数量的增加，系统发生故障的概率也在增加。仅对硬盘而言，我们假设每块硬盘在一年中发生故障的概率是 1%，对于普通消费者而言，这似乎不是什么问题，这种故障可能在硬盘的整个使用周期内都不会发生；但对于一个包含几百块硬盘的存储系统来说，这意味着几乎每天都会有若干块硬盘发生故障，而每块硬盘的故障都有可能造成系统的宕机和数据损失。因此，分布式存储系统必须有较强的容错能力，能够在一定数量的机器崩溃时，仍能对外提供服务。\n……\n上面这些问题，正是 Ceph 这类分布式存储系统所要解决的问题。简单来说，Ceph 是一个能将大量廉价的存储设备统一组织起来，并对外提供统一的服务接口的，提供分布式、横向拓展、高度可靠性的存储系统。\n对分布式系统感兴趣的同学，可以趁下学期或大四空闲的时候听一下 MIT 6.824的课程，并尽量完成它的全部实验。\n在互联网上搜索“MIT 6.824”能得到大量的资料，比如，B 站上有 翻译好的熟肉。\n除此之外，Ceph 的独特之处还在于，它在一个存储系统上，对外提供了三种类型的访问接口：\n文件存储。简单来说，你可以将 Ceph 的存储池抽象为一个文件系统，并挂载到某个目录上，然后像读写本地文件一样，在这个新的目录上创建、读写、删除文件。并且该文件系统可以同时被多台机器同时挂载，并被同时读写。从而实现多台机器间的存储共享。\n对象存储。Ceph 提供了对象存储网关，并同时提供了 S3 和 Swift 风格的 API 接口。你可以使用这些接口上传和下载文件。\n块存储。Ceph 还能提供块存储的抽象。即客户端（集群外的机器）通过块存储接口访问的“所有数据按照固定的大小分块，每一块赋予一个用于寻址的编号。”客户端可以像使用硬盘这种块设备一样，使用这些块存储的接口进行数据的读写。（一般这种块设备的读写都是由操作系统代劳的。操作系统会对块设备进行分区等操作，并在其上部署文件系统，应用程序和用户看到直接看到的是文件系统的接口（也就是文件存储））。\n需要注意的是，虽然 Ceph 对外提供了上面这三种不同类型的存储接口，但其底层会使用相同的逻辑对接收的数据进行分块和存储。\n重要概念 # 一个 Ceph 集群必须包含三种类型的进程：Monitor、OSD 和 Manager。其中，Monitor 和 OSD 是最核心的两类进程。\nMonitor 和 OSD # Monitor 进程负责维护整个系统的状态信息，这些状态信息包括当前的 Ceph 集群的拓扑结构等，这些信息对 Ceph 集群中各个进程的通信来说非常关键。除此之外，Monitor 进程还负责充当外界（也就是官方文档中总是提到的“Client”）与 OSD 进程交流的媒介。\nOSD 进程则负责进行真正的数据存储。如下图所示，外界传送给 Ceph 集群的数据（不管是通过文件存储、对象存储还是块存储的接口）都将被转化为一个个对象（object）。这些 object 将经由 OSD 进程存储到磁盘中。\n简单来说，当一个 Client 试图向 Ceph 集群读写数据时，将发生以下步骤：\nClient 向 Monitor 进程请求一个 token 校验信息\nMonitor 生成 token 校验信息，并将其返回给 Client\nMonitor 同时会将 token 校验信息同步 OSD 进程\nClient 携带着 Monitor 返回的 token 校验信息向对应的 token 发送数据读写请求\nOSD 进程将数据存储到合适的位置，或从合适的位置读出数据\nOSD 进程向 Client 返回数据\n以上读写数据的流程是经过极致简化的，主要是为了帮助大家建立对 Monitor 进程和 OSD 进程所起的作用的感性认识。\n想要了解详情，请阅读 Ceph 的文档 - Architecture。\nManager # Manager 进程主要负责跟踪当前集群的运行时状况，包括当前集群的存储利用率、存储性能等等。同时，它还负责提供 Ceph Dashboard、RESTful 接口的外部服务。\n需要注意的是，Ceph 集群中有三类这样的进程，但不是每个进程只有一个。\n我们之前提到过，Ceph 是一个有很高容错性的分布式系统，而达到高容错性的一个很重要的方式就是“冗余”。\n比如，对于 Monitor 进程来讲，集群中仅有一个就够用了。但如果运行这一个 Monitor 进程的机器挂了，那么整个集群就会瘫痪（Client 将不知道该跟谁通信来拿到校验信息和集群状态信息等）。因此，一个高可用的 Ceph 集群中会包含多个执行几乎相同任务的运行在不同机器上的 Monitor 进程；这样挂了一个，Client 还可以跟剩下的通信，整个集群依旧可以正常对外提供服务。同样的道理，OSD 进程和 Manager 进程也有多个副本。\n另外，Ceph 为了保证数据的可靠性（也就是说 Client 存储进来的数据不能丢失）——注意区分其与整个系统可靠性的区别——在默认情况下，会将每份数据存储3 份，每份都会存储在不同的 OSD 上（鸡蛋不能放到同一个篮子里）。这样，即使有部分 OSD 挂掉，也能保证大部分数据不会丢失。因此，一个健康的 Ceph 集群要求至少同时存在三个健康的 OSD 进程（当然，这个默认的数值可以更改）。\nPool 与 Placement Group (PG) 与 Placement Group for Placement purpose (PGP) # 请查阅 Ceph 的相关文档，阐述 Pool、PG、PGP 与 OSD 之间的关系。可定性/定量分析 OSD、PG、PGP、OSD、PG_NUM 之间的数量关系。\nCeph 集群的 HEALTH STATUS # 你在实验过程中，打 ceph -s 都遇到过哪些 HEALTH STATUS？（ceph health detail 能看到更为详细的状态信息）如果是 WARN/ERROR 都是哪些原因？\nPG_NUM 的相关计算 # 请查阅 Ceph 的相关文档。\n实验环境介绍 # 本次实验发给大家了三台Centos 7虚拟机。它们的命名格式为ceph-\u0026lt;序号\u0026gt;-\u0026lt;学号\u0026gt;，如ceph-01-20210000。\n关于虚拟机的连接、文件传输以及连接互联网功能，可以参阅 虚拟机使用说明\n在实验开始前，你需要保证这三台虚拟机处于开机状态、用buaalogin连接互联网，并且设置它们的主机名与名称一致。\n例如在ceph-03-20210000机器上，你需要执行（设置后必须重启生效）\necho \u0026#39;ceph-03-20210000\u0026#39; \u0026gt; /etc/hostname reboot 注意每台机器的主机名不能相同。\nCeph 部署 # 本节内容的目标是创建一个可用的 Ceph 集群。其中包括，至少一个 Monitor 进程、至少一个 Manager 进程、至少三个 OSD 进程。\nCeph 官方提供了 多种部署方式。\n在本实验文档中，我们采用 Cephadm作为部署工具。Cephadm 也是官方推荐的部署和管理 Ceph 集群的工具，它不仅可以用来部署 Ceph，还可以在安装完成后，用来管理集群（添加和移除节点、开启 rgw 等）。\nCephadm介绍 # Cephadm 是基于“容器技术（Container）”进行工作的，每个 Ceph 的工作进程都运行在相互隔离的容器中。Cephadm 支持使用 Docker 和 Podman 作为容器运行时。在部署时，Cephadm 将首先检查本机中安装的容器运行时类型，当 Docker 与 Podman 并存时，将首先使用 Podman（毕竟 Podman 也是 Red Hat 的产品）。\n在部署时，Cephadm 会首先在本地启动一个 mini 的 Ceph 集群，其中包括 Ceph 集群最基本的 Monitor 进程和 Manager 进程（当然，这两个进程都是通过容器形式运行起来的）。这个 mini 集群在某种程度上来说也是合法的，只不过其基本不能对外提供任何功能。随后，我们将继续使用 Cephadm 提供的工具，将其他机器（后文中也会称之为“节点”）加入到集群中（也就是在其他节点中启动 Ceph 的 Monitor、Manager、OSD 等进程），从而构建一个完整可用的集群。\n下面是 Red Hat 给出使用 Cephadm 构建的集群的架构图。\n图中的“Container”指的就是我们上面所说的“容器”。\n图中最左边的 Bootstrap Host 就是我们执行 Cephadm 相关命令的机器（事实上，在整个集群的构建过程中，除了修改 IP 和联网等操作外，我们都只会在这个 Bootstrap Host 上进行操作）。我们在 Bootstrap Host 执行的针对其他节点的操作，都是 Cephadm 通过 ssh 的方式发送给对应节点的。\n这个图目前大家心里有个印象就好，在后续的实验操作中，可以反复回来对照检查。\n给大家提供的机器上已经安装好Cephadm，大家无需自行安装\n在后续操作前，请务必保证所有机器已经连接互联网。\n基于 Cephadm 的便捷性，后续的操作只需要在选定的一台机器（我们把这台机器称为 Bootstrap Host）上执行即可。\nBootStrap # 我们首先需要在一台选定的机器上，使用cephadm启动一个 mini 集群。\ncephadm --image scs.buaa.edu.cn:8081/library/ceph:v16 bootstrap --mon-ip *\u0026lt;mon-ip\u0026gt;* 请将*\u0026lt;mon-ip\u0026gt;*替换为你执行这命令的机器的 IP。如：\ncephadm --image scs.buaa.edu.cn:8081/library/ceph:v16 bootstrap --mon-ip 10.251.252.182 上面这条命令中，--image制定了 Cephadm 启动容器时使用的镜像名称，--mon-ip指定了 Cephadm 要在哪个机器上启动一个 mini 集群。\n更详细地，这条命令将会做如下事情：\nCreate a monitor and manager daemon for the new cluster on the local host.\nGenerate a new SSH key for the Ceph cluster and add it to the root user\u0026rsquo;s /root/.ssh/authorized_keys file.\nWrite a copy of the public key to /etc/ceph/ceph.pub.\nWrite a minimal configuration file to /etc/ceph/ceph.conf. This file is needed to communicate with the new cluster.\nWrite a copy of the client.admin administrative (privileged!) secret key to /etc/ceph/ceph.client.admin.keyring.\nAdd the _admin label to the bootstrap host. By default, any host with this label will (also) get a copy of /etc/ceph/ceph.conf and /etc/ceph/ceph.client.admin.keyring.\n命令执行完成后，我们可以通过ceph -s查看当前集群的状态。\n可以看到，确实启动了一个 Monitor 进程和一个 Manager 进程。\n另外，我们还注意到，当前集群的健康状态是HEALTH_WARN，原因下面也列出来了：OSD count 0 \u0026lt; osd_pool_default_size 3。这是因为当前 Ceph 集群默认的每个 Pool 的副本数应该是 3（即，Ceph 中存储的每份数据必须复制 3 份，放在 3 个不同的 OSD 中），但我们 OSD 的进程数是 0。不用着急，马上我们就会创建足够的 OSD 进程（Bootstrap Host、另外两台主机都为他们创建 OSD 进程）。\nmini 集群启动完成后，可以使用docker ps -a查看当前启动的容器的状态：\n可以看到，最下面这两个容器，对应的就是 ceph 集群的 Monitor 进程和 Manager 进程。\nCeph Dashboard # 注意看bootstrap指令的输出，你可以看到一段这样的内容：\n显然，这是在告诉我们，cephadm 同样启动了一个Ceph Dashboard，这是一个 Ceph 的管理前端。通过访问这个页面，我们就可以以可视化的方式观察到当前集群的状态。\n在内网环境中，签发 SSL 证书的过程太过繁琐，我们可以手动禁用 SSL：\nceph config set mgr mgr/dashboard/ssl false 禁用 SSL 后，Dashboard 服务将默认监听 8080 端口。但在 CentOS 中，8080 端口默认是被防火墙屏蔽的。\n你可以选择手动打开防火墙的 8080 端口；也可以像下面这样，将 Dashboard 服务的监听端口手动改为 8443（因为这个端口就是使用 HTTPS 时 Dashboard 的监听端口，在刚才的 Bootstrap 时已经在防火墙中打开了）：\nceph config set mgr mgr/dashboard/server_port 8443 然后，重启该 Dashboard 服务，使刚才的配置生效。\nceph mgr module disable dashboard ceph mgr module enable dashboard 然后，查看当前的服务状态（如果输出为空的话，耐心多等一会儿）：\nceph mgr services 访问\u0026quot;dashboard\u0026quot;后面的网址，如果它仍为https开头，则需要手动改成http开头。\n现在，你应该可以正常访问 Dashboard 服务了。注意，用户名和密码是我们前面提到的 Bootstrap 命令输出的那堆信息中提到的。\n如果你忘记了密码，则可以用以下方式将密码重置为@buaa21\necho \u0026#39;@buaa21\u0026#39; \u0026gt; passwd.txt ceph dashboard set-login-credentials admin -i passwd.txt 添加其他节点 # 接下来，我们将把其他机器添加到现有的这个 mini 集群中来。\n前面提到过，cephadm 是通过 ssh 协议与其他机器通信的。所以，这里需要首先把 Bootstrap Host 机器的公钥 copy 到其他的所有机器：\nssh-copy-id -f -i /etc/ceph/ceph.pub root@*\u0026lt;new-host\u0026gt;* 例如，你的另一台机器的 IP 是10.251.252.177，那么这条命令应该是：\nssh-copy-id -f -i /etc/ceph/ceph.pub root@10.251.252.177 处理完所有的机器后，就可以正式将它们加入到 mini 集群中来了：\nceph orch host add *\u0026lt;newhost\u0026gt;* [*\u0026lt;ip\u0026gt;*] [*\u0026lt;label1\u0026gt; ...*] 例如，你另一台机器的主机名是ceph-02-20210000，IP 是10.251.252.177，那么这条命令应该是：\nceph orch host add ceph-02-20210000 10.251.252.177 添加完成后，你可以通过ceph -s查看当前集群状态的变化。也可以通过 Ceph Dashboard 看到变化。\n创建 OSD 进程 # 我们知道，OSD 进程是真正用来做数据读写的进程。我们可以用一块专门的磁盘交给 OSD 进程来读写数据，ceph 集群所存储的数据就将保存在这些磁盘中。\n这些被用来交给 OSD 进程管理的磁盘，应该满足以下条件：\nThe device must have no partitions.\nThe device must not have any LVM state.\nThe device must not be mounted.\nThe device must not contain a file system.\nThe device must not contain a Ceph BlueStore OSD.\nThe device must be larger than 5 GB.\n简单来说，就是将一块干净的磁盘插入机器后，什么都不用做就好。\n在实验提供的虚拟机中，每台机器都额外插入了一块这样干净的磁盘。\n可以通过fdisk -l来查看：\n注意看上面这两块磁盘：\n第一个名称是/dev/sda，容量是 16G，有两个分区：/dev/sda1，/dev/sda2。这就是我们现在在使用的这个系统所用的磁盘，系统数据都存储在这个磁盘中。\n第二个名称是/dev/sdb，容量是 10G，没有任何分区。这就是我们即将交给 OSD 管理的磁盘。\n使用下面的命令来创建 OSD 进程：\nceph orch daemon add osd *\u0026lt;hostname\u0026gt;*:*\u0026lt;device-name\u0026gt;* 比如，你要在主机ceph-01-20210000的名称为/dev/sdb的磁盘上创建 OSD 进程，那么命令应该是：\nceph orch daemon add osd ceph-01-20210000:/dev/sdb 这条命令默认不会有输出创建 OSD 进程的详细信息，也就是说，如果该命令很耗时的话，那么你将在什么输出都没有的情况下等待较长时间，这可能令人发慌。你可以加上--verbose参数，来让它输出详细信息。\n比如：\nceph orch daemon add osd ceph-01:/dev/sdb --verbose 执行完成后，可以使用ceph -s查看当前的集群状态，可以发现已经有一个 OSD 进程加入了进来。\n使用同样的方法，将所有节点的附加硬盘都加入进来。至此，我们可以再使用ceph -s查看当前集群的状态。\nCeph Filesystem (选) # 参考资料: CEPH FILE SYSTEM\nCeph 文件系统 (Ceph FS)是个 POSIX 兼容的文件系统，它使用 Ceph 存储集群来存储数据。 Ceph 文件系统与 Ceph 块设备、对象存储或者原生库 (librados) 一样，都使用着相同的 Ceph 存储集群系统。\nCeph 文件系统要求 Ceph 存储集群内至少有一个 Ceph 元数据服务器 MDS。\n部署 CephFS # 以上其实都是在搭建 Ceph 集群的环境，我们添加了 3 个 OSD 进程组建起了 Ceph Cluster。接下来，我们便可以在此基础上来具体地使用到 Ceph 所提供的分布式存储能力，从 Ceph 文件系统 CephFS 开始~\n查看各个节点的主机名\nceph orch host ls 输入类似如下\n# 可有类似输出 HOST ADDR LABELS STATUS host1 10.1.2.3 host2 10.1.2.4 host3 10.1.2.5 Automatic Setup 创建 CephFS 的前提是需要至少一个的 MDS daemon 元数据服务器守护进程。其实以下的一条命令即可自动地创建好 MDS daemon、Pool 等：\nceph fs volume create \u0026lt;fs_name\u0026gt; [--placement=\u0026#34;\u0026lt;placement spec\u0026gt;\u0026#34;] 其中，fs_name 是 CephFS 的名称，后面的 --placement 为可选参数，可以通过它来指定 daemon container 跑在哪几个 hosts 上（ 参考资料 ）。例如：\nceph fs volume create ceph_fs --placement=\u0026#34;ceph-01-20210000,ceph-02-20210000,ceph-03-20210000\u0026#34; More Customized Setup 参考：下列命令的一些具体参数含义\nceph osd pool create cephfs_data 8 8 # 后面的数量可以调，设大了会无法创建，数值和osd的数量有关，需要是2的倍数 # pool \u0026#39;cephfs_data\u0026#39; created ceph osd pool create cephfs_metadata 8 8 # pool \u0026#39;cephfs_metadata\u0026#39; created ceph fs new cephfs cephfs_metadata cephfs_data # new fs with metadata pool 3 and data pool 2 ceph fs ls # name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ] ceph orch apply mds cephfs --placement=\u0026#34;3 node1 node2 node3\u0026#34; # 应用部署CephFS 于是，我们便创建成功了 CephFS。\n你可能会想到如何删除 CephFS，Ceph 中非常“贴心”地防止你误删除，所以删除起来会有一些麻烦。\nceph fs volume rm \u0026lt;fs_name\u0026gt; [--yes-i-really-mean-it] # 要加上这么一长串后缀，但这样其实还是没法删除 ceph config set mon mon_allow_pool_delete true # 还需要通过这条命令修改ceph config配置 通过 rados df 命令可查看刚才创建的资源池 Pool 的相关信息。 ceph fs ls 可列出 CephFS。 ceph fs status 可查看 CephFS 状态，验证当前已有至少一个 MDS 处在 Active 状态。 还可经常性地执行 ceph -s 查看 Ceph 集群的状态。 挂载 CephFS # 参考资料：MOUNT CEPHFS USING FUSE\nCephFS 在创建后应当能被实际使用，如完成分布式存储文件的任务。在这一步，我们将把 CephFS 挂载到 Client 端，让 Client 能够创建和存储文件。你需要选择除 Bootstrap Host 之外的任意一台机器作为 Client 端。\n我们先要对 Client 端进行一些配置，保证 Client 端能连接到 MON 主机，即 Bootstrap Host。\n第一步：给 Client 端创建一个最小配置文件，放置在 /etc/ceph 目录下：\n这一步操作高危！\n以防万一，请先执行两个操作：\n在两台机器上执行cat /etc/ceph/ceph.conf \u0026gt; /etc/ceph/backup_ceph.conf 备份原来的 ceph.conf 在 cephadm shell (即 bootstarp 主机) 里面，用 ceph config generate-minimal-conf 生成 config，将生成出来的内容也保存备份一下 (复制粘贴+截图大法 / 重定向输出) 在 Client 端执行（将{user}替换成root，{mon-host}替换成 Bootstrap Host 的 IP 地址）\n# on client host mkdir /etc/ceph ssh {user}@{mon-host} \u0026#34;sudo ceph config generate-minimal-conf\u0026#34; | sudo tee /etc/ceph/ceph.conf chmod 644 /etc/ceph/ceph.conf # 赋权 如果不能成功，可直接到 MON 主机执行 sudo ceph config generate-minimal-conf，将输出的内容粘贴到 /etc/ceph/ceph.conf（下同）。如果上述操作导致 Client/Bootstrap Host 挂了，多半是 /etc/ceph/ceph.conf 被误清空了，将先前备份的 ceph.conf 写回即可恢复。\n第二步：生成 CephX 用户名和密钥（将{user}替换成root，{mon-host}替换成 Bootstrap Host 的 IP 地址）:\n# on client host ssh {user}@{mon-host} \u0026#34;sudo ceph fs authorize ceph_fs client.foo / rw\u0026#34; | sudo tee /etc/ceph/ceph.keyring chmod 600 /etc/ceph/ceph.keyring # 赋权 在上述命令中，ceph_fs 是先前所创建的 CephFS 的名称，请将其替换。foo 是 CephX 的用户名，也可自己起。\n以上是前置准备，完成后，我们可通过 ceph-fuse 工具实现文件挂接。如机器上没有，则需要连网安装一下。\nyum install -y ceph-fuse 安装完成后，我们可以创建一个被挂接的目录，如 mycephfs：mkdir /mnt/mycephfs\n执行 ceph-fuse --id foo -m {mon-host}:6789 /mnt/mycephfs 即可完成挂接，如ceph-fuse --id foo -m 10.251.252.182:6789 /mnt/mycephfs，如果此命令不能成功运行，可从 参考资料：MOUNT CEPHFS USING FUSE试一下其他的命令\n若想取消挂接非常简单，只需 umount /mnt/mycephfs。\n如何判断挂接成功？\n上述命令不报错是一方面，我们也可以通过一些命令来看挂接的情况。\nlsblk 列出所有可用块设备的信息，还能显示他们之间的依赖关系 df -h 查看磁盘占用的空间 通过这些命令，应能看到挂接盘 mycephfs 的存在，查看到其容量等信息。\n文件系统 容量 已用 可用 已用% 挂载点 ceph-fuse 9.4G 0 9.4G 0% /mnt/mycephfs Ceph RGW 对象存储 (选) # Ceph RGW(即 RADOS Gateway)是 Ceph 对象存储网关服务，是基于 LIBRADOS 接口封装实现的 FastCGI 服务，对外提供存储和管理对象数据的 Restful API。对象存储适用于图片、视频等各类文件的上传下载，可以设置相应的访问权限。目前 Ceph RGW 兼容常见的对象存储 API，例如兼容绝大部分 Amazon S3 API，兼容 OpenStack Swift API。\n通俗理解是 RGW 作为一个协议转换层，把从上层应用符合 S3 或 Swift 协议的请求转换成 rados 的请求，将数据保存在 rados 集群中。\n内部概念\nzone：包含多个 RGW 实例的一个逻辑概念。zone 不能跨集群，同一个 zone 的数据保存在同一组 pool 中。 zonegroup：一个 zonegroup 如果包含 1 个或多个 zone。如果一个 zonegroup 包含多个 zone，必须指定一个 zone 作为 master zone，用来处理 bucket 和用户的创建。一个集群可以创建多个 zonegroup，一个 zonegroup 也可以跨多个集群。 realm：一个 realm 包含 1 个或多个 zonegroup。如果 realm 包含多个 zonegroup，必须指定一个 zonegroup 为 master zonegroup， 用来处理系统操作。一个系统中可以包含多个 realm，多个 realm 之间资源完全隔离。 外部概念\nuser：对象存储的使用者，默认情况下，一个用户只能创建 1000 个存储桶。 bucket：存储桶，用来管理对象的容器。 object：对象，泛指一个文档、图片或视频文件等，尽管用户可以直接上传一个目录，但是 ceph 并不按目录层级结构保存对象， ceph 所有的对象扁平化的保存在 bucket 中。 参考阅读\nDeploy RGW # 参考：RGW SERVICE\nTo deploy a set of radosgw daemons, with an arbitrary service name name, run the following command: ceph orch apply rgw *\u0026lt;name\u0026gt;* [--realm=*\u0026lt;realm-name\u0026gt;*] [--zone=*\u0026lt;zone-name\u0026gt;*] --placement=\u0026#34;*\u0026lt;num-daemons\u0026gt;* [*\u0026lt;host1\u0026gt;* ...]\u0026#34; 其中，[]中的内容为可选项，可以都添上。如 ceph orch apply rgw ceph_rgw --realm=default --zone=default --placement=3。--placement参数的使用和先前实验也是类似的，还可以用 3 node1 node2 node3 来完成指定。\n其实这一条命令就够了，然后我们可查看各个 rgw 节点是否已启动：ceph orch ps --daemon-type rgw。\n应能看到 rgw* 均为 running 的 STATUS，则表明顺利启动。若为 starting 可稍等其转为 running。\n如一直显示 unknown/error 的 STATUS，也是因为 pg 资源不足（受 osd 数量所限）导致的，可以将前面创建的 CephFS 删掉，释放资源。\n删除刚刚创建的 unknown/error 的 rgw 的命令为ceph orch rm rgw.*\u0026lt;rgw_name\u0026gt;*\n在执行上述命令的 Bootstrap host，curl \u0026lt;bootstrap_host_ip:80\u0026gt; 应能看到包含了 \u0026lt;Buckets/\u0026gt; 的 XML 形式的输出。\n使用对象存储 # 我们为 rgw 创建用户：radosgw-admin user create --uid=\u0026lt;username\u0026gt; --display-name=\u0026lt;your_display_name\u0026gt; --system。如 radosgw-admin user create --uid=s3 --display-name=\u0026quot;object_storage\u0026quot; --system。执行后，能看到类似输出：\n{ \u0026#34;user_id\u0026#34;: \u0026#34;s3\u0026#34;, \u0026#34;display_name\u0026#34;: \u0026#34;objcet_storage\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;suspended\u0026#34;: 0, \u0026#34;max_buckets\u0026#34;: 1000, \u0026#34;subusers\u0026#34;: [], \u0026#34;keys\u0026#34;: [ { \u0026#34;user\u0026#34;: \u0026#34;x\u0026#34;, \u0026#34;access_key\u0026#34;: \u0026#34;xxxxxxxxkey01\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;xxxxxxxxkey01\u0026#34; } ], \u0026#34;swift_keys\u0026#34;: [], \u0026#34;caps\u0026#34;: [], \u0026#34;op_mask\u0026#34;: \u0026#34;read, write, delete\u0026#34;, \u0026#34;system\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;default_placement\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;default_storage_class\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;placement_tags\u0026#34;: [], \u0026#34;bucket_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;user_quota\u0026#34;: { \u0026#34;enabled\u0026#34;: false, \u0026#34;check_on_raw\u0026#34;: false, \u0026#34;max_size\u0026#34;: -1, \u0026#34;max_size_kb\u0026#34;: 0, \u0026#34;max_objects\u0026#34;: -1 }, \u0026#34;temp_url_keys\u0026#34;: [], \u0026#34;type\u0026#34;: \u0026#34;rgw\u0026#34;, \u0026#34;mfa_ids\u0026#34;: [] } 从命令行的输出中，可以看到 access_key: xxxxxxxxkey01 和 secret_key: xxxxxxxxkey01，我们将其截图或复制保存下来，后面还要用。\n使用 Ceph 的 RGW 对象存储，可以有很多工具，如 s3cmd、 minio-client 等，这里我们以 s3cmd 为例。有兴趣的还可以尝试 minio可以可视化进行操作。\n安装 s3cmd\nyum -y install s3cmd 我们执行 s3cmd --configure 生成一个配置文件。\n\u0026gt; Access Key: \u0026#34;xxxxxxxxkey01\u0026#34; \u0026gt; Secret Key: \u0026#34;xxxxxxxxkey02\u0026#34; （没有写到的直接按回车） \u0026gt; S3 Endpoint [s3.amazonaws.com]: \u0026#34;\u0026lt;bootstrap_host_ip:80\u0026gt;\u0026#34;，如 \u0026#34;10.1.1.2:80\u0026#34; \u0026gt; DNS-style bucket....[%(bucket)s.s3.amazonaws.com]: \u0026#34;\u0026lt;bootstrap_host_ip:80\u0026gt;/%(bucket)s\u0026#34;，如\u0026#34;10.1.1.2:80/%(bucket)s\u0026#34; \u0026gt; Use HTTPS protocol [Yes]: no （其余继续按回车） \u0026gt; Save settings? [y/N] y Configuration saved to \u0026#39;/root/.s3cfg\u0026#39; 进入到刚刚保存新建的 config：/root/.s3cfg中，继续修改signature_v2为True，可以自行检查其它选项是否填写正确\n[default] ... signature_v2 = True ... 查看 Bucket：\ns3cmd ls 一开始没有创建过 Bucket，故没有输出，我们来新建一个 s3cmd mb s3://s3cmd-demo，再执行 s3cmd ls，即可看到新创建的 bucket。\n参考资料：使用 s3cmd\n参考以上资料，可以尝试继续上传文件、上传文件夹、下载、ls 、删除等命令，体验 ceph-rgw 的 Bucket 与 S3 存储的交互。\n在 Dashboard 里有丰富的信息，可以多多尝试。如查看 rgw 的用户、查看 Bucket、Pool 等，欢迎多多体验。\n如上传文件不成功，有提示：ERROR: S3 error: 416 (InvalidRange) 的错误，同样也可释放掉一些先前创建的资源，如 drop 掉 CephFS 再试试，Dashboard 里也能删除 Pool。OSD 只有 3 个，导致 PG 数量吃紧可能不够用。\nCeph RBD (选) # 参考：CEPH BLOCK DEVICE\nRBD 即 RADOS Block Device 的简称，RBD 块存储是最稳定且最常用的存储类型。RBD 块设备类似磁盘可以被挂载。RBD 块设备具有快照、多副本、克隆和一致性等特性，数据以条带化的方式存储在 Ceph 集群的多个 OSD 中。如下是对 Ceph RBD 的理解：\nRBD 就是 Ceph 里的块设备，一个 4T 的块设备的功能和一个 4T 的 SATA 类似，挂载的 RBD 就可以当磁盘用； resizable：这个块可大可小； data striped：这个块在 Ceph 里面是被切割成若干小块来保存，不然 1PB 的块怎么存的下； thin-provisioned：精简置备，1TB 的集群是能创建无数 1PB 的块的。其实就是块的大小和在 Ceph 中实际占用大小是没有关系的，刚创建出来的块是不占空间，今后用多大空间，才会在 Ceph 中占用多大空间。举例：你有一个 32G 的 U 盘，存了一个 2G 的电影，那么 RBD 大小就类似于 32G，而 2G 就相当于在 Ceph 中占用的空间； 块存储本质就是将裸磁盘或类似裸磁盘(lvm)设备映射给主机使用，主机可以对其进行格式化并存储和读取数据，块设备读取速度快但是不支持共享。\nCeph 可以通过内核模块和 librbd 库提供块设备支持。客户端可以通过内核模块挂在 rbd 使用，客户端使用 rbd 块设备就像使用普通硬盘一样，可以对其就行格式化然后使用；客户应用也可以通过 librbd 使用 ceph 块，典型的是云平台的块存储服务（如下图），云平台可以使用 rbd 作为云的存储后端提供镜像存储、volume 块或者客户的系统引导盘等。\n创建 RBD ceph osd pool create rbd 8 # 值调小些，因为云平台资源有限，3个OSD的PG数默认是有上限的 application enable RBD ceph osd pool application enable rbd rbd 创建 rbd 存储, 指定大小为 1GB rbd create rbd1 --size 1024 查看 rbd 信息 \u0026gt; rbd --image rbd1 info rbd image \u0026#39;rbd1\u0026#39;: size 1 GiB in 256 objects order 22 (4 MiB objects) snapshot_count: 0 id: ace58352cf47 block_name_prefix: rbd_data.ace58352cf47 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Wed Dec 1 03:55:11 2021 access_timestamp: Wed Dec 1 03:55:11 2021 modify_timestamp: Wed Dec 1 03:55:11 2021 继续执行以下命令：\n\u0026gt; root@ceph:/mnt# ceph osd crush tunables hammer adjusted tunables profile to hammer \u0026gt; root@ceph:/mnt# ceph osd crush reweight-all reweighted crush hierarchy # 关闭一些内核默认不支持的特性 \u0026gt; root@ceph:/mnt# rbd feature disable rbd1 exclusive-lock object-map fast-diff deep-flatten # 查看特性是否已禁用 \u0026gt; root@ceph:/mnt# rbd --image rbd1 info | grep features features: layering op_features: # 映射到客户端(在需要挂载的客户端运行) \u0026gt; root@ceph:/mnt# rbd map --image rbd1 /dev/rbd0 # 查看映射情况 \u0026gt; root@ceph:/mnt# rbd showmapped id pool namespace image snap device 0 rbd rbd1 - /dev/rbd0 再继续！\n# 格式化磁盘 \u0026gt; root@ceph:/mnt# mkfs.xfs /dev/rbd0 meta-data=/dev/rbd0 isize=512 agcount=8, agsize=32768 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 data = bsize=4096 blocks=262144, imaxpct=25 = sunit=16 swidth=16 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=16 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 # 创建挂载目录, 并将 rbd 挂载到指定目录 \u0026gt; root@ceph:/mnt# mkdir /mnt/rbd \u0026gt; root@ceph:/mnt# mount /dev/rbd0 /mnt/rbd/ # 查看挂载情况 \u0026gt; root@ceph:/mnt# df -hl | grep rbd /dev/rbd0 1014M 40M 975M 4% /mnt/rbd 和 CephFS 类似，我们同样可在挂载的目录中创建修改文件，感受 Ceph 的能力——如分布式存储的容错啊，存储共享等。 实验报告模板 # # Lab03 Ceph 存储实践 \u0026gt; 班级： \u0026gt; 学号： \u0026gt; 姓名： --- ## 基本概念思考 回答下列问题： ### 请阐述Placement Group (PG) 与 Placement Group for Placement purpose (PGP)的区别和关系 ### Ceph 集群的 HEALTH STATUS ## Ceph 部署 \u0026lt;!-- 本部分内容可以根据部署方式的不同进行不同的改动，这里是使用Cephadm部署的模板 --\u0026gt; ### 实验前置准备 ### Bootstrap \u0026lt;!-- 截图查看ceph -s的输出 --\u0026gt; ### Ceph Dashboard \u0026lt;!-- 截图查看Ceph Dashboard登录后的界面 --\u0026gt; ### 添加其他节点并创建 OSD 进程 \u0026lt;!-- 截图查看ceph -s的输出 --\u0026gt; ## Ceph Filesystem (选) ### 部署 CephFS (选) \u0026lt;!-- 截图查看ceph fs status的输出 --\u0026gt; ### 挂载 CephFS (选) \u0026lt;!-- 截图查看df -h和文件读写命令的输出 --\u0026gt; ## Ceph RGW 对象存储 (选) ### Deploy RGW (选) \u0026lt;!-- 截图查看radosgw-admin user create的输出 --\u0026gt; ### 使用对象存储 (选) \u0026lt;!-- 截图查看上传文件、上传文件夹、下载、ls、删除等命令的输出 --\u0026gt; ## Ceph RBD (选) \u0026lt;!-- 截图查看rbd --image rbd1 info的输出 --\u0026gt; \u0026lt;!-- 截图查看df -hl | grep rbd和文件读写命令的输出 --\u0026gt; ## 自行扩展和设计内容 (选) "},{"id":3,"href":"/doc/cloud-labs/cloud/container_docker/","title":"容器与Docker综合实验","section":"云计算实验","content":" 容器与Docker综合实验 # 💡 文档中对各个概念的定义和阐述并不是严谨的，文档中用语尽量通俗易懂，大多数只是为了让同学们明白这个概念大致在说什么东西，能在头脑中有个感性的认识（这对知识的掌握和学习非常重要）。概念的严格定义还请参考对应的官方文档。 💡 容器和Docker的内容非常繁杂，实验文档不可能面面俱到，因此很多内容以链接的形式给出。请同学们在阅读文档本身的同时，不要忘记学习链接中指出的内容。 💡 同学们在执行命令时，一定要认真阅读命令的输出和log，通过阅读这些输出，你很容易了解你所执行的命令具体执行了哪些操作，这非常有助于理解其背后的运行原理。 💡 目前流行的绝大多数容器运行时都是用Go语言编写，著名的容器编排工具Kubernetes也是用Go语言编写的，基于Docker和Kubernetes建立起来的整个云计算生态中的绝大部分项目也都是用Go语言实现的。因此，如果想深入了解和学习云计算相关内容的话，建议同学们学习和掌握Go语言。当然，这并不是本次实验和这门课的要求:) 注意事项\n本次分配的机器的账户和密码为： buaa: \u0026amp;shieshuyuan21 务必首先修改机器的root和buaa账户的密码\n请务必阅读 虚拟机使用说明。\n分配的虚拟机中，已经安装了Docker，无需重复安装；并设置了Docker镜像地址（该地址指向校内地址），理论上docker.io中的镜像不用联网即可拉取。例如可以直接在虚拟机上docker pull nginx。\n实验目的 # 理解容器的概念，了解实现容器所使用的的底层技术，理解容器与虚拟机的区别\n理解容器与Docker之间的关系\n掌握Docker的基本使用方法和常见命令，可以使用Dockerfile构建镜像\n实验要求 # 请参考本实验文档，并查阅相关资料，回答以下问题并完整记录实验过程：\n数据持久化。容器是 “一次性的” 和 “脆弱的”（请大家务必牢记容器这一特性），容器很容易因为各种原因被kill（如资源不足等等）。而容器产生的数据文件是和容器绑定在一起的，当容器被删除时，这些数据文件也会被删除，这是我们不想看到的。\n比如，我们在机器上启动了一个mysql容器，在写入了一些重要数据后，因为某种原因该容器被意外删除了。此时即使重新启动一个mysql容器也找不会之前的数据了。请结合实验文档中的内容和查阅相关资料，讨论应该通过何种方式启动容器来避免出现这一问题？你能得出几种方案？每种方案的优劣如何？并请分别使用这些方案模拟mysql容器 创建 - 写入数据 - 销毁 - 重新创建 - 重新读到之前写入的数据 的场景，以证明方案的有效性。\n请从ubuntu镜像开始，构建一个新的包含Nginx服务的ubuntu镜像，并修改Nginx主页内容为你的学号，请分别使用docker commit 和 Dockerfile两种方式完成， 并将这个新构建的镜像推送到软院的image registry中。这个镜像推送的地址应该是 harbor.scs.buaa.edu.cn/\u0026lt;你的学号\u0026gt;/ubuntu-nginx:${TAG}，其中，使用docker commit构建的镜像的TAG为dockercommit；使用Dockerfile构建的镜像的TAG为 dockerfile。\n在测评时，助教会分别对你push的两个镜像执行以下命令（假设镜像名称为example_image_name）：\ndocker run -d -p 8899:80 example_image_name; sleep 5; curl localhost:8899 请保证上述命令的输出中包含你的学号。\nHint:\nharbor.scs.buaa.edu.cn 这个网页可以打开\nharbor.scs.buaa.edu.cn 的用户名为你的学号，默认密码为Newpass@2021\n如果你使用的分配的校园网的虚拟机，默认是无法联网的。如果你的容器内部需要联网，可以在启动容器前执行 export http_proxy=http://10.251.0.37:3128;export https_proxy=http://10.251.0.37:3128\n上述方式本质上只是修改当前bash进程的HTTP_PROXY，如果你有要求更高的联网需求，可以按照 此说明进行配置（配置中，需要使用的hostname是本次分配的虚拟机的名称，即docker_lab_学号。完成此说明中的操作后，虚拟机将持续处于联网状态。\n背景 # 我们知道，运行在操作系统中的软件，不仅包含我们实际编写的业务代码，还包含各种各样的运行时（runtime）（比如运行Java程序时需要依赖JRE，运行JS时需要依赖node、deno或浏览器环境，运行Python代码时需要Python环境，等等）和依赖库（lib）（比如，即使用C语言写个最简单的Hello World也得include一个stdio.h，以获取基本的向计算机屏幕打印字符串的能力）。在当前计算机性能普遍超越单个程序所需的性能的情况下，我们绝大多数情况下不会在一台机器中运行单个程序，而不同编程语言或不同类型的软件所需要的运行时和依赖库的类型和版本不尽相同，甚至同一个类型的运行时的不同版本之间还相互不兼容，这就需要我们在同一台机器中维护多个不同类型不同版本的运行时和依赖库，所以，大家在日常的学习和开发中，大概率会遇到下面这些问题：\n我想下载和使用一个软件，所有步骤都按照官方Guide一步步执行，但最后就怎么也启动不了，总是会报这样或那样的错误（经常用npm的同学应该深有体会）。 我想下载和使用一个软件，结果总是提示依赖库缺失或版本冲突，最后好不容易解决了，结果把自己本地的环境搞的一团糟，甚至最后不得不重装系统。 我在本地写好的代码，明明我的机器上跑的好好的，怎么到你那里就有bug了？！！ 从网上下载好的一个来源不明程序，莫名奇妙地向你申请各种系统权限，不给权限就罢工。 ……等等 在传统情况下，我们可以使用虚拟机技术来解决这些问题。虚拟机技术允许我们在一台计算机中模拟多个相互隔离的计算机硬件环境。我们可以创建一台虚拟机，把与软件相关的所有东西都塞到这个虚拟机中。当其他人需要这个软件时，我们只需要将该虚拟机打包成特定格式的文件，然后分发给对方就可以了。对方只需要在兼容的虚拟机环境中导入该虚拟机文件，就能保证获得同样的运行效果。\n虚拟机的解决方案简单有效，但有些简单粗暴：且不提Hypervisor层带来的性能损耗，为了运行一个应用程序就要虚拟化一整个计算机环境，并配套上一个完整的操作系统功能（维持客户机运行的各种守护进程、OS调度器等等），未免太奢侈了。\n有没有一种能够隔离两个不同的应用程序的更轻量级的解决方案呢？有的，那就是本文的主角——容器（Container）。\n容器（Container） # 在具体讨论容器（Container）之前，我们先大致了解一下容器所依赖的两个Linux机制：namespace和cgroups。\nnamepace # 我们知道，隔离两个应用程序本质上其实是隔离这两个应用程序所属的两个（或两组）进程。而如果能把两个进程使用的各项系统资源（文件系统、进程ID、网络、用户组等） 都隔离开，实际上也就达到了隔离两个进程的目的。\nLinux的namespace就提供了这样一种内核级别的资源隔离机制。它把系统的各项资源放到不同的namespace下，不同namespace下的资源是完全隔离的互不影响，而每个进程在使用某类资源时，只能归属于某个特定的namespace。这样就可以达到隔离不同进程所使用的的资源的目的。\n实际上，我们可以通过proc文件系统看到当前机器上的每个进程使用的各项资源所属的namespace，下图中展示的就是286486号进程的namespace使用情况：\n下面我们通过举例来进一步理解namespace机制。\nPID Namespace # 我们知道，在Linux中，所有进程都是从init进程（1号进程）或它的子进程fork出来的，进而，它们会组成一个树状结构：\n每个进程都会被分配一个独一无二的进程号（PID），不同进程间的相互识别、通信也都是基于进程号进行的。\n大家应该还记得Linux创建进程时使用的clone系统调用。我们可以用该函数写出下面简单的demo：\n#define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; static char child_stack[1048576]; static int child_fn() { printf(\u0026#34;子进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getppid()); printf(\u0026#34;子进程视角的子进程的PID: %ld\\n\u0026#34;, (long)getpid()); return 0; } int main() { printf(\u0026#34;父进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getpid()); pid_t child_pid = clone(child_fn, child_stack+1048576, SIGCHLD, NULL); printf(\u0026#34;父进程视角的子进程的PID: %ld\\n\u0026#34;, (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } 代码很简单，我们fork了一个子进程，并分别父进程和子进程中打印父进程和子进程的PID，得到的输出如下：\n父进程视角的父进程的PID: 53927 父进程视角的子进程的PID: 53928 子进程视角的父进程的PID: 53927 子进程视角的子进程的PID: 53928 结果完全符合我们的预期，对应的进程树的图景应该是：\n下面我们修改代码，在调用clone函数时，添加CLONE_NEWPID标志。该标志表示在fork子进程时会创建一个新的PID Namespace，并将新创建的这个子进程放入该namespace中：\n#define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; static char child_stack[1048576]; static int child_fn() { printf(\u0026#34;子进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getppid()); printf(\u0026#34;子进程视角的子进程的PID: %ld\\n\u0026#34;, (long)getpid()); return 0; } int main() { printf(\u0026#34;父进程视角的父进程的PID: %ld\\n\u0026#34;, (long)getpid()); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | SIGCHLD, NULL); printf(\u0026#34;父进程视角的子进程的PID: %ld\\n\u0026#34;, (long)child_pid); waitpid(child_pid, NULL, 0); return 0; } 这时程序的输出是这样的：\n父进程视角的父进程的PID: 54539 父进程视角的子进程的PID: 54540 子进程视角的父进程的PID: 0 子进程视角的子进程的PID: 1 可以看到，父进程输出的结果是符合预期的，但子进程却认为它的父进程PID为0，即它没有父进程；而认为自己的PID为1，即自己是1号进程。大致情形如下图所示：\n可以看到，在调用clone函数时，同时创建了一个新的PID namespace，新创建的子进程成为这个新的PID namespace中的新的进程树的根（PID=1），这个子进程不知道操作系统上还存在其他进程。可以想到，如果父进程再使用同样的方法fork子进程2，那么新创建的子进程2也会被放入一个新的PID namespace中，并成为那个新的namespace中的进程树的根。这样，子进程1和子进程2都互相不知道对方的存在，实现了互相之间对进程号的隔离。\n注意，上面仅仅是我们隔离进程的第一步。上面创建的子进程1和子进程2依然可以访问相同的文件系统、网络接口等其他资源。这些资源需要使用其他类型的namespace进行隔离。\nNetwork Namespace # 我们知道，Linux中的进程是通过网络接口（network interface）（可以理解为不同的网卡，这些网卡可能是物理存在的，也可以是虚拟的）进行网络通信的。网络接口的配置决定了进程可以以怎样的方式与网络上的哪些计算机进行通信（比如网络接口的配置包括了该接口的IP、DHCP、路由配置等）。\n可以通过ip link命令来查看当前机器上的网络接口：\n1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens192: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:92:39:60 brd ff:ff:ff:ff:ff:ff altname enp11s0 可以看到，当前机器上有一个名为ens192的物理网卡和一个loopback网络接口，这些网络接口属于全局默认的network namespace；默认情况下，从init进程fork出来的进程都属于这个全局的network namespace，意即每个进程都可以访问这些网络接口进行网络通信。而如果我们在使用clone函数fork进程的时候传递CLONE_NEWNET标志，就可以在fork进程的同时，创建一个新的network namespace，并使新进程使用这个新的network namespace中的网络接口。\n💡 loopback是一个特殊的虚拟网络接口，用于进程与本机的其他server进程通信。 我们通过下面的示例代码来说明这个问题：\n#define _GNU_SOURCE #include \u0026lt;sched.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; static char child_stack[1048576]; static int child_fn() { printf(\u0026#34;子进程所在的network namespace:\\n\u0026#34;); system(\u0026#34;ip link\u0026#34;); printf(\u0026#34;\\n\\n\u0026#34;); return 0; } int main() { printf(\u0026#34;父进程所在的network namespace:\\n\u0026#34;); system(\u0026#34;ip link\u0026#34;); printf(\u0026#34;\\n\\n\u0026#34;); pid_t child_pid = clone(child_fn, child_stack+1048576, CLONE_NEWPID | CLONE_NEWNET | SIGCHLD, NULL); waitpid(child_pid, NULL, 0); return 0; } 代码很简单，我们使用clone函数创建了子进程，并分别在父子进程中使用ip link获取当前进程能使用的网络接口。程序输出如下：\n父进程所在的network namespace: 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 2: ens192: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000 link/ether 00:50:56:92:39:60 brd ff:ff:ff:ff:ff:ff altname enp11s0 子进程所在的network namespace: 1: lo: \u0026lt;LOOPBACK\u0026gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 可以看到，子进程和父进程可以访问的网络接口是不同的（虽然两者都有lo网络接口，但本质上是不同的loopback接口）。network namespace 成功实现了网络资源的隔离。\nMount Namespace # Linux的文件系统一般会包含多个挂载点。我们可以在Linux的目录树中的指定位置挂载或卸载特定的文件系统来达到修改整个文件系统结构的目的。\n比如，在现有的目录树上挂载一个右侧的文件系统：\n就可以得到下图这样的目录树：\nLinux的mount namespace可以保证不同namespace的挂载点不受影响。在使用clone函数时，可以传递CLONE_NEWNS标志。这时在创建子进程时，操作系统会自动创建一个新的mount namespace，并把该子进程加入到这个新的namespace中。这样，子进程可以任意修改自己的挂载点而不会影响其他进程。\n比如，我们从上图的目录树开始，使用CLONE_NEWNS标志fork一个子进程：\n子进程所处的mount namespace虽然会直接继承父进程的mount namespace中的挂载点信息，但子进程对挂载点的修改不会影响到父进程的mount namespace。比如，如果我们这时可以在/root/a处再挂载一个文件系统：\n这时，子进程将获得与其他进程都不相同的一个目录树，如果方框中的文件系统只挂载在子进程的该节点上的话，那么这部分文件系统对子进程来说就是独享的。从这个角度来说，Linux通过mount namespace实现了不同进程之间文件系统的“隔离”。\n其他的Namespace # 除了PID namespace，network namespace，mount namespace之外，Linux还提供了以下3类namespace：\nUTS: 隔离主机名和域名信息 IPC: 隔离进程间通信 User: 隔离用户和用户组的ID 综合使用以上6种namespace，可以实现对进程所涉及的各项资源的隔离，进而达到隔离不同的进程的目的。\n💡 本节中的代码都是用C语言演示的，代码中使用的系统调用是Linux Kernel提供的，原则上可以使用任意语言实现（当然可能需要使用C语言绑定），包括但不限于Go，Rust等。 cgroups # 使用namespace机制进行隔离的进程，虽然可以被限制只能使用哪些资源，但没有被限制资源用量。比如，被隔离的进程依旧可以不受限制地执行大量CPU密集任务（消耗大量CPU时间），或占用消耗大量的内存。要想做到这种限制，就需要使用另一个Linux机制：cgroups。\ncgroups全称是“Control Groups”，顾名思义，cgroups可以将进程分组，并为每个进程组分配设定特定的CPU和内存限额，该进程组中的每个进程的CPU和内存用量都不能超过这个限额。\n简单来讲，namespace控制进程能看到什么（what can see），cgroups控制进程能用什么（what can use）。\n容器（Container） # 到现在为止，我们知道，在Linux中，可以使用namespace和cgroups机制隔离一个或一组进程。我们把这样的一个或一组被隔离的进程称作“容器（Container）”。当我们提到“启动（或创建）一个容器”时，其实就是在说“使用namespace和cgroups机制创建一个或一组进程”。\n💡 从这个角度看，容器的实现并不复杂，实际上，互联网上有很多相关的教程教你怎么“自己写一个Docker”。比如，这个 视频展示了怎么用Go语言从零开始实现一个简单的容器。 容器与虚拟机 # 虚拟机和容器的最大的区别在于，虚拟机模拟了一整个计算机环境，其上有一个完整的操作系统，它有自己的进程调度、内存管理等；而容器仅仅是被隔离的一个或一组进程，这些进程和在宿主操作系统看来跟其他普通进程没什么区别，他们会被Kernel以同样的方式调度，以同样的方式使用内存管理。\n镜像（Image） # 正如本文开头提到的，容器的运行依赖进程命令本身的可执行文件、运行时、依赖库和相关资源文件。为了使不同的容器所使用的这些文件之间能够相互隔离，我们可以利用在“mount namespace”一节中提到的那个“特殊的文件系统”，即，把可执行文件、运行时、依赖库和相关资源文件等都放到该“特殊的文件系统”中，当容器启动后，它只需要读写这部分“特殊的文件系统”即可。\n不难发现，这个“特殊的文件系统”包含了对容器行为的各种描述，可以认为它就是容器的“模板”，即，这个“特殊的文件系统”长啥样，对应的容器就应该表现出啥样。这个“特殊的文件系统”与容器的关系可以类比为类和对象之间的关系。更进一步地，即使我们在不同的机器上启动容器，只要这个“特殊的文件系统”相同，那么总会得到运行效果完全相同的容器。\n在日常使用中，这个“特殊的文件系统”会被打包成压缩包以方便在不同的机器中传递，而这个压缩包被称作“镜像（Image）”。通常情况下，容器总是从一个镜像启动的。\n💡 真正生产环境中所用的镜像不止包含上面提到的这些文件，还会包含一些元信息文件，包括但不限于环境变量、容器启动的入口、容器对外暴露的端口等等。 容器运行时（Container Runtime） # 到目前为止，我们启动的被隔离的子进程都是非常简单（只是一个非常简单的函数），在实际生产中，容器的启动过程要复杂的多：\n解析容器镜像 设置进程的启动入口 挂载读写卷 ……等等 更进一步地，同一台机器上可能会启动多个不同的容器，有些容器需要动态地停止和启动。这些工作显然不可能通过手动操作来完成。这时，就需要一个工具来帮我们完成对这些容器的管理工作，我们称这种工具为“容器运行时（Container Runtime）”。\n容器运行时一般都提供了非常简洁的指令入口，只需要非常简短的命令就可以启动一个复杂的容器，或者随时停止和重启一个容器。\n目前应用最广泛的容器运行时是 runc，我们常见的各种容器管理工具，如Docker、Podman、containerd等都是以runc为基础构建的。\n💡 在通常情况下，我们认为Docker、Podman、containerd等容器管理工具是广义的容器运行时。 在上面列举的这些容器运行时中，Docker是使用最广泛的容器管理工具，甚至在很多人的认识中，Docker和容器简直就是同义词。虽然Kubernetes将在1.24版本中不再支持使用Docker作为容器运行时，Podman和Containerd也在逐步蚕食Docker的市场，但Docker依然处于垄断地位，很多其他的容器运行时甚至也特意将自己的使用方法设计地和Docker一模一样（如Podman）。所以，我们本次课程实验还是将主要讲解Docker的原理、架构和使用方法。入门了Docker后，了解和学习其他容器运行时也会变得非常简单。\nDocker # 上图是Docker的经典Logo，一个白鲸载着集装箱的形象。“Docker”这个词是从“Dock”演变来的。Dock意为“码头”，Docker自然可以引申为“承载集装箱的工具”。“Container”本身也有“集装箱”的含义，Docker作为一个容器（Container）管理工具，这样的logo可谓是非常生动形象了。\n我们平常所说的“Docker”，其实是一个巨大的结合体。从下图中可以看出，当用户使用Docker时，要经过多层组件的调用。虽然它们中的很多部分都可以单独作为一个独立软件来用（比如Containerd、runc），但我们在谈论Docker时，通常认为它们是整个Docker软件的一部分。在安装Docker时，这些组件也会被同时默认安装。\nDocker本身自诩是开源软件，它的上游构建组件确实是开源的，可以在这里找到它的代码 moby/moby。\n💡 对于Dodcker的学习，互联网上有很多完整可靠的教程，比如Docker官方的 Docker 101 tutorial。这些都是非常不错的学习资料。 Docker的安装 # 💡 如果你是使用本次实验分配的虚拟机，请忽略本节所有内容。由于实验人数较多，云平台压力较大，可能出现虚拟机卡顿的情况，因此，建议同学们优先使用本地环境进行实验。 macOS # 直接下载 Docker Desktop安装即可\nWindows # 如果你正在使用WSL2，直接按照该 指南安装Docker Desktop即可 如果你没有使用WSL2，可以选择先安装或升级到WSL2，然后执行上一步；也可以直接安装一个Linux虚拟机，然后在Linux中安装Docker Linux # 请参考 文档，并从“使用脚本自动安装”开始读起。\n线上环境 # 实际上，有很多网站提供了免费的线上Docker环境供大家学习和体验，免去了在本地配置环境的繁琐，而且没有网络问题，在浏览器中就可以学习Docker各种概念。比如，你可以注册并登录 Play with Docker尝试看看。在这些线上环境中同样可以完成本次实验。\n配置镜像源 # 我们常用的Docker官方的image registry在国内的连接非常不稳定，拉取镜像时很可能非常缓慢，这时可以配置镜像源，请参考 国内镜像加速。另外，如果你在校园网内，可以考虑使用软院的镜像加速器地址：http://10.251.0.37:5000\n⚠️ 请再次注意，我们在本次实验中讨论的容器是使用namespace和cgroups隔离的进程。所以理论上，这些容器只能在有Linux内核的操作系统上启动并运行。虽然可以通过Docker Desktop在macOS或Windows上启动容器，但本质上是因为Docker Desktop自动在你的机器上安装了一个Linux虚拟机，这些容器是启动在这个虚拟机里的。 Docker的简单使用 # 我们首先启动一个非常简单的Ubuntu容器，下面这条命令可以将会在一个隔离Ubuntu环境中执行/bin/bash进程：\ndocker run -it --rm ubuntu /bin/bash 可以看到，docker成功为我们启动了一个Ubuntu环境中的/bin/bash进程，当前我们所在的命令行环境已经不是原来主机上的环境了。可以使用下面的命令进行验证。\n容器环境拥有与宿主机不同的hostname：\n容器环境的发行版标记与宿主机不同（容器环境是Ubuntu，而主机是Debian）：\n容器环境中可以看到启动的进程只有/bin/bash，并且该进程为容器中的1号进程：\n我们在容器中对文件的增删并不会对宿主机中的环境造成任何影响：\n但是，当我们查看内核版本时，可以发现两者内核完全相同，这说明我们启动的容器不包含完整的操作系统，本质上只是一个被隔离的进程而已：\nDocker的架构 # 上图展示了一条docker命令是如何被执行的：\n用户输入命令\ndocker cli解析这条命令，转化成相应格式的请求，通过读写 Unix socket 的方式与docker engine通信，告诉docker engine应该执行怎样的操作\n正如我们前面提到的，镜像是容器的模板，我们使用docker命令创建容器需要先得到容器对应的镜像才行。当docker engine发现本机上没有对应的容器镜像时，就会根据镜像的名称从远程仓库（image registry）（可以将其想象成一个网盘）中下载对应的镜像到本地。实际上，你可能注意到我们在前面启动Ubuntu容器时就发生过这个下载操作：\n图中标出来的这段log清晰地说明了发生了什么：无法在本地找到image ‘ubuntu:latest’ ，于是从远程仓库中将这个镜像pull了下来。\n当镜像准备就绪后，docker engine就可以将任务向下传递，最终使用系统调用（namespace和cgroups）创建或管理容器\n⚠️ 上述步骤只是对docker工作流程的一个非常非常简化的讨论。 综上，我们可以看到docker实际上主要由三部分组成docker cli、docker engine、image registry。在后面的实验中，我们将逐步加深对着三部分的理解。\n💡 实际上，docker cli并不是必须的，任何可以读写docker engine所暴露的Unix socket（通常这个socket的文件名是/var/run/docker.sock）的程序都可以通过docker engine来实现docker的功能；甚至docker engine还可以对外暴露tcp端口，使外部程序使用特定的HTTP接口发起调用。 docker image # 镜像（Image）是容器的基础，所以我们先从镜像谈起。\n💡 从这里开始，我们将会涉及到大量的命令，这些命令看似繁杂，但平时常用的就那么几个，并且大部分都有规律，请同学们注意体会。当忘记某条命令时，可以随时使用docker —help查看帮助；或者访问 Docker官网的文档。注意，本实验的目的不是死记硬背这些命令，而是理解相关概念并对容器技术和Docker有个感性认识。 基本命令 # docker中镜像与相关的操作都包含在image子命令中，如：\ndocker image ls 可以列出当前机器的所有容器镜像：\nroot@template-debian11-base:/## docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu latest ba6acccedd29 5 months ago 72.8MB docker image pull \u0026lt;image_name\u0026gt; 可以从image registry中拉取名称为image_name的镜像：\nroot@template-debian11-base:/## docker image pull harbor.scs.buaa.edu.cn/library/mysql:8 8: Pulling from library/mysql b380bbd43752: Pull complete f23cbf2ecc5d: Pull complete 30cfc6c29c0a: Pull complete b38609286cbe: Pull complete 8211d9e66cd6: Pull complete 2313f9eeca4a: Pull complete 7eb487d00da0: Pull complete a5d2b117a938: Pull complete 1f6cb474cd1c: Pull complete 896b3fd2ab07: Pull complete 532e67ebb376: Pull complete 233c7958b33f: Pull complete Digest: sha256:882e55f40d61034a2bb8a1abab1353571ad2a33866f382350788eb34740528b5 Status: Downloaded newer image for harbor.scs.buaa.edu.cn/library/mysql:8 harbor.scs.buaa.edu.cn/library/mysql:8 镜像名 # 一般地，镜像名完整格式为{image registry地址}/{仓库名}/{镜像名}:TAG。\n例如本例中使用的 harbor.scs.buaa.edu.cn/library/mysql:8 ，其中：\nharbor.scs.buaa.edu.cn为image registry的地址，用来告诉docker去哪里pull这个镜像； library表示仓库名，表示这个镜像的所有者是谁； mysql表示镜像的名称； 8是镜像的TAG，一般用来表示镜像的版本号。如果一个镜像名没有TAG，那么将会被认为TAG为latest，即harbor.scs.buaa.edu.cn/library/mysql等同于harbor.scs.buaa.edu.cn/library/mysql:latest。 你可能已经注意到，我们在最初创建Ubuntu容器的时候也没那么麻烦，直接用ubuntu就表示了镜像名。那是因为当仅使用一个单词表示镜像名时，docker自动为它补上仓库名library、自家的image registry地址docker.io，以及TAGlatest；即ubuntu等同于docker.io/library/ubuntu:latest。\n镜像ID # 你可能还注意到，当使用docker image ls时，有一列叫做IMAGE ID，这一列中的字符串其实就是每个镜像对应的独一无二的ID，它是一个镜像独一无二的标识。我们可以使用这个ID对镜像做各种操作，比如删除一个镜像：docker image rm 3218b38490ce。\n为啥需要镜像ID呢？用镜像名标识镜像不好吗？不好。因为镜像名是可以任意改变的。我们可以使用docker tag命令来为镜像设置别名。比如，对上图中列出的ubuntu镜像，我们可以：\n可以看到，docker tag之后，生成了一个新的镜像areyouok，但这个镜像拥有和原来的ubuntu一样的ID，说明areyouok和ubuntu这两个不同的镜像名指向同一个实体。\nimage registry # 在介绍镜像名称时，同学们可能会疑惑，image registry有很多个吗，为啥还需要地址来标识？是的，image registry有很多个。image registry有的是公开的，任何人都可以访问，并从中拉取镜像；也有私有的，需要特殊的口令访问。目前，世界上最大的几个公开的image registry有Docker公司提供的 docker.io（目前也是世界上最大、使用最广泛的image registry，如果你需要通过浏览器访问的话，需要使用这个地址： hub.docker.com）、Redhat提供的 quay.io、Google提供的 gcr.io（很可惜，这个地址在国内被*了）；当然还有我们软院的image registry： harbor.scs.buaa.edu.cn。\nimage registry不仅可以下载已经存在的镜像，还可以上传和保存自己制作的新的镜像。任何人都可以在上述registry网站创建账户和自己的仓库。对于用户上传到image registry中的镜像，用户可以自行选择是否对其他用户公开访问（公开或私有）。如果是私有镜像，则需要在每次上传和下载镜像前，在本地执行 docker login操作。\n容器管理 # 可以使用docker ps命令查看当前机器上处于活跃状态的容器：\ndocker ps -a 可以列出所有状态的容器（包括活跃的和不活跃的）：\n从上图中的输出可以看出，和镜像一样，每个容器也都有一个唯一的ID作为标识，我们对容器的各项操作也是通过该ID进行的。除了ID之外，每个容器也都有一个独一无二的name，我们也可以使用name来唯一指定一个容器。\n在Docker的管理下，容器有以下6种状态：\n当用户输入docker run命令后，容器被创建，进入短暂的Created状态 当容器进程启动完毕后，容器进入Running状态，这表示容器正在正常工作 当用户使用docker stop显式地终止一个容器，或容器运行出错时，容器进入Exited状态。这个状态是不活跃的，处于这个状态的容器不会消耗任何资源 对于处于Exited的容器，可以被手动使用docker start重启，重新进入Running状态；也可能被Docker管理服务重启，短暂进入Restarting状态后，重新回到Running状态 可以手动使用docker pause暂停容器，此时容器将进入Paused状态。在这种状态中，容器将停止运行，即不会消耗任何CPU，但依旧会占据内存（以便随时从这个被pause的状态恢复运行） 当使用docker rm删除容器，但容器中的一些资源依旧被外部进程使用时（即无法立即删除时），容器将进入Dead状态 关于容器状态的更加详细的讨论，可以参考 这篇文章。\n启动容器 # 对于一般的用户，docker run命令是最常用也是最复杂的命令之一。docker run命令用于创建并运行容器。完整的命令参数及用法参见 Docker官方文档，下面我们会以两个例子来讨论一些常见的用法。\n基本结构 # docker run命令的结构是\ndocker run [一堆各种各样的参数] \u0026lt;image_name\u0026gt; [启动命令] [命令所使用的参数]。\n其中，只有镜像名image_name是必须的，其余全是可选的。\n启动命令 # 回想我们创建第一个docker镜像时使用的命令：docker run -it --rm ubuntu /bin/bash。这里的参数是-it —rm，镜像名是ubuntu，启动命令是/bin/bash，没有命令参数。\n这里/bin/bash的含义是，启动ubuntu容器后，执行/bin/bash命令，即启动一个bash shell。\n我们可以将这里的/bin/bash换成ls /usr看看效果：\n这时，启动命令是ls，命令参数是/usr，表示启动ubuntu容器，并在其中执行ls /usr命令，即列出/usr目录下的所有目录和文件。可以看到，执行效果确实如此。并且，请注意，因为我们没有执行/bin/bash命令，在ls命令执行完返回后，并没有进入容器的命令行中，而是回到了宿主机。\n不难发现，启动命令决定了容器启动后的具体行为。事实上，启动命令是可以省略的，如果省略的话，docker会执行镜像指定的启动命令（这个信息一般会作为元信息打包到镜像中）。\n-it # 参数i表示interactive，参数t表示创建一个虚拟的TTY（pseudo-TTY）。简单来说，-it参数可以让我们进入一个可以与容器进行交互的终端环境。\n比如，如果我们在启动命令中去掉-it，启动的ubuntu容器将会在后台运行，我们将无法和它交互：\n—-rm # 参数—-rm表示当容器退出后，自动删除容器。\n如果在执行容器时，不加—-rm参数，则当从容器退出后，容器进入Exited状态，继续存在在机器上（虽然此时为非活跃状态，不消耗任何资源），并且我们可以用docker restart等命令重启容器，或使用docker exec命令查看容器中的文件。\n在上图中，我们首先使用—rm参数启动了一个ubuntu容器，在与其交互后，退出容器（即结束/bin/bash进程，此时容器进入Exited状态）。因为使用了—rm参数，所以此时容器会被Docker自动删除，使用docker ps -a命令找不到该容器的存在。\n而如果启动命令中不加—rm参数，则容器退出后，将继续存在在机器上：\n这时，只能使用docker rm命令手动删除容器。\n-v # 到现在为止，我们使用的容器的文件系统都是与宿主机完全隔离的。但在很多时候，容器需要与宿主机共享一些目录，比如位于宿主机上的一些进程希望能方便地看到容器运行中产生的文件，或者通过修改一些文件来影响容器的行为。\n为了解决这一问题，我们可以使用-v参数将容器的某个目录和宿主机的某个目录绑定起来，使得容器在读写某个目录时，相当于在同时读写宿主机的某个目录。\n参数-v的使用方法是docker run -v \u0026lt;宿主机的目录\u0026gt;:\u0026lt;容器的目录\u0026gt; \u0026lt;image_name\u0026gt;。还是以前面使用的ubuntu容器举例。比如，我们希望将宿主机的/opt/mycontainer和容器中的/root目录绑定起来：\n可以看到，我们在容器的/root目录下创建了test.txt文件，然后在宿主机上的/opt/mycontianer确实观察到了该文件。同样地，在宿主机上对应目录的修改，也会被容器观察到：\n注意，-v参数是可以重复的，也就是说，可以在一条docker run命令中同时指定多个共享目录，比如：docker run -it --rm -v /opt/mycontainer:/root -v /another/host/dir:/another/contianer/dir ubuntu /bin/bash\n—-name # 参数—-name可以为启动的容器添加名字。我们之前的docker run命令都没有使用该参数，那么这时docker自己会为该容器分配一个随机字符串作为name。\n-p, —-publish # 参数—-publish（可以简写为-p）表示将容器的某个端口暴露到宿主机的某个端口。\n在详细讨论该参数之前，我们先从Nginx容器说起。 Nginx是目前应用非常广泛的网页服务器（当然它的用处不仅限于此），在一个启动了Nginx服务的系统中，可以通过80端口访问到网页。Nginx同样提供了开箱即用的docker镜像，只需要一条简单的命令docker run nginx就可以创建一个Nginx进程：\n可以看到，Nginx进程已经成功启动了，这说明它能在80端口监听HTTP请求。我们可以打开一个新的终端，使用curl尝试访问一下：\n访问失败！为啥呢？还记得“network namespace”吗？我们启动的Nginx容器进程和当前启动的curl进程不属于同一个network namespace！它们自然无法相互通信。当前这个Nginx进程对80端口的监听仅在它所在的network namespace有效。\n但这不满足需求啊，我们启动Nginx容器的目的就是向外提供服务啊，Nginx在自己的容器里自娱自乐怎么行！这时就要用到参数—-publish（可以简写为-p）了，它可以将容器的端口“发布”（publish）到宿主机的某个端口。用法是-p \u0026lt;宿主机端口\u0026gt;:\u0026lt;容器端口\u0026gt;。\n比如：\n这样，Nginx容器的80端口就会和宿主机的80端口绑定，访问宿主机的80端口也就相当于访问Nginx容器的80端口。不信试试看：\n成功🌶️！而且，对应的Nginx进程也打印出来请求日志：\n注意！宿主机的端口不一定要和容器的端口相同（一般都是不同的），比如：\n和-v参数类似，-p参数也可以同时有多个。\n-d, --detach # 大家可能注意到，上节中启动的Nginx容器都是在前台运行的，它在不断地输出日志，占据了整个终端，导致我们无法进一步和shell交互，以至于要执行curl操作时还得新打开一个终端。如果想终止这种输出，就得手动Ctrl+c终止Nginx容器。而且，如果此时关闭终端的话，也会同时退出这个容器。\n在很多时候我们不希望容器运行在前台，而只是想让它安安静静地作为后台进程提供服务。这时，就可以使用—-detach（简写为-d）参数。比如：\n可以看到，在输出了容器的ID后，Nginx安安静静地在后台运行了。\n那么，我们有时候又需要查看容器的日志怎么办呢？很简单，使用docker logs命令：\n进入容器内部 # 对于一个在后台运行的容器（如上节中的Nginx容器），如果想进入容器中查看当前容器的文件结构或执行一些命令用于debug，该怎么办呢？可以使用docker exec命令。\n比如，如果需要进入容器中的bash shell的话，首先需要找到容器对应的ID\n然后执行docker exec即可：\n构建新镜像 # 到现在为止，我们只是在使用他人已经提前做好的镜像。如何制造我们自己的镜像呢？下面给出两种方法。\ndocker commit # 假设我们正在使用一个ubuntu容器，并且在该容器的根目录下创建了一个非常重要的数据文件：\n如果我想把当前容器的状态保存下来，以便下次启动容器的时候可以重新使用该文件；或者我想把当前容器发送给别人，让别人也看到我当前看到的容器的样子，该怎么办呢？可以使用docker commit命令将当前容器打包成一个新的镜像。\n重新打开一个终端，查看一下当前容器的ID：\n然后直接docker commit \u0026lt;container_id\u0026gt; \u0026lt;new_image_name\u0026gt;即可：\n这时，使用docker image ls可以看到这个新生成的镜像：\n我们尝试运行一下这个新生成的镜像，可以看到刚才创建的文件果然存在：\n💡 实际上，docker commit还支持很多参数，详情请见 官方文档。 Dockerfile # docker commit虽然可以非常直观地从当前容器创建一个新的镜像。但整个过程不够规范，也很难实现自动化，一般情况下，我们都是使用Dockerfile来构建镜像的。\n所谓的Dockerfile，其实就是一个配置文件，里面描述了构建镜像的步骤。对于上一小节中构建出的镜像，如果使用Dockerfile来写的话，是这样的：\nFROM ubuntu RUN echo \u0026#39;something important\u0026#39; \u0026gt; /important.data 很显然，上述文件内容是自解释的。一般Dockerfile的开头一条FROM语句，表示从以哪个镜像为基础进行构建。下面的RUN语句表示在容器中执行一条命令。\n可以将上述Dockerfile保存在一个干净的目录中，然后在该目录中执行docker build -t my_new_image_from_dockerfile -f ./Dockerfile .（请注意，命令的最后有个.）：\n可以看到，我们成功build了一个新的镜像。下面详细解释一下上面那条docker build命令的含义：\n-t表示要构建的新的镜像的名称 -f表示Dockerfile文件的路径 命令中最后的单词表示表示构建镜像的上下文路径，上图中这个最后的单词是.，则表示上下文路径是当前目录。在镜像构建开始的时候，docker cli会把“上下文路径”指定的目录中的所有内容打包，然后整个发送给docker engine 事实上，Dockerfile还支持非常多的指令，具体请查阅 官方文档。\n发布镜像 # 截止目前，我们构建的镜像全是在本机上，别人根本访问不到，也没法使用。我们可以将镜像push到image registry上，然后通知对方从该image registry拉取即可。具体可以参考 docker push的文档。\n"},{"id":4,"href":"/doc/ns-labs/resources/os/","title":"操作系统","section":"Resources","content":" 操作系统 # 常见操作系统 ISO 镜像可至官网、 MSDN，我告诉你、 Next, ITELLYOU 等网站上去下载。\nWindows Server 2019 # ED2K 下载：ed2k://|file|cn_windows_server_2019_updated_july_2020_x64_dvd_2c9b67da.iso|5675251712|19AE348F0D321785007D95B7D2FAF320|/\n使用 KMS 激活方式如下：\n# 使用管理员身份打开PowerShell DISM /online /Set-Edition:ServerDatacenter /ProductKey:WMDGN-G9PQG-XVVXX-R3X43-63DFG /AcceptEula # 打开 CMD (管理员) slmgr.vbs /ipk WMDGN-G9PQG-XVVXX-R3X43-63DFG slmgr.vbs /skms kms.teevee.asia slmgr.vbs /ato "},{"id":5,"href":"/doc/cloud-labs/cloud/cloud_course_design/","title":"云计算课程设计","section":"云计算实验","content":" 云计算课程设计 # 背景 # 当今时代是云原生的时代。\n容器化带来的优势 # 在之前的《容器与Docker》的实验中，我们从“隔离不同的应用程序”入手，介绍了引入容器的必然性。容器之间这种相互隔离的特性为使它非常便于进行弹性伸缩和迁移部署。\n各位试想一下自己之前部署一个应用的过程，通常分为这么几个步骤：\n编译源代码，得到编译产物（你可能需要根据不同的部署目标编译多个不同平台的编译产物，比如在macOS和Windows上开发，却需要编译Linux目标平台的产物） 在目标机器上安装运行时环境（对于部分类型的应用程序来说，这一步可以省略，如大部分Go、Rust等编写的应用） 将编译产物传送到目标机器 设置程序运行时需要的环境变量、配置文件等（大多数情况下至少需要维护开发时和运行时两种不同的配置文件和环境变量） 启动应用程序 如果是需要长期稳定运行的程序，还需要配置程序作为守护进程启动（可能需要编写systemd配置文件等） 而对于以容器形式部署的应用来说，只需要：\n编写Dockerfile（这里会包含进程在运行时需要使用的环境变量和配置文件等） 根据Dockerfile构建镜像 推送镜像到目标机器，然后使用docker run启动容器 整个过程流畅而优雅，并且因为需要操作的步骤很少，所以会有更低的出错的可能（在传统的部署过程中，敲错一行命令导致部署失败是常有的事）。而且，更进一步地：\n有些时候，我们将机器A上的应用迁移到机器B上时，如果使用传统部署方式，则很可能需要将1至6这几个步骤重复走一遍；而如果使用容器部署方式，只需要在机器B上拉取机器A中使用的镜像，重新启动容器即可。 有些时候，我们可能需要在同一台机器上部署和管理一个应用的多个不同的实例（如对于单进程应用，出于负载均衡，充分榨干机器性能的需要），如果使用传统部署方式，很可能需要为每个需要部署的实例修改不同的配置，并手动开启或关闭这多个实例；而如果使用容器部署的方式，则只需修改docker run的参数，对于启动的多个实例，可以轻松地使用docker本身提供的工具进行管理。 大多数情况下，一个应用程序往往会依赖若干个不同的外部服务。比如一个Java编写的后端服务，可能需要依赖数据库、对象存储等服务。这些外部服务的部署和配置在部署应用时也是一个很让人头疼的事情。而对于容器的部署方式，这些外部服务，诸如数据库、对象存储等，完全也可以实现容器化，它们的部署也可以使用基于容器的方式进行（相信各位在上次实验中已经体会到了使用容器的方式部署MySQL服务的便利性）。 微服务与容器 # 相信很多同学都接触或者实践过微服务架构（比如Spring Cloud之类的）。微服务，简单来讲就是说将原来的整个软件系统，拆分成不同的模块，每个模块对外都表现为一个独立的系统（它们可以独立进行开发、测试和部署），每个模块对外暴露规划好的接口，不同模块之间通过某种协议（一般是原生的HTTP或各种各样的RPC协议）相互调用这些接口，从而组织成整个完整的软件系统。\n微服务带来了很多好处：\n整个业务系统“高内聚，低耦合”。不同模块只需要维护好对外的接口即可，对内完全是自治的。这给整个软件系统的迭代更新带来了极大的便利。比如，可以根据需要很方便地增减不同模块等等。 不同模块相互独立，其之间的沟通交流一般使用的是基于HTTP的与编程语言和编程框架无关的协议。这就使得同一个软件系统内部的不同微服务模块可以交给不同团队开发，不同团队可以根据自身的技术积累，或者当前模块的特点，使用不同的编程语言和框架来实现。 不同微服务模块的部署是相互独立的。这就意味着，可以根据需要调整不同模块部署的实例个数，而无需调整整个软件系统的部署情况，从而可以充分利用硬件资源。 微服务架构的上述特点很适合互联网业务敏捷开发、快速迭代的工作方式。 不难发现，微服务的特点和容器的优势有很大的重合点，容器非常适合用来作为微服务的实现方式。即，对每个模块构建一个或多个镜像，然后以部署容器的方式部署每个微服务模块。事实上，我们当前使用的各个大厂的主要业务都是跑在容器里面的。“微服务化”和“容器化”也基本成为了同义词。\n容器管理与云 # 当前大部分互联网公司的业务的部署都是以容器的方式进行的。整个软件系统被拆成一个个微服务模块，每个微服务的实例都是一个容器。这些成千上万的容器翱翔在以数以万计的服务器组成的大规模集群的资源池中。容器定义了新时代的云。而所谓的“上云”，也基本意味着“将应用以容器方式进行部署和管理”。\n随着容器数量的扩大，容器管理问题也逐渐凸显出来。试想，在容器数量很少时，你可以在自己的机器上手动使用docker命令开启或关闭容器。但当容器数量数以万计时，手动操作的成本就会迅速增加，因为：\n即使每个容器因为各种各样的原因崩溃的概率很低，但乘以一个很大的容器总数，就会使“每时每刻都会有若干容器挂掉”变成大概率事件。为了保证业务的正常运行，必须能够即使发现这些崩溃的容器，并将它们重新启动。 在业务迭代时，经常需要更新容器的镜像。这意味着，我们必须能够在数以万计的容器中，找到先前版本的所有容器实例，并将它们关闭移除，然后启动新版本镜像的容器。 集群中的各个机器的状态可能不同（集群可能是异构的），为了充分发挥硬件资源的能力，必须能够即使根据不同机器的状态（空闲与否）和容器占用资源的情况，将容器调度到不同机器上运行。 不仅如此，随着单纯的容器化本身只是一种“理想的愿景”，有很多实际问题需要考虑：\n前面我们提到，微服务的各个模块之间需要相互调用，这也意味着，不同容器之间需要相互交换信息，那么如何定位不同的容器、容器之间以怎样的方式发送和接收消息就成为一个非常重要的问题。 容器本身是“一次性”的，它的存储相关工作一般都交给数据卷来保存。不仅如此，数据对任何业务来讲都是最宝贵的资源，为了保证高可用，数据卷的管理必须考虑备份、容灾等。当容器数量增大时，数据卷的管理将变得非常复杂。 围绕这上述这些实际生产中的问题，很多科技公司都积极尝试并给出了自己的解决方案，但最终还是Google开源的Kubernetes笑到了最后，并已经在当下成为了分布式容器管理和容器编排的事实标准。而当下繁荣的云原生生态，也正是以容器和Kubernetes为基石，围绕它们构建起来的。\n如果同学们对云计算感兴趣，可以去关注一下 CNCF（云原生计算基金会），它是Linux基金会的一部分，收录了大量云计算方面的开源项目，很多国内外大厂都是它的会员，同时也会给它捐献自己的云计算方面的开源项目，在 这里你可以看到CNCF的全景图。CNCF每年都会举办很多面向学生的开源项目活动，是一个接触并深入云计算领域的非常不错的窗口，感兴趣的同学可以多留意相关信息（比如各个大厂的云原生公众号，CNCF在国内的公众号等）。\n课程设计内容 # 相信大家都上过软件学院的软件工程这门课。课程的大作业一般都需要完成做一次完整的软件开发，最终一般都需要提交源代码和相关文档等。但这门课的作业检查是非常痛苦的。不同小组使用的编程语言、编程框架及其相应的版本都不一而同。助教或教师如果想实际检查源代码的正确性（即能正确编译并在部署后能呈现出你文档中描述的那种效果）是非常困难的。\n因此，请以软件学院为样本，结合容器技术，设计这样一个Web应用，教师或助教部署作业任务后，学生提交源代码和必要的编译部署选项（比如编译脚本、部署参数等）；教师或助教在学生提交完成后，只需要在系统中点击“编译”和“部署”按钮，就可以完成学生作品的部署流程，并观察到学生作品的运行效果。\n以上只是对系统的笼统描述，有很多细节需要考虑：\n软院云平台现有课程管理、实验管理、提交作业功能，如何将这些现有功能与新系统联动起来？ 学生提交作业时使用什么方式？直接提交源代码压缩包，还是给出一个代码托管地址？有自己的代码托管平台BuGit，如果将新系统与其联动起来？ 学生在完成作业后，提交代码前，很可能也需要自己在系统上尝试编译部署进行自测，以保证自己代码的运行效果与本地测试的效果相同。 （可选）大多数情况下，学生在完成作业时，都使用版本控制系统（如Git等）来管理代码。学生很可能需要在每次提交代码时能验证本次所提交的代码没有破坏之前的功能。即，系统应该可以在每次学生提交新代码（如果使用Git进行管理的话，就是在每次提交新commit时）时自动拉取学生代码，或者运行代码库的单元测试，或者进行编译和部署，并将结果展示出来。熟悉 GitHub Actions和 CI/CD的同学应该很容易理解这里在说什么。 本次课程设计并不需要大家真正实现这一系统，而是要编写文档，描述：\n对上述场景进行需求分析，可以使用用例图等形式描述系统中包含哪些角色，每个角色应该有怎样的功能。请大家重视这一部分的工作，这是你接下来设计系统的重要前提。 描述系统的设计结构，可以使用原型图、架构图、流程图等形式，说明系统从教师或助教部署作业，到教师或助教看到学生的作业效果这一端到端的流程是怎样工作的。并请结合一个典型的作业类型（例如，一个包含了前端、后端和数据库的课程作业），来举例描述系统的工作流程。 描述技术选型，描述计划使用哪些关键技术，并且这些技术是怎样为最终系统功能服务的。 "},{"id":6,"href":"/doc/cloud-labs/cloud/kube-single-3/","title":"Kubernetes综合实验","section":"云计算实验","content":" Kubernetes综合实验 # 实验目的 # 了解Kubernetes的各种特性 掌握Kubernetes的常用功能 注意事项\n本次分配的机器的账户和密码为： buaa: \u0026amp;shieshuyuan21 为了避免权限问题，建议切换到root账户操作：\n首先使用sudo passwd root为root账户设置密码 然后使用sudo su命令切换到root账户 务必首先修改机器的root和buaa账户的密码\n请务必阅读 虚拟机使用说明。\n分配的虚拟机中，已经安装了Docker，无需重复安装。\n背景 # 上学期的云计算课中，我们主要了解了什么是容器，以及目前最流行的容器运行时和容器管理工具Docker；并且在此过程中体会了容器技术给软件开发和部署带来的极大的便利性。\n但到此为止，我们对容器的使用和管理依旧处于非常“手工”的状态，难以胜任实际生产环境中对容器管理的要求。在实际生产环境中，\n通常一个应用包含多个容器；即，一个应用的部署，需要按照一定的顺序启动多个容器。例如，即便是一个最简单的前后端应用，我们也需要依次启动三个容器：数据库容器（例如一个MySQL容器）、后端应用容器、前端应用容器（通常是一个Nginx容器）。特别是在微服务场景中，后端可能涉及到几十个微服务模块，每个模块都对应着一个容器，不同服务之间又有复杂的依赖调用关系。 每个类别的服务通常会有多个容器实例。对于一些负载较高的服务，通常会部署多个相同的容器实例达到负载均衡的效果，从而提高服务整体的吞吐量。 容器是不稳定的，随时可能会因为各种各样的原因挂掉，因此，需要时刻监控容器的状态，在它挂掉的时候及时重新启动服务，保证服务整体的高可用。 大部分应用都是分布式的。即，一个应用中的不同服务是部署在不同机器上的，即使是一个服务的不同实例往往也会部署在多个机器上。如何在多台机器上做好资源（内存、CPU、磁盘）的负载均衡（即，避免出现某些机器负载过高的同时，其他机器负载空闲的情况）也是棘手的问题。 如果仅靠我们已经学到的几个docker命令显然是难以完成上述任务的。这就需要一个专门的容器编排调度工具帮我们处理这些事情。自Docker兴起后，很多厂商都进入该领域并推出了自己的容器编排调度解决方案，例如Docker Swarm、Mesos等，都想在新兴的容器市场中分一杯羹，但最终Kubernetes笑到了最后，并且作为CNCF的毕业项目，成为了当下容器编排调度领域的事实标准。本次实验我们就来认识一下Kubernetes，学习并实践其中一些基本概念。\nKubernetes以其复杂难懂著称，在本次实验中，我们主要学习其中最基本的部分，培养大家对Kubernetes的感性认识，帮助大家开始入门云计算领域。\n初识Kubernetes # Kubernetes简介 # Kubernetes在希腊语中的含义是船长/领航员，这个名字生动地体现了它在容器集群管理中的作用——调度和监控全局的集装箱（container，容器）。由于Kubernetes这个单词太长，人们通常会用k8s来作为简称（Kubernetes的首尾两个字母之间正好有8个字母）。\n请始终记住，Kubernetes和Docker之类的容器运行时不是互相替代的关系，也不是包含与被包含的关系，而是互补的关系。Kubernetes仅仅是一个容器编排和调度工具，其必须运行在“容器运行时（container runtime）”之上。它能做的仅仅是接收用户的命令，然后通知其下层的容器运行时做具体的工作。\n上图可以看出，在之前，我们是直接通过Docker命令行或Docker HTTP接口来与Docker容器运行时通信，控制其构建镜像、推送或拉取镜像、启动或停止容器，等等。\n而现在，我们可以通过Kubernetes的命令行工具（即Kubectl）或Kubernetes的HTTP接口来控制Kubernetes，然后，Kubernetes会根据我们发出的命令，“翻译”成对应的Docker容器运行时的调用，从而控制Docker容器运行时构建镜像、推送或拉取镜像、启动或停止容器等等。\n另外，请注意，Kubernetes是一个非常模块化的系统，它定义了一套“容器运行时接口（CRI）”，凡是实现了这套接口的容器运行时都可以作为Kubernetes运行容器的后端。目前比较流行的有Containerd和CRI-O，实际上，从1.20版本开始，Kubernetes官方已经弃用Docker引擎作为容器运行时。\n在本次实验中，为了前后知识的连贯性，我们依旧选择使用Docker作为Kubernetes的容器运行时。\n创建Kubernetes集群 # 在Kubernetes官网的 Get Started中，分别给出了面向个人初学者的学习测试环境和一线生产环境的若干Kubernetes集群部署方法。对于生产环境， Kubernetes官方推荐使用kubeadm来启动集群。但实际上，kubeadm对非专业的运维，特别是初学者来说并不十分友好（需要用户事先完成对主机的一系列配置，如开放防火墙端口、关闭swap、安装容器运行时等），再加上国内特殊的网络环境（kubeadm默认会从gcr.io拉取启动Kubernetes所需的系统镜像），因此不推荐初学者直接使用kubeadm启动集群。\n云计算生态非常繁荣，社区中已经有很多成熟的工具帮助用户快速启动一个标准的Kubernetes集群用于学习、测试等目的。本实验文档分别针对集群版和本地版提供创建Kubernetes集群的选项，供大家参考和使用。\n集群版 # 推荐使用此种方式创建集群，这样有利于在后续章节中学习和实践Kubernetes“集群”的相关特性。如确实有问题，可以切换到个人电脑上使用“单机版”的方式。\n在本次实验中，我们将部署一个简单的 k3s（注意，不是“k8s”哦！）集群。k3s是通过CNCF认证的Kubernetes的一个发行版，是基于上游的“原生”的Kubernetes的代码进一步构建的。k3s和Kubernetes的关系可以简单类比为Ubuntu、CentOS等于Linux之间的关系。你可以在这里找到k3s官方维护的 k3s的中文文档。\n实际上，还有很多其他的Kubernetes发行版，比如k0s之类的。只不过k3s的中国化做得非常好，在国内的网络环境下使用非常便利，也有大量的资料可以使用，所以我们选用其作为本次实验的主要工具。当然，不同Kubernetes发行版之间存在着各种各样的差异，但这并不影响我们学习Kubernetes的基础知识。\n准备一台能够连接互联网的Linux机器。如果你选择使用软院云平台分配的机器，可以直接进行下一步；否则，请查看 安装要求，确保你的机器满足其中所述条件。\n确保机器已经安装Docker，并且其版本高于或等于19.03，可以使用docker info命令验证。如果你使用的是软院云平台分配的机器，可直接进行下一步；否则，请注意检查Docker是否正确安装，并且是否正确配置Docker Registry镜像地址，以保证可以顺利从docker.io中拉取镜像。（当然，如果你已经比较熟悉Kubernetes，可以忽略此步骤，选择使用k3s默认集成安装的containerd作为容器运行时。）\n保证你的机器已经连接互联网。\n切换到root用户（如果你觉得root操作危险，请保证自己有足够的能力使用普通用户权限完成所有操作），执行以下命令以初始化一个k3s的master节点（当前不理解什么是“master节点”没有关系，在后续章节中我们会进行详细介绍）：\ncurl -sfL https://rancher-mirror.oss-cn-beijing.aliyuncs.com/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn sh -s - --docker 如果你选择使用containerd作为容器运行时，请去除上述命令中的--docker参数。\n等待上述命令执行结束后，实际上我们的k3s已经安装完成，即，我们已经得到了一个可用的Kubernetes集群。在继续后续操作前，我们先完善一下配置：\necho \u0026#39;export KUBECONFIG=/etc/rancher/k3s/k3s.yaml\u0026#39; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc 注意，如果你使用的不是bash，请注意更改上述命令。\n使用kubectl get node验证一下Kubernetes集群是否就绪，一般可以得到如下结果：\nNAME STATUS ROLES AGE VERSION debian Ready control-plane,master 2d1h v1.24.4+k3s1 kubectl get node这条命令是自解释的，含义是，获取当前Kubernetes集群中的所有节点信息。可以看到，当前我们这个Kubernetes集群中，仅有一个节点，并且：\n其名字（NAME）是debian（这个名字一般是当前机器的hostname）； 其状态（STATUS）是Ready（Ready表示节点状态健康，如果你刚刚完成集群创建，这里的状态显示可能是Not Ready，不要着急，等一会儿再看看）； 其在Kubernetes集群中的角色（ROLES）是control-plane,master，即两个角色：控制面（control-plane）、master； 其加入当前Kubernetes集群的时间（AGE）是2天1小时（2d1h）前； 其当前运行的Kubernetes版本号（VERSION）是v1.24.4+k3s1。 到目前为止，我们已经得到了一个可用的Kubernetes集群。但我们注意到，当前该集群中，仅包含一个节点，并且该节点的角色为control-plane,master。在实际生产环境中，很少会有这样的场景，但这对我们完成一些基础的学习和实践任务是完全足够的。如果你想拓展自己的这个单节点集群为真正的生产可用的多节点集群，可以参考 k3s的文档。\n本地版 # Kubernetes集群从一开始就是为大规模的生产环境准备的（其最早就起源于Google内部的Borg），因此，我们推荐在本实验中使用集群版的方法创建集群。\n但实际上在很多场景中，我们并不关心Kubernetes的多节点特性，而更关心如何快速在本地机器上启动一个Kubernetes集群用于简单的学习和测试目的。在这里，我们使用 minikube介绍如何在你 本地的电脑（Windows或macOS） 中启动一个简单的Kubernetes集群。\n从名字上可以看出minikube表示“mini kubernetes”。\n需要提前说明的是，minikube本质上是在你的Windows或macOS上启动了一个虚拟机，并在这个虚拟机中配置了Kubernetes的相关组件。因此，minikube用起来不会和k3s那样“直接”，在需要的时候，请及时查阅 其文档。\n本节内容只能保证你能成功启动一个minikube实例，如果你需要进一步的个性化配置，请参考 安装文档。\n理论上，minkube的安装非常简单，按照其官方文档的介绍进行即可。但由于众所周知的网络问题，我们这里对官方文档的步骤进行了一些修改。\n确保已经安装Docker，并且自己的计算机已经联网。\n安装minikube二进制文件。请进入 这个地址，进入“Installation”一节，选择自己合适的选型，按照提示，进行下载和安装。完成该步骤后，无需进行该网页中的后续步骤，请回到本文档。 使用minikube启动一个Kubernetes集群（注意根据自己电脑配置的实际情况调整命令中--cpus和--memory的值）：\nminikube start --driver=docker --cpus=4 --memory=4096mb --registry-mirror=https://mirror.baidubce.com --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers 然后，你大概会得到如下的输出：\n😄 Darwin 13.0 上的 minikube v1.26.1 ✨ 根据用户配置使用 docker 驱动程序 ✅ 正在使用镜像存储库 registry.cn-hangzhou.aliyuncs.com/google_containers 📌 Using Docker Desktop driver with root privileges 👍 Starting control plane node minikube in cluster minikube 🚜 Pulling base image ... 🔥 Creating docker container (CPUs=4, Memory=4096MB) ... 🐳 正在 Docker 20.10.17 中准备 Kubernetes v1.24.3… 🔎 Verifying Kubernetes components... ▪ Using image registry.cn-hangzhou.aliyuncs.com/google_containers/storage-provisioner:v5 🌟 Enabled addons: default-storageclass, storage-provisioner 🏄 Done! kubectl is now configured to use \u0026#34;minikube\u0026#34; cluster and \u0026#34;default\u0026#34; namespace by default 从输出信息可以看出，我们的Kubernetes集群已经成功在我们自己的电脑上启动好了。\n集群启动完成后，我们该如何访问集群呢？在k3s部分的介绍中，我们使用的是kubectl这个工具（还记得那个kubectl get nodes命令吗？）k3s在安装的时候默认会安装kubectl，minikube也会自动下载kubectl这个工具，但使用起来比较麻烦。因此，我们推荐手动安装一下kubectl。具体安装方法在 Kubernetes的文档中写得非常详细，按照其说明自行安装即可，此处不再赘述。 最后，我们还是使用kubectl get node来验证集群的安装情况：\n$ kubectl get node NAME STATUS ROLES AGE VERSION minikube Ready control-plane 22h v1.24.3 使用Kubernetes启动第一个容器 # 前面提到，Kubernetes最大的作用就是用来“管理容器”。因此，我们的第一个Kubernetes操作就从“创建一个容器”开始。\n执行：\nkubectl run first-container --image=docker.io/library/nginx --port=80 上述命令是自解释的，表示使用kubectl创建一个名为first-contaier的容器，创建该容器使用的镜像是docker.io/library/nginx，并在创建完容器后，暴露出容器的80端口。\n上述的“创建一个名为first-contaier的容器”的表述并不准确，后面会详细解释。\n执行完上述命令后，我们可以接着使用kubectl get pod查看当前所创建的容器的状态：\n可以看到，当前这个fist-container的STATUS是ContainerCreating，表示容器正在被创建。稍等一会儿我们再看一下：\n此时容器的STATUS是Running，表示容器已经成功运行了。\n前面我们还提到，Kubernetes并不会替代Docker容器运行时的工作，而只是作为一个管理者在上层存在。那么，我们刚才创建的这个first-contaienr本质上应该是Docker容器运行时“代劳”的，那么我们应该可以使用docker ps看到刚才的创建的容器。事实也确实如此。\n我们再来对比一下使用Kubernetes创建容器和使用Docker创建容器的异同：\n至此，想必大家已经对Kubernetes有了非常朴素的初步认知，接下来就让我们深入了解一下Kubernetes的一些基础概念。\nPod # 想必细心的同学已经发现了，前面在使用kubectl run创建容器的时候，命令返回的提示信息是pod/first-container created：\n意思是，一个名为first-container的pod被创建了。我们创建的不是容器吗？这个“pod”是什么东西？\nPod是Kubernetes中的基本执行单元，即管理、创建、计划的最小单元（而不是容器）。一个Pod中可以包含多个容器，可以把Pod理解为“容器组”。在英文中，pod的意思是“豆荚”，豆荚中一般会包含多个豆子，豆荚和豆子的关系就类似于Pod和容器的关系。Pod内的各个容器共享网络和存储。比如可以使用localhost互相通信。\n在上一小节的“使用Kubernetes启动第一个容器”中，我们实际上是创建了一个名为“first-container”的Pod，这个Pod中包含了一个Nginx Container，而这个container，恰恰是我们使用docker ps看到的那个容器。\n查看Pod # Kubernetes的接口非常Restful，即对其进行的任何操作都可以归结为对各种资源的增删改查。因此，当我们想查看当前集群中的Pod时，只需要使用get方法kubectl get pod：\n如果你已经知道了自己要查看的Pod名字，可以直接指定名称，例如kubectl get pod first-container：\n可以使用-o wide参数查看Pod更详细的内容：\n可以看到，上述命令的输出中，包含了Pod的IP，这个IP是我们访问容器的重要工具。由于我们在first-container中启动的是一个Nginx Container，并且暴露了80端口，因此，可以通过该IP的80端口访问到Nginx Container：\n删除Pod # 删除Pod很简单：\n$ kubectl delete pod first-container pod \u0026#34;first-container\u0026#34; deleted Pod的生命周期 # kubectl get pod中使用--watch参数可以监控集群中Pod的变化。接下来，我们通过使用--watch参数来学习Pod的生命周期。\n首先打开两个终端窗口。在其中一个终端中，输入kubectl get pod --watch。该命令不会立即返回，而是会持续一段时间，在此过程中，集群中Pod发生的变化都会在该终端中输出：\n然后，我们在另一个终端窗口中，依次执行Pod的创建操作：\n耐心等待一会儿，注意观察左侧终端窗口的变化。可以发现，名为first-container的Pod被创建，其状态\n最开始是Pending，表示Pod已经被Kubernetes系统接受，正在等待被创建。 然后ContainerCreating，表示Pod中的容器正在被创建中，这个过程可能会从网络拉取镜像，所以可能花费较长时间，特别是网络状态不好的时候。 最后是Running，并且其状态将最终停留在Running。当Pod状态为Running时，说明其已经被成功创建并正常运行了。 下面，我们在右侧的终端窗口中，删除刚创建的Pod：\n可以看到，Pod的状态变成了Terminating（中止），并最终不再变化（被删除了）。\n使用YAML文件管理Pod # 通畅情况下，我们不会直接使用kubectl run这样的命令行来管理容器，而是使用一个 YAML格式的文件来描述Pod。比如，下面是一个典型的Pod的YAML描述：\napiVersion: v1 kind: Pod metadata: name: nginx-yaml spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 YAML格式和JSON、XML一样，是一种用于配置描述语言（简单来说就是这种语言一般用来写配置文件）。YAML的语法非常简单，基本由KV键值对组成，并且使用缩进来表示嵌套关系。下面，我们就来详细说明一下上述这个Pod的YAML文件中各个字段的含义。\napiVersion: v1表示该Pod描述适用的Kubernetes的Pod的API版本是v1（如果不理解也问题不大，不影响后续的学习） kind: Pod表示当前这个YAML描述，描述的对象是一个Pod metadata表示“元信息”，一般会在这里描述Pod的名字、命名空间等“身份识别信息”。注意，metadata后面没有值，而且第二行缩进，表示metadata的值是一个对象，这个对象包含下面缩进中的若个键值对 name: nginx-yaml表示，该Pod的名字是nginx-yaml。它位于metadata的缩进中，表示其是metadata的值的一部分。 spec是对Pod的具体描述 containers描述了Pod中会包含哪些容器。可以看到，其第二行的- name: nginx是以- 开头的，这说明，containers的值是一个数组，这个数组中的每个元素都是一个对象，这个对象包含了name、image、ports等键值对 name: nginx表示容器的名字是nginx。这个名字其实没有特殊含义，符合命名规范的情况下，随便写就行 image: nginx:1.14.2表示这个容器使用的镜像是nginx:1.14.2 ports表示容器会向外暴露哪些端口 将上述Pod的YAML描述保存到一个YAML文件中，如pod-demo.yaml，然后执行kubectl apply -f pod-demo.yaml，即可将该Podapply到Kubernetes集群中。\napply的含义是，\n如果当前集群中不存在存在文件中对描述的资源，那将创建该资源； 如果已经存在了，则根据YAML文件中的描述对资源进行更新。 因此，在我们执行完kubectl apply -f pod-demo.yaml，Kubernetes将会创建一个名为nginx-yaml的Pod。\n类似地，kubectl delete -f pod-demo.yaml将会把pod-demo.yaml文件中定义的资源删除。\napply操作体现的是Kubernetes的声明式编程的思想，即，我告诉你我想要啥（我只提供一个YAML文件，即我希望最终这个Pod的样子是什么），具体怎么做由你来负责（你怎么创建Pod，怎么启动Container，是你Kubernetes的事，跟我无关）。其实，我们经常使用的SQL就有声明式编程的影子。比如一条SELECT语句，我们只是告诉数据库我们想查什么东西，但具体怎么查，使用哪个索引还是别的什么东西，作为用户的我们并不关系。与声明式编程相对的是命令式编程，即需要用户自己将问题拆解，告诉执行器该怎样一步步地解决问题。\n在我们后续的学习中，几乎所有的资源都会使用YAML文件来描述。YAML在Kubernetes中使用非常广泛，因此云计算工程师又经常自嘲自己是“YAML工程师”。\nLabel # Kubernetes支持给资源对象打标签（Label）。Pod是Kubernetes中最基本的资源对象，自然也支持打标签（Label）。\n这里所说的标签，其实就是一组键值对（key-value），内容没有限制，只要key不重复，写啥都行；并且Label需要写在metadata里。\n比如，给上一小节用到的nginx-yaml加上标签的话，应该是：\napiVersion: v1 kind: Pod metadata: name: nginx-yaml labels: kkk: hahaha hhh: hahaha kubernetes: yyds spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 上面的代码给Pod加上了三个标签，kkk: hahaha，hhh: hahaha，kubernetes: yyds。\n我们将修改后的pod-demo.yaml重新apply一下，即可给Pod打上这三个标签：\n从输出内容可以看出，nginx-yaml这个pod被更新了。\n如何验证我们设置的标签生效了呢？可以在kubectl get pod的时候使用-l参数，例如kubectl get pod -l kkk=hahaha就将返回那些被打上kkk: hahaha标签的Pod：\n可以想到，如果使用kubectl get pod -l kkk=hiahiahia，那将没有任何结果：\n基于Label选取对应的资源，是Kubernetes中非常重要的在不同类型的资源对象间建立联系的方式。\nPod Controller # 在介绍容器的时候我们提到过，container是脆弱的。在实际的生产环境中，container中运行的进程很可能因为各种各样的原因挂掉（比如JVM进程OOM），这时候，快速恢复业务的方法是重新启动一个新的容器实例。\n另一方面，为了实现负载均衡或并行计算，我们需要维护相同的多个容器实例，来共同完成任务。\n上述两方面的讨论，归结起来可以表示为：在集群中维护一定数量的容器实例。\n纯粹由人工来维护一定数量的容器实例当然是可以的，但那将是十分低效和不可靠的。Kubernetes给出了一种新的解决方法——Pod Controller，来解决这一问题。\n在学习Pod Controller之前，我们先来了解一下Kubernetes中的Controller机制。\nController # 在这里引用Kubernetes文档中给出的关于控制器的讨论：\n在机器人技术和自动化领域，控制回路（Control Loop）是一个非终止回路，用于调节系统状态。 这是一个控制环的例子：房间里的温度自动调节器。 当你设置了温度，告诉了温度自动调节器你的期望状态（Desired State）。 房间的实际温度是当前状态（Current State）。 通过对设备的开关控制，温度自动调节器让其当前状态接近期望状态。\n和上述提到的“温度自动调节器”类似，Kubernetes中的控制器（Controller）将会监控当前集群中的状态，并努力使集群的当前状态满足用户设置的目标状态。\nPod Controller # Pod Controller，顾名思义，就是用于调节集群中当前Pod状态的Controller。下面，我们依次来介绍几种Kubernetes最常用的Pod Controller。\nDeployment # Deployment的主要作用是努力使当前集群中的Pod数量与用户期望的状态相同。\n我们先来看一个简单的Deployment的YAML定义：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 selector: matchLabels: kkk: hahaha template: metadata: labels: kkk: hahaha kubernetes: yyds spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 apiVersion、kind、metadata都是Deployment的元信息配置，与Pod中的写法非常类似，不再赘述。\nspec中包含以下字段：\nreplicas：表示“副本数”。即该Deployment将会管理多少个Pod。 selector：选择器。表示该Deployment如何在集群中查找到它需要管理的Pod。matchLabels表示被打上kkk: hahaha这个标签的Pod将会被管理。 template中的内容想必大家都很眼熟，这其实就是我们上面提到的一个典型的Pod的描述。 （可能上面的描述有点抽象，下面我们将举例说明）\n将上述代码保存到本地文件中，例如deployment.yaml，然后执行kubectl apply -f deployment.yaml：\n可以看到，一个名为nginx-deployment的Deployment被创建了。可以使用kubectl get deployment查看：\n如果你得到的AVAILABLE的数量不是3，请耐心等一会儿。\n同时，我们可以查看一下当前集群里的Pod：\n可以看到，集群新启动了3个Pod，其名称前缀都是nginx-deployment，说明它们就是被我们刚刚创建的Deployment创建的。下面具体讨论一下这个过程是怎样发生的：\nkubectl apply -f nginx-deployment.yaml在Kubernetes集群中创建了一个名为nginx-deployment的Deployment。 集群中的Deployment控制器发现了新创建的名为nginx-deployment的Deployment，然后尝试解析其中的内容。 Deployment控制器对Deployment的spec.template部分做hash，得到pod-template-hash的值，例如该值为584784cc75。 Deployment控制器发现这个Deployment中描述的Pod的副本数是3，并且发现选择器（selector）选取那些带有标签kkk:hahahha的Pod。于是，Deployment控制器尝试检索集群中同时带有标签kkk:hahahha和pod-template-hash:584784cc75的Pod数量，检查其是否是3个。 Deployment控制器发现上述Pod数量0，于是，根据spec.template中的定义创建3个Pod。 我们可以尝试删除其中上述Deployment中的某一个Pod，来模拟该Pod意外崩溃的情况：\n可以在到，在其中一个Pod被删除后（进入Terminating状态后），一个新的Pod立即被创建，补上了缺位。也就是说，Deployment将永远保证集群中，被打上kkk:hahahha和pod-template-hash:584784cc75标签的Pod的数量是3。\n更新Deployment # 更新Deployment（比如更改Deployment中的副本数）非常简单，只需要编辑YAML文件，重新apply一下即可，例如修改deployment.yaml中的副本数为5：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 5 selector: matchLabels: kkk: hahaha template: metadata: labels: kkk: hahaha kubernetes: yyds spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 kubectl apply -f deployment.yaml之后可以发现Pod数量增加到了5：\n当然你也可以减小spec.replicas的值，比如从3减小到1，这时Deployment控制器会删除多余的Pod。\n总结一下，Deployment控制器的工作原理与本节开头提到的温度控制器工作原理非常相似：\n其他Pod Controller # Job # Job会创建一个或多个Pod，并确保指定数量的Pod成功终止。当Pod成功完成时，Job将追踪成功完成的情况。当达到指定的成功完成次数时，Job就完成了。删除一个Job将清除它所创建的Pod。Job一般用于定义并启动一个批处理任务。批处理任务通常并行（或串行）启动多个计算进程去处理一批工作项，处理完成后，整个批处理任务结束。\nKubernetes支持一下几种Job:\n非并行Job: 通常创建一个Pod直至其成功结束 固定结束次数的Job: 设置.spec.completions，创建多个Pod，直到.spec.completions个Pod成功结束 带有工作队列的并行Job: 设置.spec.Parallelism但不设置.spec.completions，当所有Pod结束并且至少一个成功时，Job就认为是成功。 DaemonSet # DaemonSet用于管理在集群中每个Node上运行且仅运行一份Pod的副本实例，一般来说，在以下情形中会使用到DaemonSet：\n在每个Node上都运行一个存储进程 在每个Node上都运行一个日志采集程序 在每个Node上都运行一个性能监控程序 StatefulSet # StatefulSet用来搭建有状态的应用集群（比如MySQL、MongoDB等）。Kubernetes会保证StatefulSet中各应用实例在创建和运行的过程中，都具有固定的身份标识和独立的后端存储；还支持在运行时对集群规模进行扩容、保障集群的高可用等功能。\nService # Service可以将运行在一组Pods上的应用程序公开为网络服务，简单地实现服务发现、负载均衡等功能。\nk8s的Pods具有自己的生命周期，同一时刻运行的Pod集合与稍后运行的Pod集合很有可能不同（如发生更新、node故障等），Pods的IP地址可能会随时发生变化。这就会导致一个问题：如果一组后端Pods为集群内其他前端Pods提供功能，那么前端Pods该如何找出并跟踪需要连接的IP地址？通过Service，能够解耦这种关联，方便的通过Service地址访问到相应的Pods，前端不应该也没必要知道怎么访问、访问到的具体是哪一个Pod。\nService一共有4种类型：\nClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort： 通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。仅作了解。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如，在集群内查找my-service.my-namespace.svc时，k8s DNS service只返回foo.bar.example.com这样的CNAME record）。没有任何类型代理被创建，网络流量发生在DNS层面。由于ExternalName要求kube-dns而我们使用的是coredns，也只作了解。 创建Service # Service通常通过selector（比如通过选取标签）来选择被访问的Pod。\n继续沿用我们之前所创建的nginx-deployment。可以通过下列YAML文件创建Service (将下面的内容写入service.yaml)\n#service.yaml apiVersion: v1 kind: Service metadata: name: nginx-service spec: ports: - port: 80 targetPort: 80 protocol: TCP name: anyway selector: kkk: hahaha 注意到，我们在selector中使用了标签kkk: hahaha。\n解释一下spec.ports中的各个字段：\nport：Service暴露在集群IP上的端口。集群内通过\u0026lt;clusterIP\u0026gt;:\u0026lt;port\u0026gt;可以访问Service。 targetPort：被代理的Pod上的端口。默认与port相同。 protocol：Service暴露出来的这个端口所支持的通信协议，通常是TCP或UDP。 name：端口名称，当Service具有多个端口时必须为每个端口提供唯一且无歧义的端口名称，具体内容写啥都行。 创建Service：\nkubectl apply -f service.yaml 查看service kubectl get svc 或 kubectl get service\n可以看到，第二个就是我们刚才创建的service，其中，它有一个cluster-ip：10.43.95.143。\n验证是否可以通过Service访问Pod，注意，上述这个IP是“cluster-ip”，也就是说，它是一个集群内ip，因此，只能在集群中的机器上访问：\ncurl 10.43.95.143:80 查看当前5个Pod的IP地址 kubectl get pod -l kkk=hahaha -o wide\n删除这5个Pod并等待Deployment重新创建kubectl delete pod -l kkk=hahaha。\n可以看到重新创建的5个Pods的IP地址都已经发生变化：\n但通过Service，仍能访问对应的Pod：\ncurl 10.97.91.103:80 暴露端口 # 之前创建的Service并没有指定类型，因此为默认的ClusterIP，只能在集群内部访问。如果需要将服务端口暴露在公网，可以使用NodePort类型。\n将service.yaml修改为下面的内容\n#service.yaml apiVersion: v1 kind: Service metadata: name: nginx-service labels: svc: nginx-svc spec: type: NodePort ports: - port: 80 nodePort: 32180 protocol: TCP name: anyway selector: kkk: hahaha 修改Service kubectl apply -f service.yaml\n查看service kubectl get svc nginx-service\n此时，从集群内任一节点IP的32180端口均可访问到该Service指定的Pod的80端口。\n比如，你现在所使用的虚拟机的IP是10.251.254.183，那么，你在校园网内的任何一台机器上，执行curl http://10.251.254.183:32180，都能得到如下的输出：\n当然，你也可以使用浏览器访问：\n可以尝试删除Pods并等待新的Pods创建完成，仍可以通过上述方式访问。\n基于云的课程设计（二选一） # 注意，以下两个课程设计二选一即可。\n选项一：云PaaS平台设计 # 以上学期的云计算课程设计中设计的云PaaS平台为基础，对其中的“自动部署”功能基于Kubernetes做进一步细化。\n假设有这样一个需求场景，学生针对某一门课程的某一项作业，提供了若干代码仓库地址和部署所用的配置文件；教师或助教点击“部署”按钮，云平台将会自动按照学生提供的配置文件，拉取代码仓库，进行编译、部署等操作，并返回给教师或助教一个可以访问的URL或其他形式的编译产物。\n请你：\n设计学生所提交的配置文件格式，该配置文件应该描述如何对代码仓库进行编译（或者如何打包为镜像）、编译产物如何部署、在部署时各个组件的依赖关系等等。 结合包括但不限于架构图和顺序图等方式，结合具体案例（例如，一个简单的前后端+数据库的系统），描述你的系统是如何工作的，特别是其是如何与Kubernetes进行交互的（在这个过程中，将会创建或修改哪些Kubernetes的资源对象，包括但不限于Pod、Deployment、Service等等）。 注意：仅需完成设计并编写文档即可，不需要编码实现。\n选项二：“数据分析开发环境”迁移上云 # 在数据分析实验中，在本地部署Hadoop开发环境是繁琐的，并且很容易出错，因此，可以尝试将本次课程实践中的“数据分析开发环境”的部署迁移到Kubernetes上。\n请你：\n设计并编写一系列YAML文件，这些YAML文件包含在Kubernetes中部署“数据分析开发环境”的各类Pod、Deployment、Service等的定义。用户使用kubectl将这些文件apply到Kubernetes集群中，即可完成对“数据分析开发环境”的部署，从而可以直接在Kubernetes中完成数据分析工作。 提供说明，用户应该如何使用你的YAML文件，从而在Kubernetes中完成“数据分析开发环境”的部署；并且在部署完成后，用户应该如何使用该“数据分析开发环境”进行数据分析工作。 Hint # 在启动Hive容器时，可以直接使用已经云平台已有的Hive镜像 scs.buaa.edu.cn:8081/iobs/hive:1.0.0，其Dockerfile是：\nFROM openjdk:8-bullseye RUN mkdir -p /export/server \u0026amp;\u0026amp; \\ wget https://mirror.tuna.tsinghua.edu.cn/apache/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz \u0026amp;\u0026amp; \\ tar -xvf apache-hive-3.1.2-bin.tar.gz -C /export/server \u0026amp;\u0026amp; \\ rm -rf apache-hive-3.1.2-bin.tar.gz ENV HIVE_HOME=/export/server/apache-hive-3.1.2-bin ENV PATH=\u0026#34;${PATH}:${HIVE_HOME}/bin\u0026#34; RUN wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz \u0026amp;\u0026amp; \\ tar -xvf hadoop-3.3.4.tar.gz -C /export/server \u0026amp;\u0026amp; \\ rm hadoop-3.3.4.tar.gz RUN cp ${HIVE_HOME}/conf/hive-env.sh.template ${HIVE_HOME}/conf/hive-env.sh \u0026amp;\u0026amp; \\ echo \u0026#34;export HADOOP_HOME=/export/server/hadoop-3.3.4\u0026#34; \u0026gt;\u0026gt; ${HIVE_HOME}/conf/hive-env.sh \u0026amp;\u0026amp; \\ echo \u0026#34;export HADOOP_CONF_DIR=/export/server/hadoop-3.3.4/etc/hadoop\u0026#34; \u0026gt;\u0026gt; ${HIVE_HOME}/conf/hive-env.sh \u0026amp;\u0026amp; \\ echo \u0026#34;export HIVE_CONF_DIR=${HIVE_HOME}/conf\u0026#34; \u0026gt;\u0026gt; ${HIVE_HOME}/conf/hive-env.sh \u0026amp;\u0026amp; \\ echo \u0026#34;export HIVE_AUX_JARS_PATH=${HIVE_HOME}/lib\u0026#34; \u0026gt;\u0026gt; ${HIVE_HOME}/conf/hive-env.sh \u0026amp;\u0026amp; \\ echo \u0026#34;export HIVE_LOG_DIR=${HIVE_HOME}/logs\u0026#34; \u0026gt;\u0026gt; ${HIVE_HOME}/conf/hive-env.sh RUN wget https://mirrors.tuna.tsinghua.edu.cn/mysql/downloads/Connector-J/mysql-connector-java-8.0.29.tar.gz \u0026amp;\u0026amp; \\ tar -xvf mysql-connector-java-8.0.29.tar.gz \u0026amp;\u0026amp; \\ cp mysql-connector-java-8.0.29/mysql-connector-java-8.0.29.jar ${HIVE_HOME}/lib RUN mkdir -p /data/hive EXPOSE 10000 "},{"id":7,"href":"/doc/ns-labs/table-of-contents/","title":"Table of Contents","section":"网络存储实验规划","content":" 你好，网存！ # 欢迎来到网络存储的实验课堂！\n目录 # 实验目录可见左侧边栏\n"},{"id":8,"href":"/doc/ns-labs/resources/","title":"Resources","section":"网络存储实验规划","content":" 课程相关资源 # 此处存放了课程相关资源，如系统 ISO 镜像和各种软件的下载地址。\n"},{"id":9,"href":"/doc/02_bugit/build_deploy/","title":"自动构建与部署","section":"BuGit代码托管与自动部署平台","content":" 自动构建与部署 # 原理 # 本质上，自动构建与部署的过程是，系统根据用户提供的包含在代码仓库中的配置文件，将代码编译成一个OCI规范的镜像，然后上传到镜像中心，最后通知Kubernetes集群拉取镜像运行，以对外提供可用服务。\n自动构建与部署的几个步骤：\n用户编写配置合法的配置文件，并包含在代码仓库的根目录中。\n用户触发自动构建与部署。目前支持自动监听代码推送（git push）动作，和在前端手动点击按钮触发。\n系统拉取用户指定的仓库的指定的某次提交的代码，并根据指定的Dockerfile进行镜像构建。\n系统将构建完成的镜像将推送到镜像中心（harbor.scs.buaa.edu.cn）。\n系统通知Kubernetes拉取镜像，并部署之。\n配置文件 # 配置文件一共包含两个：Dockerfile 和 .bugit.yaml。\nDockerfile # Dockerfile用来描述该代码仓库希望被如何编译和打包成一个OCI镜像。具体的编写规则可以参考 Dockerfile Reference。\n.bugit.yaml # .bugit.yaml是一个YAML文件（名称.bugit.yml也是合法的，并且请注意文件名最前面那个.）。它是对整个构建和部署过程的描述。\n下面是一个.bugit.yaml文件支持的全部指令的示例（请注意缩进）。\n下方示例中，提到的非必需字段，都可以在.bugit.yaml省略不写。 # 必需字段。表示当前的.bugit.yaml 所适用的构建与部署流程的版本号，目前仅支持 0.0.1 version: 0.0.1 # on 字段中的内容用来表示在哪个分支发生什么事件时，自动启动构建与部署流程 # 该字段中可以包含若干组内容，每一组的 key （比如，下方示例中的 main 和 master） 都是分支名称，其 value （比如下方示例中的 [push]）是一个数组，表示希望系统监听哪些事件的发生 # 比如下面的示例就表示，希望系统在远程仓库的 main 分支和 master 分支发生代码推送事件（git push）时，自动启动构建与部署流程 # 如果希望开启“自动”构建与部署的功能，那么该字段是必需的 on: main: [\u0026#34;push\u0026#34;] master: [\u0026#34;push\u0026#34;] # 必需字段。build 字段用来描述如何系统如何构建OCI镜像 build: name: build-1 # 必需。名称标识，目前没有太大意义。可以是任意字符串，但请不要带空格 type: docker # 必需。构建的类型，目前仅支持docker docker_tag: simple # 非必需。表示希望给构建好的字段加的额外tag dockerfile: ./Dockerfile # 必需。表示使用的 Dockerfile 与代码仓库根目录的相对路径 # 非必需字段。build 字段用来描述如何构建好的镜像将会被如何部署。如果不希望使用部署功能的话，该字段可以忽略。 deploy: # 必需。其值为一个列表。表示希望在哪几个分支的代码中开启部署功能。（“部署”是一个非常重的操作，需要用户明确确认） on: [\u0026#34;main\u0026#34;, \u0026#34;master\u0026#34;] # 必需(至少包含一个端口）。表示运行起来的容器将向外暴露哪些端口 ports: - name: name-1 # 名称，该端口的一个标识。必须为小写的英文字母和数字组合，可以包含短横线。但数字不能作为开头，短横线不能作为结尾。 protocol: tcp # 使用的协议，支持tcp和udp，默认是tcp port: 80 # 容器向外暴露的端口 - name: name-2 protocol: udp port: 9934 # 非必需。envs 表示服务部署时需要使用的环境变量。key 和 value 一一对应。 envs: SOME_ENV_1: \u0026#34;some_env_1\u0026#34; SOME_ENV_2: \u0026#34;some_env_2\u0026#34; # 非必需。stateful 表示该服务是有状态的还是无状态的。该字段默认为false，即默认无状态。 stateful: false # 非必需。work_dir 表示容器开始运行时，执行的命令所在的目录。如果该字段为空，默认使用镜像中指定的 workDir work_dir: /path/to/work_dir # 非必需。cmd表示容器启动时指定的命令。其又分为两部分（两个列表），其中，command用来指定命令， args 用来指定命令需要使用的参数 # 如下方的示例，对应我们常见的命令形式就是 java -jar awesome.jar，表示使用java命令运行一个jar cmd: command: [\u0026#34;java\u0026#34;] args: [\u0026#34;-jar\u0026#34;, \u0026#34;awesome.jar\u0026#34;] # 非必需。cpu限额，表示该服务最多使用多少CPU资源。默认为250m（四分之一个CPU核心） cpu: 250m # 可以直接使用数字，如 3、100，分别表示使用3个CPU核心、100个CPU核心；也可以使用m作为单位，一个CPU核心是1000m，那么250m就表示使用四分之一个CPU核心 # 非必需。内存限额，表示该服务最多使用多少内存资源。默认为0.5G memory: 512Mi # 请使用单位 Mi，Gi，或 M，G，如 0.5G，512Mi 等 BuGit平台的每个个人和项目的资源配额为2核4G。所以请合理调配每个代码仓库的资源配额。 对于典型的资源消耗大户（如Java应用对内存的消耗），需要注意在容器的启动命令中手动限制资源配额（如手动指定jvm的内存参数等），防止容器因为资源有限而使服务启动失败。 示例 # BuGit平台中的 test-project项目包含的每个项目都进行了自动构建和部署的配置，都可以作为参考。\n特别地，下面给出了一些典型的示例。\nStatic Web # 适用于纯静态文件的网站部署（如，仅包含html，css，js等文件）。\n可参考项目 static-web。\nDockerfile # FROM nginx # 下面的 . 表示使用的是当前这个Dockerfile所在的目录作为网站的根目录 # 如果你的 index.html 所在的位置与此不同，请根据实际情况修改 COPY . /usr/share/nginx/html .bugit.yaml # version: 0.0.1 on: master: [push] main: [push] build: name: build-static-nginx type: docker docker_tag: web dockerfile: ./Dockerfile deploy: on: [main, master] ports: - name: web protocol: tcp port: 80 "},{"id":10,"href":"/doc/01_common/virtual_machine_help/","title":"虚拟机使用说明","section":"云平台使用手册","content":" 虚拟机使用说明 # 连接虚拟机 # Linux系统 # 首先从云平台中获取虚拟机的IP和登录名，之后即可在本地通过任意ssh客户端登录。\nMacOS 使用系统自带的Terminal.app登录即可。\n为了更好的使用体验，推荐使用 iterm2登录。\n当然，你也可以使用 termius进行多个ssh连接的管理。\nLinux 如果你是Linux Desktop用户，那么你肯定已经有了自己喜爱的终端模拟器，此处不再赘述。 Windows 一般来讲，Windows 10（及以上）自带的cmd.exe都自带ssh client，打开cmd后直接ssh foo@x.x.x.x即可登录。\n为了更好的使用体验，推荐下载使用 Windows Terminal。\n当然，你也可以使用 termius或者其他工具（如 Xshell等）进行多个ssh连接的管理。\n校外访问 || 浏览器访问 联网 # Linux系统 # 可以依次尝试以下两种方式。\n校园网登录脚本 # 这里推荐使用buaalogin（即 srun）。\n2022年8月开始分配的虚拟机默认已经安装了buaalogin工具，可使用which buaalogin验证其是否存在，如果存在，则可以跳过此步骤。 下载并安装登录工具：\nsudo wget https://scs.buaa.edu.cn/scsos/tools/linux/buaalogin -O /usr/local/bin/buaalogin sudo chmod +x /usr/local/bin/buaalogin 配置校园网登录使用的用户名和密码：\nbuaalogin config 登录校园网：\nbuaalogin login 登出校园网：\nbuaalogin logout wukuard 服务 # 鉴于校网络中心的某些限制，上述联网方式可能在某短时间内无法使用，这里特别给出软院信息化小组的基于 Wireguard的曲线救国方案。其本质上是将虚拟机加入一个 wireguard虚拟内网，然后覆盖默认路由指向一个可以联网的内网机器，从而实现虚拟机本身与互联网的联通。\n2022年8月开始分配的虚拟机默认配置好了 wukuard服务，可使用systemctl status wukuard验证之，如果运行正常，则可跳过下述的各个步骤，直接进行最后的配置hostname即可。 请注意，以下步骤是针对Debian系发行版（包括Debian、Ubuntu等）给出的，其他发行版请自行对照着修改命令。 首先需要安装wireguard-tools（这里需要短暂联网，但完整整个步骤之后就不需要了）：\nexport http_proxy=http://10.251.0.37:3128;export https_proxy=http://10.251.0.37:3128 sudo apt update \u0026amp;\u0026amp; sudo apt install wireguard-tools -y 如果上述命令执行中，apt没有命中http_proxy，可以手动配置apt的proxy: 创建文件/etc/apt/apt.conf.d/proxy.conf，并在其中写入以下内容\nAcquire::http::Proxy \u0026#34;http://10.251.0.37:3128\u0026#34;; Acquire::https::Proxy \u0026#34;http://10.251.0.37:3128\u0026#34;; 然后重新执行 apt update \u0026amp;\u0026amp; apt install wireguard-tools -y\n注意：完成所有配置后，请将该文件删除。\n然后安装 wukuard（作为wireguard管理工具）\nsudo wget https://scs.buaa.edu.cn/scsos/tools/linux/wukuard -O /usr/local/bin/wukuard sudo chmod +x /usr/local/bin/wukuard 然后配置wukuard-client服务\nsudo wget https://scs.buaa.edu.cn/scsos/tools/linux/wukuard-client.service -O /etc/systemd/system/wukuard-client.service sudo systemctl enable --now wukuard-client 然后找管理员所要虚拟机应该使用的hostname，然后配置机器的hostname：\nsudo hostnamectl set-hostname ${your_hostname} # hostname的值找管理员要 传输文件 # Linux系统 # MacOS \u0026amp; Linux 可以使用使用SCP命令进行服务器与本地之间的文件交换。 Windows 除了在终端中使用SCP命令外，\n还可以使用 WinSCP进行图形化的文件管理。\n校外跳板 d.buaa.edu.cn 的Linux界面已经提供了比较完善的文件管理工具。 "},{"id":11,"href":"/doc/cloud-labs/cloud/faq/","title":"FAQ","section":"云计算实验","content":" FAQ # "},{"id":12,"href":"/doc/02_bugit/","title":"BuGit代码托管与自动部署平台","section":"软院云平台文档","content":" BuGit代码托管与自动部署平台 # "},{"id":13,"href":"/doc/01_common/","title":"云平台使用手册","section":"软院云平台文档","content":" 云平台使用手册 # "},{"id":14,"href":"/doc/cloud-labs/cloud/","title":"云计算实验","section":"大数据和云计算综合实践","content":" 云计算实验 # "},{"id":15,"href":"/doc/cloud-labs/","title":"大数据和云计算综合实践","section":"软院云平台文档","content":" 大数据和云计算综合实践 # "},{"id":16,"href":"/doc/ns-labs/","title":"网络存储实验规划","section":"软院云平台文档","content":" 网络存储实验规划 # 提交方式 # 我们会提供实验的 Markdown 模板，请在完成实验后导出 PDF 上传至北航软件学院云平台。命名方式见各实验的详细说明。 实验进度 # 预计总共有 3 个实验和一个课设实验 RAID 阵列实验 虚拟化实验 Ceph 实践 助教联系方式 # 李楠（微信 zhanoan619） 诚信说明 # 所有的参考资料请注明来源。实验报告将严格查重，若发现有作业抄袭现象，作业按零分处理。\n"}]