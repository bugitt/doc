[{"id":0,"href":"/doc/cloud-labs/cloud/kube-single-3/","title":"Kubernetes实验（三）","section":"云计算实验","content":"Kubernetes实验（三） Kubernetes的基本使用 #  实验目的 #   了解Kubernetes的各种特性 掌握Kubernetes的常用功能  注意事项\n 本次分配的机器的账户和密码为：  buaa: \u0026amp;shieshuyuan21 root: \u0026amp;\u0026amp;shieshuyuan21 务必首先修改机器的root和buaa账户的密码\n 请务必阅读 虚拟机使用说明。\n  分配的虚拟机中，已经安装了Docker，无需重复安装；并设置了Docker镜像地址（该地址指向校内地址），理论上docker.io中的镜像不用联网即可拉取。例如可以直接在虚拟机上docker pull nginx。\n   创建Kubernetes集群\n在实验开始前，请利用云平台提供的虚拟机或者自己的本地资源，创建一个至少包含两节点的Kubernetes集群。\n详情请参考： 附录：创建Kubernetes集群\n 配置资源的两种方式 #  使用命令 #  回想之前我们之前初始化Kubernetes集群后，执行过的命令：\nkubectl create deployment nginx-test --image=harbor.scs.buaa.edu.cn/library/nginx --replicas=2 --port=80 使用kubectl命令创建资源时，会将Pod的配置都写入命令参数中。以我们执行过的kubectl create命令（在集群中使用指定镜像启动容器）为例，命令中包含了镜像名（--image）、伸缩情况（--replicas）。当然，kubectl命令还有许多其他的功能，详情可以在 Kubernetes文档-kubectl命令看到。\n直接使用kubectl创建资源简单、直观、快捷，很适合临时测试或者试验。但实际上，创建资源时通常需要一系列的配置，如果单纯使用kubectl命令会比较麻烦，因此通常在配置资源会使用yaml配置文件。\n使用配置文件 #  在之前的实验中，我们使用了这个命令来安装Weave Scope\nkubectl apply -f https://git-v1.scs.buaa.edu.cn/iobs/static_files/raw/main/kube/weave_scope/scope.yml 这个命令实际上从后面的地址读取了一个yaml配置文件，并根据这个文件来配置资源。如果你将这个文件打开，你会看到这样的内容（省略了很多内容）：\napiVersion: v1 kind: List items: - apiVersion: v1 kind: Namespace metadata: name: weave annotations: cloud.weave.works/version: v1.0.0-302-g76376bb - apiVersion: v1 kind: ServiceAccount metadata: name: weave-scope labels: name: weave-scope namespace: weave YAML #    YAML是一种可读性高、用来表达数据序列化的格式。YAML使用空白字符和分行来分隔数据，使用一些符号来标记清单、散列表、标量等资料形态。\n 常用的YAML语法：\n 对象（键值对）：用冒号结构表示。注意，键值对的key无需使用引号。 纯量：数值直接以字面量表示，布尔值用true和false表示，当值为空时，用~表示。一般情况下，字符串可以不用引号，但是当字符串中包含空格时则必须使用引号（单双均可）。字符串也可以写成多行，但从第二行开始，必须有一个单空格缩进。换行符会被转换为空格。对于多行字符串，使用|可以保留换行符，+表示保留文字块末尾的换行，-表示删除字符串末尾的换行。 数组：数组的成员前以-标记，每个成员一行；也可以在同一行内使用[]。例如：  事实上，我们常用的用于数据传递的JSON语法是YAML1.2版的子集，将上面的YAML文件转换为我们可能更加熟悉的JSON格式的话：\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;List\u0026#34;, \u0026#34;items\u0026#34;: [ { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;weave\u0026#34;, \u0026#34;annotations\u0026#34;: { \u0026#34;cloud.weave.works/version\u0026#34;: \u0026#34;v1.0.0-302-g76376bb\u0026#34; } } }, { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ServiceAccount\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;weave-scope\u0026#34;, \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;weave-scope\u0026#34; }, \u0026#34;namespace\u0026#34;: \u0026#34;weave\u0026#34; } } ] } 这个例子应该可以帮助你理解YAML格式。节选的YAML配置文件会让Kubernetes进行如下工作：\n 创建一个Namespace，名字为\u0026quot;weave\u0026quot;，并且将版本信息加到了注释上 创建一个ServiceAccount，名字为\u0026quot;weave-scope\u0026quot;，并且也注释了一些信息  调度Pod #  前面我们已经稍微了解到，Kubernetes通过各种Controller来管理Pod的生命周期，Kubernetes也提供了数种不同功能的内置Controller。每个控制器管理集群状态的一个特定方面。最常见的情况是：一个特定的控制器使用一种类型的资源作为它的期望状态，控制器管理控制另外一种类型的资源向它的期望状态发展。\nDeployment #  回忆我们运行的第一个Pod：\n运行一个Nginx镜像 ```bash kubectl create deployment nginx-test --image=harbor.scs.buaa.edu.cn/library/nginx --replicas=2 --port=80 ``` 查看创建结果 ```bash sudo kubectl get deployment nginx-test ```  可以发现我们在查看它的时候使用的是get deployment命令。事实上我们已经部署了包含两个副本的Deployment，它的名字就是nginx-test。\n 一个Deployment控制器为Pods和ReplicaSets提供描述性的更新方式：描述Deployment中的期望状态，并且Deployment控制器以受控速率更改实际状态，以达到期望状态。\n 在这个例子中，我们的“期望状态”就是启动一个有两个副本的Nginx镜像。在这个过程中，我们创建了一个Deployment对象，通过Deployment生成了对应的ReplicaSet并完成了Pod副本的创建过程。除此之外，Deployment的主要功能还有以下几个：\n  检查Deployment的状态来看部署动作是否完成（Pod副本的数量是否达到了预期的值）\n 例如，在本例中，如果你执行kubectl create命令后马上执行kubectl get deployment命令，你可能会看到READY一栏并不是2/2，AVAILABLE一栏也不是2，也就是说，部署动作尚未完成，Pod副本数量没有达到预期的值（2）。    更新Deployment以创建新的Pod（比如镜像升级）\n  回滚之前的Deployment版本\n  暂停Deployment（修改Pod中的镜像信息，之后再恢复Deployment进行新的发布）\n  扩展Deployment（以应对高负载情况）\n  清理不再需要的旧版本ReplicaSet\n  查看更详细的信息：kubectl describe deployment nginx-test\nName: nginx-test Namespace: default CreationTimestamp: Fri, 14 May 2021 15:46:51 +0800 Labels: app=nginx-test Annotations: deployment.kubernetes.io/revision: 1 Selector: app=nginx-test Replicas: 2 desired | 2 updated | 2 total | 2 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx-test Containers: nginx: Image: harbor.scs.buaa.edu.cn/library/nginx Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-test-6884fd56dd (2/2 replicas created) Events: \u0026lt;none\u0026gt;   大多数内容都是自解释的，比如Deployment的名字、命名空间、创建时间等等。我们主要关注最后的一些信息：NewReplicaSet和Events。这些信息告诉我们Deployment创建了一个新的ReplicaSet，其中包含2个副本。\nDeployment配置文件 #  以之前的nginx-test为例，它的配置文件可以写成\n#nginx-test apiVersion: apps/v1 kind: Deployment metadata: name: nginx-test spec: replicas: 2 selector: matchLabels: run: nginx-test template: metadata: labels: run: nginx-test spec: containers: - name: nginx image: harbor.scs.buaa.edu.cn/library/nginx ports: - containerPort: 80 protocol: TCP 将上述内容写入文件nginx.yml中，然后执行：\nkubectl apply -f nginx.yml 其效果与我们在实验三中直接执行kubectl create deployment nginx-test --image=harbor.scs.buaa.edu.cn/library/nginx --replicas=2 --port=80效果是完全一样的。\n在上述这个文件nginx.yml中：\n apiVersion 当前配置所使用的API版本。其格式为组/版本号，例如刚刚使用的配置文件中，使用Deployment要表明API版本为apps/v1。 kind 当前配置资源的类型 metadata 当前配置资源的元数据。其中name是必填项。 spec 当前资源的规格，此处为Deployment的规格  replicas 副本数 selector 选择控制的Pod  matchLabels   template Pod的模板  metadata Pod的元数据，至少要定义一个label。每个label是一个键值对，key和value都可以自定义。为了和使用kubectl run创建的Deployment保持一致，这里用了run: nginx-test。  labels   spec Pod的规格。此部分定义Pod中每一个容器的属性，对于每一个容器，name和image是必须的。  ports中详细指明了该容器需要向外暴露哪些端口。        ReplicaSet #   ReplicaSet确保任何时间都有指定数量的Pod副本在运行。虽然ReplicaSet可以独立使用，但如今它主要被Deployment用作协调Pod创建、删除和更新的机制。Deployment是一个更高级的概念，它管理ReplicaSet，并向Pod提供声明式的更新以及许多其他有用的功能（比如版本记录、回滚、暂停升级等高级特性）。实际上，我们可能永远不需要操作ReplicaSet对象，而是使用Deployment。\n 查看由Deployment创建的ReplicaSet\nkubectl get replicaset NAME DESIRED CURRENT READY AGE nginx-test-6884fd56dd 2 2 2 6h7m 查看ReplicaSet的详细信息\nkubectl describe replicaset nginx-test-6884fd56dd Name: nginx-test-6884fd56dd Namespace: default Selector: app=nginx-test,pod-template-hash=6884fd56dd Labels: app=nginx-test pod-template-hash=6884fd56dd Annotations: deployment.kubernetes.io/desired-replicas: 2 deployment.kubernetes.io/max-replicas: 3 deployment.kubernetes.io/revision: 1 Controlled By: Deployment/nginx-test Replicas: 2 current / 2 desired Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=nginx-test pod-template-hash=6884fd56dd Containers: nginx: Image: harbor.scs.buaa.edu.cn/library/nginx Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Events: \u0026lt;none\u0026gt; Controlled By指明了此ReplicaSet是由我们之前创建的Deployment所创建的。那么Pod是由什么创建的呢？在之前我们查看Pod时已经知道了Pod的Name，直接查看Pod的详细信息：\n查看Pod详细信息\nkubectl describe pod nginx-test-6884fd56dd-8j7sg Name: nginx-test-6884fd56dd-8j7sg Namespace: default Priority: 0 Node: k8s-master/10.251.254.114 Start Time: Fri, 14 May 2021 15:46:51 +0800 Labels: app=nginx-test pod-template-hash=6884fd56dd Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.42.0.17 IPs: IP: 10.42.0.17 Controlled By: ReplicaSet/nginx-test-6884fd56dd Containers: nginx: Container ID: containerd://135a7109de612ec0b0e35e9b9cf3599ffd66f8e383d3a0b02c86d514fd4288fd Image: harbor.scs.buaa.edu.cn/library/nginx Image ID: harbor.scs.buaa.edu.cn/library/nginx@sha256:42bba58a1c5a6e2039af02302ba06ee66c446e9547cbfb0da33f4267638cdb53 Port: 80/TCP Host Port: 0/TCP State: Running Started: Fri, 14 May 2021 15:46:52 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2fhnf (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: kube-api-access-2fhnf: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 ConfigMapName: kube-root-ca.crt ConfigMapOptional: \u0026lt;nil\u0026gt; DownwardAPI: true QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: \u0026lt;none\u0026gt; 同样地，从Controlled By中我们可以看到，这里的Pod是由ReplicaSet创建的。根据以上的创建结果，我们可以大概推断出Kubernetes在这个过程中都做了什么：\n 用户通过kubectl创建了Deployment，名为nginx-test 该Deployment创建了ReplicaSet，名为nginx-test-6884fd56dd 该ReplicaSet创建了两个Pod，名为nginx-test-6884fd56dd-8j7sg和nginx-test-6884fd56dd-rqjxv  同时我们也可以发现，Kubernetes的对象命名方式：对象名=父对象名+字符串\n现在，使用kubectl create创建的Pod已经完成了它的使命，之后我们会用新的方式创建新的Pod。\n删除之前部署的Nginx\nsudo kubectl delete deployment nginx-test 伸缩 #  伸缩是指在线增加或减少Pod的副本数。\n使用前面所说的yml文件的方式再次创建一个名为nginx-test的Deployment。\nkubectl apply -f nginx.yml 我们创建的Deployment有两个副本，执行kubectl get pod -o wide可以看到，两个副本分别运行在master节点和普通节点上：\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 17s 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Running 0 17s 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 修改yaml配置文件，将replicas改为5，并再次执行\nkubectl apply -f nginx.yml 再次查看Pod\nkubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 66s 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Running 0 66s 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-8fl7v 1/1 Running 0 5s 10.42.1.93 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-dgqn5 1/1 Running 0 5s 10.42.1.95 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-zfpj2 1/1 Running 0 5s 10.42.1.94 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Pod副本数已经变为了5个。\n再次修改yaml配置文件，将replicas修改为3，并应用更改，再次查看Pod\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 106s 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Running 0 106s 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-dgqn5 1/1 Running 0 45s 10.42.1.95 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Pod副本数变为了3个。\n故障转移 #  由于服务器资源紧张的原因，本节涉及的开关机操作比较危险，很可能导致虚拟机长时间无响应，因此，本节内容只阅读理解相关内容即可，可以不实践操作。\n这里，我们使用直接关机的方式来模拟Kubernetes集群中有一台机器故障的情况。\n将node节点关机，这一步可以在云平台上进行，也可以直接登录node节点，使用sudo shutdown now来直接关机。\n关机后，master节点会找不到node节点，因此node节点会变为NotReady状态。\n 可能需要等待一段时间才能看到节点状态变为NotReady。不过如果用curl访问node节点上的Nginx服务器，可以马上看到Nginx已经无响应了。\n 查看节点状态 kubectl get node -o wide\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master Ready control-plane,master 14d v1.21.0+k3s1 10.251.254.114 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-73-generic containerd://1.4.4-k3s1 k8s-node NotReady \u0026lt;none\u0026gt; 14d v1.21.0+k3s1 10.251.254.110 \u0026lt;none\u0026gt; Ubuntu 20.04.2 LTS 5.4.0-73-generic containerd://1.4.4-k3s1 可以看到，node节点的状态已经更改为了NotReady\n稍等几分钟后，查看Pod kubectl get pod -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 32m 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-dgqn5 1/1 Terminating 0 31m 10.42.1.95 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-vj48s 1/1 Terminating 0 32m 10.42.1.92 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-n7mmj 1/1 Running 0 5m52s 10.42.0.16 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-jndr8 1/1 Running 0 5m52s 10.42.0.17 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 原来在node节点上的两个Pod副本，都处于Terminating状态。并且master节点上新增了两个Pod副本。处于正常运行状态的Pod副本的总数依然维持在三个。\n进入云平台，打开刚才关掉的服务器，一段时间后再次查看node列表和Pod列表，可以看到节点已经恢复为了Ready状态，处于Terminating的Pod已经被删除。\nJob #  Job会创建一个或多个Pod，并确保指定数量的Pod成功终止。当Pod成功完成时，Job将追踪成功完成的情况。当达到指定的成功完成次数时，Job就完成了。删除一个Job将清除它所创建的Pod。Job一般用于定义并启动一个批处理任务。批处理任务通常并行（或串行）启动多个计算进程去处理一批工作项，处理完成后，整个批处理任务结束。\nKubernetes支持一下几种Job:\n 非并行Job: 通常创建一个Pod直至其成功结束 固定结束次数的Job: 设置.spec.completions，创建多个Pod，直到.spec.completions个Pod成功结束 带有工作队列的并行Job: 设置.spec.Parallelism但不设置.spec.completions，当所有Pod结束并且至少一个成功时，Job就认为是成功。  一个运行一次的Job例子:\napiVersion: batch/v1 kind: Job metadata: name: pi spec: # completions: 1 # parallelism: 1 template: metadata: name: pi spec: containers: - name: pi image: harbor.scs.buaa.edu.cn/library/perl command: [\u0026#34;perl\u0026#34;, \u0026#34;-Mbignum=bpi\u0026#34;, \u0026#34;-wle\u0026#34;, \u0026#34;print bpi(2000)\u0026#34;] restartPolicy: Never Job yaml格式 #    RestartPolicy: Pod的重启策略，在这里仅支持Never或OnFailure completions: 标志Job结束需要成功运行的Pod个数，默认为1 parallelism: 标志并行运行的Pod的个数，默认为1 activeDeadlineSeconds: 标志失败Pod的重试最大时间，超过这个时间不会继续重试 container中的command: 以string数组的格式输入待执行指令 (样例中为perl输出2000位数字的pi的语句)   运行该job:\nkubectl create -f job.yaml 几分钟后，可以看到pod的创建情况 kubectl get pod：\nNAME READY STATUS RESTARTS AGE pi-5qng4 0/1 Completed 0 2m30s 此时Job已经按照预设的任务完成了，使用kubectl logs \u0026lt;podname\u0026gt;查看Pod的日志也能看到按照预设的command输出的2000位数字的$\\pi$。\n3.1415926535897932384626433832795028841971693993751058209749445923078164062862089986280348253421170679821480865132823066470938446095505822317253594081284811174502841027019385211055596446229489549303819644288109756659334461284756482337867831652712019091456485669234603486104543266482133936072602491412737245870066063155881748815209209628292540917153643678925903600113305305488204665213841469519415116094330572703657595919530921861173819326117931051185480744623799627495673518857527248912279381830119491298336733624406566430860213949463952247371907021798609437027705392171762931767523846748184676694051320005681271452635608277857713427577896091736371787214684409012249534301465495853710507922796892589235420199561121290219608640344181598136297747713099605187072113499999983729780499510597317328160963185950244594553469083026425223082533446850352619311881710100031378387528865875332083814206171776691473035982534904287554687311595628638823537875937519577818577805321712268066130019278766111959092164201989380952572010654858632788659361533818279682303019520353018529689957736225994138912497217752834791315155748572424541506959508295331168617278558890750983817546374649393192550604009277016711390098488240128583616035637076601047101819429555961989467678374494482553797747268471040475346462080466842590694912933136770289891521047521620569660240580381501935112533824300355876402474964732639141992726042699227967823547816360093417216412199245863150302861829745557067498385054945885869269956909272107975093029553211653449872027559602364806654991198818347977535663698074265425278625518184175746728909777727938000816470600161452491921732172147723501414419735685481613611573525521334757418494684385233239073941433345477624168625189835694855620992192221842725502542568876717904946016534668049886272327917860857843838279679766814541009538837863609506800642251252051173929848960841284886269456042419652850222106611863067442786220391949450471237137869609563643719172874677646575739624138908658326459958133904780275901 Cron Job定时任务 #  Kubernetes还提供了定时计划任务：\napiVersion: batch/v1 kind: CronJob metadata: name: hello spec: schedule: \u0026#34;*/1 * * * *\u0026#34; jobTemplate: spec: template: spec: containers: - name: hello image: harbor.scs.buaa.edu.cn/library/busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure 与一般的Job相比，CronJob的主要不同点在于额外有了schedule及jobTemplate两个字段。schedule是定时表达式，其格式与Linux Cron的格式基本相同，如示例的cron表达式为每分钟执行一次。jobTemplate一节与Job中的template格式相同。\n创建定时任务 kubectl create -f cron.yml\n查看定时任务 kubectl get cronjob\nNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE hello */1 * * * * False 0 2s 47s get命令只会输出上一次创建Job的时间，而不是所有。可以使用watch参数来更加直观地观察Job的创建过程 kubectl get jobs --watch\nNAME COMPLETIONS DURATION AGE hello-27032437 1/1 1s 2m34s hello-27032438 1/1 1s 94s hello-27032439 1/1 1s 34s 可以看到，每隔60s就会创建一个容器。\n获取其中一个job的Pod kubectl get pods --selector=job-name=hello-27032439 --output=jsonpath={.items[*].metadata.name}\n执行上述命令后，会输出对应pod的名字\nhello-27032439-cxt45 查看Pod输出 kubectl logs hello-27032439-cxt45\nTue May 25 12:59:24 UTC 2021 Hello from the Kubernetes cluster  最后，记得将计划任务删除，否则会一直运行。kubectl delete -f cron.yml  其他Controller #  DaemonSet #  DaemonSet用于管理在集群中每个Node上运行且仅运行一份Pod的副本实例，一般来说，在以下情形中会使用到DaemonSet：\n 在每个Node上都运行一个存储进程 在每个Node上都运行一个日志采集程序 在每个Node上都运行一个性能监控程序  StatefulSet #  StatefulSet用来搭建有状态的应用集群（比如MySQL、MongoDB等）。Kubernetes会保证StatefulSet中各应用实例在创建和运行的过程中，都具有固定的身份标识和独立的后端存储；还支持在运行时对集群规模进行扩容、保障集群的高可用等功能。\nService #  Service可以将运行在一组Pods上的应用程序公开为网络服务，简单地实现服务发现、负载均衡等功能。\nk8s的Pods具有自己的生命周期，同一时刻运行的Pod集合与稍后运行的Pod集合很有可能不同（如发生更新、node故障等），Pods的IP地址可能会随时发生变化。这就会导致一个问题：如果一组后端Pods为集群内其他前端Pods提供功能，那么前端Pods该如何找出并跟踪需要连接的IP地址？通过Service，能够解耦这种关联，方便的通过Service地址访问到相应的Pods，前端不应该也没必要知道怎么访问、访问到的具体是哪一个Pod。\n Service一共有4种类型：\n ClusterIP：通过集群的内部 IP 暴露服务，选择该值，服务只能够在集群内部可以访问，这也是默认的 ServiceType。 NodePort： 通过每个 Node 上的 IP 和静态端口（NodePort）暴露服务。NodePort 服务会路由到 ClusterIP 服务，这个 ClusterIP 服务会自动创建。通过请求 \u0026lt;NodeIP\u0026gt;:\u0026lt;NodePort\u0026gt;，可以从集群的外部访问一个 NodePort 服务。 LoadBalancer：使用云提供商的负载局衡器，可以向外部暴露服务。外部的负载均衡器可以路由到 NodePort 服务和 ClusterIP 服务。仅作了解。 ExternalName：通过返回 CNAME 和它的值，可以将服务映射到 externalName 字段的内容（例如，在集群内查找my-service.my-namespace.svc时，k8s DNS service只返回foo.bar.example.com这样的CNAME record）。没有任何类型代理被创建，网络流量发生在DNS层面。由于ExternalName要求kube-dns而我们使用的是coredns，也只作了解。   创建Service #  Service通常通过selector来选择被访问的Pod。\n继续沿用我们之前所创建的nginx-test。查看Pod详细信息：\nkubectl describe pod nginx-test-6478975c66-jtd2r\n在Labels栏能看到如下的标签（selector使用该标签来选择被访问的Pod）：\nLabels: pod-template-hash=6478975c66 run=nginx-test 因此可以通过下列yaml文件创建Service (将下面的内容写入nginx-service.yaml)\n#nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-test-service labels: svc: nginx-test-svc spec: ports: - port: 80 protocol: TCP selector: run: nginx-test   port：Service暴露在集群IP上的端口。集群内通过\u0026lt;clusterIP\u0026gt;:\u0026lt;port\u0026gt;可以访问Service。 targetPort：被代理的Pod上的端口。默认与port相同。 nodePort：Service暴露在节点外部IP上的端口。集群外通过\u0026lt;nodeIP\u0026gt;:\u0026lt;nodePort\u0026gt;可以访问Service。仅在spec.type=NodePort时可用(spec.type默认为ClusterIP)。 name：端口名称，当Service具有多个端口时必须为每个端口提供唯一且无歧义的端口名称。   创建Service\nkubectl apply -f nginx-service.yaml 查看service kubectl get svc\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.43.0.1 \u0026lt;none\u0026gt; 443/TCP 15d nginx-test-service ClusterIP 10.43.149.18 \u0026lt;none\u0026gt; 80/TCP 11s 可以看到，第二个就是我们刚才创建的service，其中，它有一个cluster-ip：10.43.149.18。\n验证是否可以通过Service访问Pod，注意，上述这个IP是“cluster-ip”，也就是说，它是一个集群内ip，因此，只能在集群中的机器上访问：\ncurl 10.43.149.18 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 查看当前三个Pods的IP地址 kubectl get pod -l run=nginx-test -o wide\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-jtd2r 1/1 Running 0 5h48m 10.42.0.15 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-n7mmj 1/1 Running 0 5h22m 10.42.0.16 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-jndr8 1/1 Running 0 5h22m 10.42.0.17 k8s-master \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 删除这三个Pods并等待Deployment重新创建\n~$ kubectl delete pods -l run=nginx-test pod \u0026quot;nginx-test-79cd7499bf-vrlss\u0026quot; deleted pod \u0026quot;nginx-test-79cd7499bf-zd42s\u0026quot; deleted ~$ kubectl get pod -l run=nginx-test -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-test-6478975c66-7ktl4 1/1 Running 0 21s 10.42.1.109 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-cffrr 1/1 Running 0 21s 10.42.1.111 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-test-6478975c66-jssj4 1/1 Running 0 21s 10.42.1.110 k8s-node \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 可以看到重新创建的三个Pods的IP地址已经发生变化，再次通过Service，仍能访问对应的Pod\ncurl 10.97.91.103 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 暴露端口 #  之前创建的Service并没有指定类型，因此为默认的ClusterIP，只能在集群内部访问。如果需要将服务端口暴露在公网，可以使用NodePort类型。\n将nginx-service.yaml修改为下面的内容\n#nginx-service.yaml apiVersion: v1 kind: Service metadata: name: nginx-test-service labels: svc: nginx-test-svc spec: type: NodePort ports: - port: 80 nodePort: 32180 protocol: TCP name: http selector: run: nginx-test 修改Service kubectl apply -f nginx-service.yaml\n查看service kubectl get svc nginx-test-service\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-test-service NodePort 10.43.115.127 \u0026lt;none\u0026gt; 80:32180/TCP 17s 此时，从集群内任一节点IP的32180端口均可访问到某个Pod的80端口。\n比如，你的两台虚拟机的IP分别为10.255.9.80和10.255.9.81，那么，你在校园网内的任何一台机器上，执行curl http://10.255.9.80:32180或者curl http://10.255.9.81:32180，都能得到如下的输出：\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 或者在浏览器中也可以访问：\n 可以尝试删除Pods并等待新的Pods创建完成，仍可以通过上述方式访问。\n滚动更新 #  为了在更新服务的同时不中断服务，kubectl支持滚动更新，它一次更新一个Pod，而不是停止整个服务。\n使用Deployment可以查看升级详细进度和状态，当升级出现问题的时候，可以使用回滚操作回滚到指定的版本，每一次对Deployment的操作，都会保存下来，方便进行回滚操作，另外对于每一次升级都可以随时暂停和启动，拥有多种升级方案：Recreate删除现在的Pod，重新创建；RollingUpdate滚动升级，逐步替换现有Pod，对于生产环境的服务升级，显然这是一种最好的方式。\n创建Deployment\napiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: harbor.scs.buaa.edu.cn/library/nginx:1.16 使用kubectl create -f nginx-deploy.yaml，由于replicas为3，因此会创建三个Pod\nNAME READY STATUS RESTARTS AGE nginx-77bf7f47fd-9q7fw 1/1 Running 0 80s nginx-77bf7f47fd-567nf 1/1 Running 0 80s nginx-77bf7f47fd-k795n 1/1 Running 0 80s 此时尝试访问Nginx主页，会看到版本提示。\n 注意：使用的镜像并不是官方原版镜像，而是修改了默认的index.html后的镜像。访问Nginx主页，既可以自己创建一个service进行访问，也可以查看pod的IP后直接在虚拟机上使用curl访问。\n 通过配置文件更新：\n在刚刚的配置文件中将镜像修改为harbor.scs.buaa.edu.cn/library/nginx:1.17，然后在spec中添加滚动升级策略，改动后如下\napiVersion: apps/v1 kind: Deployment metadata: name: nginx spec: minReadySeconds: 5 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: harbor.scs.buaa.edu.cn/library/nginx:1.17  minReadySeconds:  Kubernetes在等待设置的时间后才进行升级 如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了 如果没有设置该值，在某些极端情况下可能会造成服务服务正常运行   maxSurge:  升级过程中最多可以比原先设置多出的POD数量 例如：maxSurage=1，replicas=5,则表示Kubernetes会先启动1一个新的Pod后才删掉一个旧的POD，整个升级过程中最多会有5+1个POD。   maxUnavaible:  升级过程中最多有多少个POD处于无法提供服务的状态 当maxSurge不为0时，该值也不能为0 例如：maxUnavaible=1，则表示Kubernetes整个升级过程中最多会有1个POD处于无法服务的状态。    然后执行命令 kubectl apply -f nginx-deploy.yaml\n接着查看deployment的升级情况 kubectl rollout status deploy/nginx\nWaiting for deployment \u0026quot;nginx\u0026quot; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;nginx\u0026quot; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;nginx\u0026quot; rollout to finish: 2 out of 3 new replicas have been updated... Waiting for deployment \u0026quot;nginx\u0026quot; rollout to finish: 2 of 3 updated replicas are available... Waiting for deployment \u0026quot;nginx\u0026quot; rollout to finish: 2 of 3 updated replicas are available... deployment \u0026quot;nginx\u0026quot; successfully rolled out 执行kubectl rollout status deploy/nginx可以实时观测滚动更新的进度，在更新过程中，可以使用kubectl rollout pause(resume) deployment \u0026lt;deployment\u0026gt;来暂停(继续)更新\n更新结束后，查看Replica Set的状态 kubectl get replicaset\nNAME DESIRED CURRENT READY AGE nginx-77bf7f47fd 0 0 0 8m28s nginx-5b8c7c5877 3 3 3 3m7s 可以看到旧的Replica Set和新的Replica Set的状态，此时查看任意一个Pod的镜像信息\n ...... Containers: nginx: Container ID: containerd://0790761a1248fcb4dc8d9432aebeb37ff7ee96b7267a83fe0172812323764e85 Image: harbor.scs.buaa.edu.cn/library/nginx:1.17 Image ID: harbor.scs.buaa.edu.cn/library/nginx@sha256:6f3b7f003243dca41dae46fe442e826df08e056a76cdd3e11e6f41cdaef ...... 其中image的信息已经得到改动了。\n此时再次尝试访问Nginx主页，会看到版本已经变化。\n 回滚Deployment  首先，查看Deployment的升级历史 kubectl rollout history deploy/nginx\ndeployment.apps/nginx REVISION CHANGE-CAUSE 1 \u0026lt;none\u0026gt; 2 \u0026lt;none\u0026gt; 其中，1和2为历史版本，可以带上参数来查看该次的版本信息 kubectl rollout history deploy/nginx --revision=1\ndeployment.apps/nginx with revision #1 Pod Template: Labels:\tapp=nginx pod-template-hash=77bf7f47fd Containers: nginx: Image:\tharbor.scs.buaa.edu.cn/library/nginx:1.16 Port:\t\u0026lt;none\u0026gt; Host Port:\t\u0026lt;none\u0026gt; Environment:\t\u0026lt;none\u0026gt; Mounts:\t\u0026lt;none\u0026gt; Volumes:\t\u0026lt;none\u0026gt; 可以使用kubectl rollout undo deploy/nginx来回退到上一个版本，也可以在后面加上参数--to-revision=3来回退到3所指定的历史版本\n数据管理 #  Pod中的数据默认并没有进行持久化，它们会随着Pod的销毁而一同被消灭。 我们都知道，Pod并不总是稳定可靠的，它有可能会被频繁销毁并重建，我们并不希望重要的数据（例如MySQL中的数据）跟着Pod一同被销毁。这就需要引入一种持久的存储系统。\n Volume和Persistent Volume  Kubernetes提供了两种储存介质，它们是Volume和Persistent Volume。简单来说，Volume中的数据不能被持久化，Pod销毁，数据消失；而Persistent Volume中的数据则独立于Pod，即使Pod销毁，数据依然可以永久保存，除非你不希望。\n 常用的(Persistent) Volume  emptyDir(非Persistent Volume) hostPath(Persistent Volume) nfs(Persistent Volume)    使用hostPath #  使用hostPath，将容器的/storage目录挂载到容器所在主机的/home/storage目录下\napiVersion: v1 kind: Pod metadata: name: hostpath-test spec: containers: - name: busybox image: harbor.scs.buaa.edu.cn/library/busybox:1.24 volumeMounts: - name: storage mountPath: /storage #将容器的 /storage 目录作为挂载点 args: - /bin/sh - -c - echo \u0026#34;test file\u0026#34; \u0026gt; /storage/test.txt restartPolicy: \u0026#34;Never\u0026#34; volumes: - name: storage hostPath: path: /home/buaa/storage # 挂载到主机的 /home/buaa/storage目录下   volumeMounts: container中需要被mount的目录 volumes: 根据name来对应container中的volumeMounts并选择mount到本地的路径   使用kubectl get pod -o wide可查看pod所在主机，进入主机查看/home/buaa/storage下的文件\n$ ls /home/buaa/storage test.txt $ cat /home/storage/test.txt test file hostPath的缺点为若使用一个pod需要跨主机到另外一台主机上重建，数据将会丢失，这时我们需要PersistentVolume来进行持久化\n使用PV和PVC #  两者的关系为：PV用于定义存储，而PVC用于向已有存储申请存储空间\n用nfs来实现PV比较容易，因此用nfs来做实例\n  nfs:网络文件系统（英语：Network File System，缩写作 NFS）   nfs需要一个server端，一个client端，对于实验的Kubernetes集群来说，可将Master节点作为server同时为client(因为设置了Master节点开启工作负载)，其他所有从节点为client。\n配置nfs server端 #  在nfs服务器上安装nfs-kernel-server （理论上实验用的机器已经安装，如果没有，请自行联网安装）\napt install nfs-kernel-server 首先创建一个用于存放数据的文件夹\nmkdir /data 接下来修改nfs的配置文件\n$ vim /etc/exports #添加如下内容 /data *(rw,sync,no_root_squash) 其中：\n  /data： 共享的目录 * ： 谁可以访问(这里设置为所有人能访问) (rw,sync,no_root_squash): 权限设置  rw: 能读写 sync: 同步 no_root_squash: 不降低root用户的权限(不安全，不过本次实验选择这样做更方便)     重启nfs服务：\nservice nfs-kernel-server restart 在所有节点安装nfs-common （理论上实验用的机器已经安装，如果没有，请自行联网安装）\napt install nfs-common 安装好后可使用如下指令查看是否能看到nfs服务器的挂载点(x.x.x.x为nfs服务器的ip地址)\n$ showmount -e x.x.x.x Export list for x.x.x.x: /data * 这时候说明已经能够访问到这个挂载点了，使用以下指令将挂载点挂载到本地的/mnt路径下(x.x.x.x同样为nfs服务器地址)\nmount x.x.x.x:/data /mnt 无提示则说明挂载成功，此时在nfs客户端上进行测试\n$ cd /mnt $ touch success\n若看到nfs服务器上/data目录中出现success文件，则说明nfs已经部署成功，将集群所有机器都进行挂载(包括服务器本机)，在一个挂载点删除后，所有客户端和服务器也会同步删除\n创建PV #  准备好nfs环境后，创建PersistentVolume(pv)\napiVersion: v1 kind: PersistentVolume metadata: name: nfspv spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: nfs nfs: path: /data #要mount到的路径 server: x.x.x.x #nfs服务器ip   accessModes:  ReadWriteOnce \u0026ndash; PV能以read-write模式mount到单个节点 ReadOnlyMany \u0026ndash; PV能以read-only模式mount到多个节点 ReadWriteMany \u0026ndash; PV能以read-write模式mount到多个节点   persistentVolumeReclaimPolicy  Retain \u0026ndash; 需要管理员手工回收 Recycle \u0026ndash; 清除PV中的数据，效果相当于执行rm -rf /thevolume/* Delete \u0026ndash; 删除Storage Provider上的对应存储资源   nfs  path的路径要存在，若不存在，Pod无法正常运行，请提前手动创建好(动态pv无需手动创建)     使用指令kubectl create -f \u0026lt;filename\u0026gt;按照上述yaml文件创建PV后\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfspv 1Gi RWO Retain Available nfs 3s 接下来创建PVC\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: nfspvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: nfs 创建后查看PVC和PV的情况\nkubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nfspvc Bound nfspv 1Gi RWO nfs 5s kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE nfspv 1Gi RWO Retain Bound default/nfspvc nfs 82m 发现pvc已经和pv绑定了，因为Kubernetes将查找满足申领要求的pv，并将pvc绑定到具有相同StorageClass的适当的pv上\n接下来在裸Pod中使用PVC\nkind: Pod apiVersion: v1 metadata: name: test-pod spec: containers: - name: test-pod image: harbor.scs.buaa.edu.cn/library/busybox:1.24 command: - \u0026#34;/bin/sh\u0026#34; args: - \u0026#34;-c\u0026#34; - \u0026#34;touch /mnt/SUCCESS \u0026amp;\u0026amp; exit 0 || exit 1\u0026#34; volumeMounts: - name: nfs-pvc mountPath: \u0026#34;/mnt\u0026#34; #此处为容器内的被挂载点，根据pv中的设置，将这个目录直接挂载到nfs服务器的/data目录下 restartPolicy: \u0026#34;Never\u0026#34; volumes: - name: nfs-pvc persistentVolumeClaim: claimName: nfspvc 这个pod使用busybox来执行一段语句，在需要mount的路径下新建一个名为SUCCESS的文件。创建好pod后，等待pod的Status变为Completed之后，查看所有/mnt文件夹以及/data文件夹\n$ kubectl get pod NAME READY STATUS RESTARTS AGE test-pod 0/1 Completed 0 10s #pod已经完成，查看被挂载以及挂载目录 #nfs-server: $ ls /data SUCCESS #nfs-client $ls /mnt SUCCESS 通过busybox创建文件已经通过pv存储在了nfs服务器中，并同步到了各个客户端的挂载点上。\n实际上，只需要nfs服务器即可达到实验效果，使用客户端可加深对nfs的理解。\n 动态PV  动态PV是指使用PVC之前不需要创建PV，而是在PVC申请存储空间的时候自动根据条件创建，也叫做动态供给（Dynamical Provision）。动态供给的基础是StorageClass，详细可参考： StorageClass\n动手做 #    复现手册中提到的Kubernetes操作，并理解操作所涉及到的Kubernetes特性/功能\n  部署nginx服务\n  创建nginx的Deployment\n  复现故障转移和伸缩功能\n  创建nginx的Service\n  ServiceType应为NodePort\n  Deployment的replicas至少为2\n  为每个Nginx Pod创建不同的service.html\n  通过Service NodePort对Pod的service.html进行多次curl，要求每次结果都不相同(因为被代理到了不同的Pod，且默认的选择算法为round-robin)，例如：\n~$ curl \u0026lt;nodeIP\u0026gt;:\u0026lt;nodePort\u0026gt;/service.html 2 ~$ curl \u0026lt;nodeIP\u0026gt;:\u0026lt;nodePort\u0026gt;/service.html 1  Tips:\n可以通过kubectl exec -it \u0026lt;podName\u0026gt; -- bash进入Pod内部 要创建的文件位于/usr/share/nginx/html/service.html 容器内并未安装文本编辑器，可以通过echo和cat命令对service.html进行编辑，如\necho 'single string' \u0026gt; service.html cat \u0026gt; service.html \u0026lt;\u0026lt;EOF complicated string with multiple lines EOF      尝试使用滚动更新，对比前后主页版本 (可以自行创建Service通过代理访问网页，也可以直接curl ClusterIP访问。注意浏览器缓存影响)\n    部署持久化服务\n 在集群中部署nfs环境 创建pv和pvc 创建mysql的Deployment和Service，确保绑定了pvc (mysql的mount目录为/var/lib/mysql)，创建MySQL时，可以使用镜像harbor.scs.buaa.edu.cn/library/mysql:8 查看nfs server的mount目录，mysql数据文件已经写入    "},{"id":1,"href":"/doc/ns-labs/table-of-contents/raid/","title":"Lab01 RAID 阵列","section":"Table of Contents","content":"Lab01 RAID 阵列 #  实验目的 #  通过 RAID 原理实践理解阵列相关知识\n实验内容 #  本实验通过软件 RAID 管理磁盘：\n 打开“管理工具-计算机管理-存储-磁盘管理”，体验“磁盘管理工具” C 盘扩展卷 检查其它磁盘是否联机，联机并初始化磁盘  尝试创建跨区卷并存储部分数据 尝试创建镜像卷并存储部分数据 尝试创建 RAID5 卷并存储部分数据   通过设备管理禁用其中一块磁盘，模拟磁盘损坏。观察有什么情况发生，数据是否损坏或者丢失 [选做] 自行设计与扩展，体会 RAID 0 1 5 10 等方案下，在遇到磁盘损坏故障、冗余备份的效果等 [选做] 思考题：  为什么及在什么条件下选择用 RAID RAID \u0026amp; 分布式存储 \u0026amp; 集中存储的区别    请在 10 月 11 日中午 12：00 前提交至云平台，命名为：lab01-学号-姓名.pdf，如 lab01-18373722-朱英豪.pdf。\n实验指南 #  通过实验平台获取虚拟机，通过远程桌面访问获取的虚拟机的 IP 地址（ipv4），用户名密码通过实验平台获取。\n按要求完成实验，撰写实验报告（每种配置都做一遍），以下为样例的 IP 和云平台默认分配的用户名、密码。\n登录IP：10.200.200.155 # 请到云平台上查看自己所分配到的虚拟机的 IP 地址 登录用户名：Administrator # 注意在连接虚拟机时需要指定用户名，不是 admin 登录密码：@buaa21 # 记得改密码，防止被别人登 分配磁盘时不宜过大，1 G 足矣，可选择使用“快速格式化”，提高实验速度。\n如何连接虚拟机 #   在 Windows 中可使用“远程桌面连接”工具——win+R 打开“运行”，然后输入 mstsc，然后在弹出窗口中输入 IP、账号、密码即可。 在 MacOS 上可使用 Parallels 等工具连接  注意：\n云平台上没有显示虚拟机 IP 或显示的 IP 无法登录怎么办？首先，可以用来登录的 IP 位于 10.251.252.1~10.251.255.254 之间。\n 如果未登录过实验虚拟机：云平台上显示的 IP 是 169 或者无 IP 则暂时无法登录，需要重启虚拟机重新获取 IP。需要注意的是，虚拟机的重启、以及 DHCP IP 的获取都需要时间，开机后应等待 3 分钟以上，请勿频繁开关机 如果之前登录过实验虚拟机：云平台上显示的是 192 开头的 IP 或者无 IP，可以尝试使用之前的可用 IP 进行登录。如果无法登录（包括密码错误），请联系助教解决。  可能出现“发生身份验证错误，要求的函数不受支持”的错误，请参考该 博客，使用解决方案一：配置本地机的组策略。\n Win+R 在弹出的“运行”框中，输入 gpedit.msc 进入到：计算机配置-\u0026gt;管理模板-\u0026gt;系统-\u0026gt;凭据分配-\u0026gt;加密数据库修正-\u0026gt;启动该策略，将保护层级设置为“易受攻击\u0026quot;。  重要提醒：\n由于分配的 IP 均为 DHCP 自动获取，有可能实验虚拟机的 IP 会发生变化，尤其是在长时间关机再开启后。因此请登录实验虚拟机后，及时更改虚拟机密码，以避免以下情况发生：\n 未意识到登录了他人的虚拟机， 帮助别人完成了实验工作 自己的虚拟机被别人登录，已经做完的实验被破坏  同时，虚拟机密码请设置非常用敏感密码。因为助教在帮助大家解决问题时往往需要你的密码登录到你的虚拟机。\n自行配置环境 #  你也可以自己在 VMware 等虚拟机中安装 Windows Server 2019 (或其他版本)，不使用云平台的资源去做实验。可至 Next, ITELLYOU 该网站上去下载相关系统的 ISO 镜像，本课程网站上的“Resources”区，给出了其 ED2K 的下载链接以及 KMS 激活方法。\n参考作业提交的 Markdown 模板 #  # Lab01 RAID 阵列 \u0026gt; 班级： \u0026gt; 学号： \u0026gt; 姓名：  ## 实验内容  ### 体验“磁盘管理工具”  ### C 盘扩展卷  ### 检查其它磁盘是否联机，联机并初始化磁盘  - 尝试创建跨区卷并存储部分数据 - 尝试创建镜像卷并存储部分数据 - 尝试创建 RAID5 卷并存储部分数据 ### 通过设备管理禁用其中一块磁盘，模拟磁盘损坏。 \u0026gt; 观察有什么情况发生，数据是否损坏或者丢失  ## 实验感想 "},{"id":2,"href":"/doc/ns-labs/resources/os/","title":"操作系统","section":"Resources","content":"操作系统 #  常见操作系统 ISO 镜像可至官网、 MSDN，我告诉你、 Next, ITELLYOU 等网站上去下载。\nWindows Server 2019 #  ED2K 下载：ed2k://|file|cn_windows_server_2019_updated_july_2020_x64_dvd_2c9b67da.iso|5675251712|19AE348F0D321785007D95B7D2FAF320|/\n使用 KMS 激活方式如下：\n# 使用管理员身份打开PowerShell DISM /online /Set-Edition:ServerDatacenter /ProductKey:WMDGN-G9PQG-XVVXX-R3X43-63DFG /AcceptEula # 打开 CMD (管理员) slmgr.vbs /ipk WMDGN-G9PQG-XVVXX-R3X43-63DFG slmgr.vbs /skms kms.teevee.asia slmgr.vbs /ato "},{"id":3,"href":"/doc/cloud-labs/cloud/cloud_lab/","title":"云PaaS平台开发","section":"云计算实验","content":"云PaaS平台开发 #  实验要求 #  基于Kubernetes，设计并实现一PaaS平台。该平台的终极目标，用户可以通过该平台，实现从源代码到可访问的服务之间的整个自动化流程。\n比如，在传统方式下，用户编写了一个React应用，为了能让其他人通过互联网访问到他的应用，他需要首先在云厂商购买服务器，然后在服务器上安装操作系统和NodeJS，然后手动拉取依赖和编译，然后手动将编译好的静态文件放到合适的地方（配置Nginx等）。\n而在通过这个PaaS平台，用户只需要提供自己的源代码，并按照平台的需求，编写一个配置文件（可能会包含一个Dockerfile，也可能是某种模板的形式），剩下的工作可以放心地交给PaaS平台来完成。\n那么PaaS平台是如何实现这个功能的呢？首先，它根据用户提供的源代码和配置文件，将源代码编译成一个OCI镜像；然后将该镜像交给一个容器管理平台部署为一个容器组（一般是Kubernetes中的Deployment或StatefulSet）；然后根据用户提供的配置文件，将容器组的特定端口暴露到互联网上（例如使用Service的NodePort或LoadBalancer）。\n进一步地，一个真正的生产级别的应用不可能由一个容器组提供的功能支撑（最简单的一个前后端分离的应用，就要包含一个前端、一个后端，可能还有数据库和缓存服务等），为了完成整个应用的部署，用户一般会通过配置文件来指定多个不同的容器。这些容器之间的逻辑是相互联系，平台需要能够将它们在逻辑上与其他的应用容器区分开。\n因此，可以看到，一个完整的PaaS平台可以包含（最终小组作品可以包含下述一个或多个功能点）：\n 镜像管理  用户可以向平台添加自己需要的镜像，可以直接提供一个Dockerfile，也可以直接提供镜像的压缩包，也可以提供源代码（可以是代码的压缩包，也可以是一个代码仓库地址等等），也可以直接让平台去拉公共的镜像仓库 用户可以浏览、更改和删除自己添加的镜像 在镜像管理部分，可以自己在本地通过Docker Image管理，也可以考虑搭建一个私有的 Docker Registry   容器管理 用户可创建和修改容器，并监控容器状态 （考虑使用Kubernetes进行部署和管理） 应用部署 用户可以直观地将若干个逻辑上统一的容器编排成完成的应用，并发布。这里这个“应用”的概念是Kubernetes本身没有的，需要你自己去抽象。比如，可以把不同应用的容器组使用namespace隔离等等。在KubeSphere上，这个概念其实就是一个“项目”）  希望同学们在实现上述功能的过程中加深对Kubernetes各项概念的理解，并体会云计算为应用的发布和运维带来的便利。\n开发过程中，需要使用到Kubernetes和Docker Engine对外提供的API，可以直接调用他们的OpenAPI，也可以使用官方或第三方封装好的Client SDK。\n Kubernetes的OpenAPI描述文件可以在 这里找到，这里列出了比较流行的一些Kubernetes 客户端库。\nDocker的OpenAPI描述文件可以在 这里找到，这里列出了比较流行的一些Docker 客户端库。\n实验代码管理与部署 #  实验代码请托管到软院代码托管平台 BuGit上。\n首次使用代码托管平台时需要激活账户。激活账户时，请注意邮箱的正确性，并牢记密码。  系统开发将分小组进行，需要小组在 BuGit上创建项目，并邀请所有小组成员加入。\n可以使用的资源 #     KubeSphere，该平台的初始账号密码与BuGit相同（如果登录失败，可以尝试使用密码Newpass@2021登录），并且其上的项目与BuGit同步。在BiGit上创建项目后，可在KubeSphere对应的项目中部署容器。\n   Harbor，该平台的初始账号密码与BuGit相同，并且其上的项目与BuGit同步。在BiGit上创建进行代码仓库的构建后，可在Harbor对应的项目中查看到创建的镜像。\n  校内的Docker Hub镜像地址：10.251.0.37:5000。\n  "},{"id":4,"href":"/doc/ns-labs/table-of-contents/","title":"Table of Contents","section":"网络存储实验规划","content":"你好，网存！ #  欢迎来到网络存储的实验课堂！\n目录 #  实验目录可见左侧边栏\n"},{"id":5,"href":"/doc/cloud-labs/cloud/appendix_create_kubernetes/","title":"附录：创建Kubernetes集群","section":"云计算实验","content":"创建Kubernetes集群 #  Kubernetes生态发展至今已非常完善，部署一个Kubernetes集群已经不再是一件非常繁琐和困难的事情。社区有大量简单可靠的解决方案。\n下面给出几种可选的方案，根据自己的实际情况，选择其一即可。\nkubeadm（不推荐） #  Kubernetes官方推荐使用 kubeadm 来初始化一个Kubernetes集群。通过它，用户可以获得是一个相对“纯净”的Kubernetes集群。但该方法相对繁琐，而且对国内用户非常不友好，因此不推荐这种方式部署。\n有兴趣的同学可以尝试。\nKubeKey（推荐） #   KubeKey 是由 Kubesphere （一个国内公司主导的开源的Kubernetes管理平台）开源的Kubernetes和Kubesphere部署工具。\nKubeKey使用声明式的配置方式，用户只需要通过一个YAML配置文件给出所需集群的相关配置，即可通过KubeKey创建集群或修改集群的状态。更详细内容和使用方式请参考 KubeKey的文档。\n创建集群 #  在本次实验分配的虚拟机中，已经提前完成了KubeKey的部分配置，只需按照下面几步操作即可完成Kubernetes集群的创建。\n注意\n为了减少不必要的麻烦，请直接使用root账户登录虚拟机，并完成相关操作。\n   进入/root/kubesphere目录\ncd /root/kubesphere   修改配置文件中的IP和登录密码。可以按需修改主机的hostname。如果修改了hostname，也需要同步修改roleGroups中的值。\napiVersion: kubekey.kubesphere.io/v1alpha1 kind: Cluster metadata: name: main-cluster spec: hosts: - {name: node1, address: 1.1.1.1, internalAddress: 1.1.1.1, user: root, password: \u0026#39;\u0026amp;\u0026amp;shieshuyuan21\u0026#39;} - {name: node2, address: 1.1.1.1, internalAddress: 1.1.1.1, user: root, password: \u0026#39;\u0026amp;\u0026amp;shieshuyuan21\u0026#39;} roleGroups: etcd: - node1 master: - node1 worker: - node1 - node2 controlPlaneEndpoint: domain: lb.kubesphere.local address: \u0026#34;\u0026#34; port: 6443 kubernetes: version: v1.20.4 imageRepo: kubesphere clusterName: cluster.local network: plugin: calico kubePodsCIDR: 172.20.0.0/16 kubeServiceCIDR: 172.21.0.0/16 registry: registryMirrors: [] insecureRegistries: [] addons: []   保存文件并退出后，执行以下命令即可：\n./kk create cluster -f config.yaml   等待完成即可。完成后，可以直接在机器中使用kubectl get node来验证。\n  删除集群 #  如果需要删除创建好的集群，只需要执行下述删除命令即可：\ncd /root/kubesphere ./kk delete cluster -f config.yaml RKE（一般推荐） #  使用RKE时，需要向目标机器传递公钥。请不要使用虚拟机默认生成的公钥。\n因为所有实验机器密钥对都相同，当你向目标机器传递了公钥后，本次实验的其他同学将可以无障碍登录你的目标机器。\n  RKE（Rancher Kubernetes Engine）是rancher提供的一个Kubernetes管理工具。\n与KubeKey一样，RKE同样提供了声明式的配置方式，你可以在RKE的引导下，创建配置文件，并以此创建集群和管理集群的状态。\n本次实验分配的虚拟机中，提供了rke的可执行文件，可以直接使用。对此感兴趣的同学可以参考 rke的安装说明文档。\nk3s（一般推荐） #  k3s是一个非常轻量的Kubernetes发行版。其安装和配置方法在其 官方文档中写的很详细。\n其他（单机推荐） #  如果需要在本地创建Kubernetes集群，可以选择使用Minikube、Docker Desktop等。\n"},{"id":6,"href":"/doc/ns-labs/resources/","title":"Resources","section":"网络存储实验规划","content":"课程相关资源 #  此处存放了课程相关资源，如系统 ISO 镜像和各种软件的下载地址。\n"},{"id":7,"href":"/doc/01_common/virtual_machine_help/","title":"虚拟机使用说明","section":"云平台使用手册","content":"虚拟机使用说明 #  连接虚拟机 #  Linux系统 #  首先从云平台中获取虚拟机的IP和登录名，之后即可在本地通过任意ssh客户端登录。\nMacOS 使用系统自带的Terminal.app登录即可。\n为了更好的使用体验，推荐使用 iterm2登录。\n当然，你也可以使用 termius进行多个ssh连接的管理。\nLinux 如果你是Linux Desktop用户，那么你肯定已经有了自己喜爱的终端模拟器，此处不再赘述。Windows 一般来讲，Windows 10（及以上）自带的cmd.exe都自带ssh client，打开cmd后直接ssh foo@x.x.x.x即可登录。\n为了更好的使用体验，推荐下载使用 Windows Terminal。\n当然，你也可以使用 termius或者其他工具（如 Xshell等）进行多个ssh连接的管理。\n校外跳板 通过 d.buaa.edu.cn 跳转登录即可。 联网 #  Linux系统 #  登录虚拟机后，可以执行下面命令进行联网，请注意替换学号和密码：\n/usr/bin/curl -k -d \u0026#34;action=login\u0026amp;username=学号\u0026amp;password=密码\u0026amp;type=2\u0026amp;n=117\u0026amp;ac_id=1\u0026#34; \u0026#34;https://gw.buaa.edu.cn/cgi-bin/srun_portal\u0026#34; 传输文件 #  Linux系统 #  MacOS \u0026amp; Linux 可以使用使用SCP命令进行服务器与本地之间的文件交换。Windows 除了在终端中使用SCP命令外，\n还可以使用 WinSCP进行图形化的文件管理。\n校外跳板 d.buaa.edu.cn 的Linux界面已经提供了比较完善的文件管理工具。 "},{"id":8,"href":"/doc/cloud-labs/cloud/faq/","title":"FAQ","section":"云计算实验","content":"FAQ #  "},{"id":9,"href":"/doc/01_common/","title":"云平台使用手册","section":"软院云平台文档","content":"云平台使用手册 #  "},{"id":10,"href":"/doc/cloud-labs/cloud/","title":"云计算实验","section":"大数据和云计算综合实践","content":"云计算实验 #  "},{"id":11,"href":"/doc/cloud-labs/","title":"大数据和云计算综合实践","section":"软院云平台文档","content":"大数据和云计算综合实践 #  "},{"id":12,"href":"/doc/ns-labs/","title":"网络存储实验规划","section":"软院云平台文档","content":"网络存储实验规划 #  提交方式 #   我们会提供实验的 Markdown 模板，请在完成实验后导出 PDF 上传至北航软件学院云平台。命名方式见各实验的详细说明。  实验进度 #   预计总共有 5 个实验，其中包含一个可选申优答辩的课设实验  RAID 阵列实验 主机虚拟化实验 FreeNas 实验 Ceph 实践    助教联系方式 #   李楠（微信 zhanoan619） 朱英豪（微信 zhuyinghao）  诚信说明 #  所有的参考资料请注明来源。实验报告将严格查重，若发现有作业抄袭现象，作业按零分处理。\n"}]